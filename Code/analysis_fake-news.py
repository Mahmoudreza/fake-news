import string, json, os, sys, code, time, multiprocessing, pickle, glob, re, cPickle
import collections, shelve, copy, itertools, math, random, argparse, warnings
import sqlite3, gzip, datetime, socket, getpass, csv, gc, traceback
import operator, gc
import pylab as Plab
from scipy.stats import expon

sys.path.append(os.path.expanduser("/home/babaei/Desktop/SVN/streaming_api/with_tweepy/dataset-master"))
# import dataset
# import Image
import psycopg2 as pgsql

import numpy as np
import pandas as pd
import matplotlib.pyplot as mplpl
import pickle
import cPickle

import urllib
# import mysql.connector as conn
from scipy import spatial
from scipy.stats.stats import pearsonr
from matplotlib.backends.backend_pdf import PdfPages
import scipy
import scipy.stats
import scipy.optimize
from scipy.sparse import coo_matrix, hstack, csr_matrix
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
# from textblob import TextBlob


# from ifa.distribution import Distribution
# from ifa.divergence import jsd

sys.path.append(os.path.dirname(os.path.realpath(__file__)))

import tweetstxt_basic
from pandas import Series
from numpy.testing import assert_allclose
# import seaborn as sns
from dateutil.parser import parse

sys.path.append(os.path.dirname(os.path.realpath(__file__)))
# import base_prediction
# import basictools

import entropy_estimator

import sklearn
import basics_prediction

from numpy import zeros, array
from math import sqrt, log

import gzip, json, pickle, numpy
from sklearn.cluster import KMeans
from sklearn import metrics
from sklearn import preprocessing
# from textblob import TextBlob
# from lxml import html
import requests
# from ehp import *
import twokenize

import nltk
from nltk.cluster.kmeans import KMeansClusterer

from HTMLParser import HTMLParser
# from htmlentitydefs import name2codepoint
#
class MyHTMLParser(HTMLParser):
    globflag = False
    def handle_starttag(self, tag, attrs):
        # out_list = []
        # return "Start tag:", tag
        try:
            for attr in attrs:
                try:
                    if 'article-link-category' in attr or 'claimReviewed' in attr or 'datePublished' in attr or 'author' in attr:
                        out_list.append(self.get_starttag_text())
                        # print(self.get_starttag_text())
                except:
                    continue
        except:
            print('something is wrong')


    def handle_decl(self, data):
        return "Decl     :", data
#
#


def plot_density_mean_median(dataframe_data, m_xlim=(-0.5, 0.5), m_ylim=[0, 10], m_color='r',
                             m_xlable='', m_ylable='',m_title='no_title', saving_path=''):

    mplpl.clf()
    mplpl.rc('xtick', labelsize='medium')
    mplpl.rc('ytick', labelsize='medium')
    mplpl.rc('xtick.major', size=3, pad=3)
    mplpl.rc('xtick.minor', size=2, pad=3)
    mplpl.rc('legend', fontsize='small')

    dataframe_data.plot(kind="density", xlim= m_xlim, color=m_color)
    # mplpl.plot(frange(-0.45, .5, 0.05), ave_ret_dist_cluster_hist, '*', color='orange')
    mplpl.vlines(dataframe_data.mean(), ymin=0, ymax=2, linewidth=2.0)
    mplpl.vlines(dataframe_data.median(), ymin=0, ymax=2, linewidth=2.0, color="red")


    mplpl.xlim([-1,1])
    mplpl.ylim(m_ylim)

    mplpl.xlabel(m_xlable, fontsize='large')
    mplpl.ylabel(m_ylable, fontsize='large')
    mplpl.title(m_title, fontsize='small')
    mplpl.legend(loc="upper right")


    mplpl.savefig(saving_path, format='png')

    # mplpl.show()


def plot_2_density_mean_median(dataframe_data, m_xlim=(-0.5, 0.5), m_ylim=[0, 10],
                             m_xlable='', m_ylable='',m_title='no_title', saving_path=''):

    mplpl.clf()
    mplpl.rc('xtick', labelsize='medium')
    mplpl.rc('ytick', labelsize='medium')
    mplpl.rc('xtick.major', size=3, pad=3)
    mplpl.rc('xtick.minor', size=2, pad=3)
    mplpl.rc('legend', fontsize='small')

    # col_1, col_2 = dataframe_data.columns
    # print(col_1)
    # print(col_2)
    #
    # df = dataframe_data.copy()
    # df_1 = df.drop(col_2,1)
    # df_2 = df.drop(col_2,1)
    try:
        dataframe_data.plot(kind="density", xlim= m_xlim)
        mplpl.vlines(dataframe_data.mean(), ymin=0, ymax=2, linewidth=2.0)
        mplpl.vlines(dataframe_data.median(), ymin=0, ymax=2, linewidth=2.0, color="red")



        mplpl.xlim([-1,1])
        mplpl.ylim(m_ylim)

        mplpl.xlabel(m_xlable, fontsize='large')
        # mplpl.ylabel(m_ylable, fontsize='large')
        mplpl.title(m_title, fontsize='small')
        mplpl.legend(loc="upper right")


        mplpl.savefig(saving_path, format='png')
    except:
        print ("there is something wrong with plotting")
    # mplpl.show()


def modify_demographic(dem_name):
    if dem_name == 'prefernotrespond':
        return 'prefer not respond'
    elif dem_name == 'living_with_partner':
        return 'living with partner'

    elif dem_name == 'collegegraduatebsbaorother4yeardegree':
        return 'college graduate bs ba or other 4 year degree'
    elif dem_name == 'technicaltradeorvocationalschoolafterhighschool':
        return 'technical trade or vocational school after highschool'
    elif dem_name == 'postgraduatetrainingorprofessionalschoolingaftercollegeegtowardamastersdegreeorphdlawormedicalschool':
        return 'post graduate training or professional schooling after college (e.g.toward a masters degree or phd law or medical school'
    elif dem_name == 'highschoolgraduategrade12orgedcertificate':
        return 'highschool graduate grade 12 or e.g. dcertificate'
    elif dem_name == 'somecollegeassociatedegreeno4yeardegree':
        return 'some college associate degree no 4 year degree'
    elif dem_name == '100001ndash150000':
        return '100001-150000'
    elif dem_name == '40001ndash50000':
        return '40001-50000'
    elif dem_name == '70001ndash100000':
        return '70001-100000'
    elif dem_name == '60001ndash70000':
        return '60001-70000'
    elif dem_name == '150001ormore':
        return '150001 or more'
    elif dem_name == '10000ndash20000':
        return '10000-20000'
    elif dem_name == '50001ndash60000':
        return '50001-60000'
    elif dem_name == '20001ndash30000':
        return '20001-30000'
    elif dem_name == 'under10000':
        return 'under 10000'
    elif dem_name == '30001ndash40000':
        return '30001-40000'
    elif dem_name == 'hispanicorlatino':
        return 'hispanic or latino'
    elif dem_name == 'americanindianoralaskanative':
        return 'american-indian or alaska-native'
    elif dem_name == 'other':
        return 'other'
    elif dem_name == 'asian':
        return 'asian'
    elif dem_name == 'blackorafricanamerican':
        return 'black or african-american'
    elif dem_name == 'prefernotrespond':
        return 'prefer not respond'
    elif dem_name ==  'nativehawaiianorotherpacificislander':
        return 'native hawaiian or other pacificis lander'
    elif dem_name == 'white':
        return 'white'
    elif dem_name == 'unitedstates':
        return 'united states'
    elif dem_name == 'canada':
        return 'canada'
    elif dem_name == 'unitedkingdom':
        return 'united kingdom'
    elif dem_name == 'unitedstates':
        return 'unitedstates'
    elif dem_name == '0.0':
        return '0.0'
    elif dem_name == '1.0':
        return '1.0'
    elif dem_name == '-1.0':
        return '-1.0'
    elif dem_name == '-10.0':
        return '-10.0'
    elif dem_name == '65-74':
        return '65-74'
    elif dem_name == '55-64':
        return '55-64'
    elif dem_name == '25-34':
     return '25-34'
    elif dem_name == '18-24':
     return '18-24'
    elif dem_name == '45-54':
        return '45-54'
    elif dem_name == '35-44':
        return '35-44'

    elif dem_name == 'male':
        return 'male'
    elif dem_name == 'female':
        return 'female'
    elif dem_name == 'liberal':
        return 'liberal'
    elif dem_name == 'veryconservative':
        return 'very conservative'
    elif dem_name == 'conservative':
        return 'conservative'
    elif dem_name == 'other':
        return 'other'
    elif dem_name == 'veryliberal':
        return 'very liberal'
    elif dem_name == 'moderate':
        return 'moderate'
    elif dem_name == 'separated':
        return 'separated'
    elif dem_name == 'widowed':
        return 'widowed'
    elif dem_name == 'divorced':
        return 'divorced'
    elif dem_name == 'married':
        return 'married'
    elif dem_name == 'living_with_partner':
        return 'living with partner'
    elif dem_name == 'single':
        return 'single'
    elif dem_name == 'infulltimeworkpermanent':
        return 'infull time work permanent'
    elif dem_name == 'retired':
        return 'retired'
    elif dem_name == 'infulltimeworktempcontract':
        return 'infull time work temp contract'
    elif dem_name == 'unemployed':
        return 'unemployed'
    elif dem_name == 'inparttimeworkpermanent':
        return 'inpart time work permanent'
    elif dem_name == 'inpart timeworktempcontract':
        return 'inparttime work temp contract'
    elif dem_name == 'studentonly':
        return 'student only'
    elif dem_name == 'parttimeworkparttimestudent':
        return 'part time work part time student'

    elif dem_name == 'selfemployed':
        return 'self employed'

def converting_demographic_num(dem_name):
    if dem_name == 'prefernotrespond':
        return 'prefer not respond'
    elif dem_name == 'living_with_partner':
        return 'living with partner'

    elif dem_name == 'collegegraduatebsbaorother4yeardegree':
        return 1 #'college graduate bs ba or other 4 year degree'
    elif dem_name == 'technicaltradeorvocationalschoolafterhighschool':
        return 2#'technical trade or vocational school after highschool'
    elif dem_name == 'postgraduatetrainingorprofessionalschoolingaftercollegeegtowardamastersdegreeorphdlawormedicalschool':
        return 3#'post graduate training or professional schooling after college (e.g.toward a masters degree or phd law or medical school'
    elif dem_name == 'highschoolgraduategrade12orgedcertificate':
        return 4#'highschool graduate grade 12 or e.g. dcertificate'
    elif dem_name == 'somecollegeassociatedegreeno4yeardegree':
        return 5#'some college associate degree no 4 year degree'
    elif dem_name == '100001ndash150000':
        return 1#'100001-150000'
    elif dem_name == '40001ndash50000':
        return 2#'40001-50000'
    elif dem_name == '70001ndash100000':
        return 3#'70001-100000'
    elif dem_name == '60001ndash70000':
        return '60001-70000'
    elif dem_name == '150001ormore':
        return '150001 or more'
    elif dem_name == '10000ndash20000':
        return '10000-20000'
    elif dem_name == '50001ndash60000':
        return '50001-60000'
    elif dem_name == '20001ndash30000':
        return '20001-30000'
    elif dem_name == 'under10000':
        return 'under 10000'
    elif dem_name == '30001ndash40000':
        return '30001-40000'
    elif dem_name == 'hispanicorlatino':
        return 'hispanic or latino'
    elif dem_name == 'americanindianoralaskanative':
        return 'american-indian or alaska-native'
    elif dem_name == 'other':
        return 'other'
    elif dem_name == 'asian':
        return 'asian'
    elif dem_name == 'blackorafricanamerican':
        return 'black or african-american'
    elif dem_name == 'prefernotrespond':
        return 'prefer not respond'
    elif dem_name ==  'nativehawaiianorotherpacificislander':
        return 'native hawaiian or other pacificis lander'
    elif dem_name == 'white':
        return 'white'
    elif dem_name == 'unitedstates':
        return 'united states'
    elif dem_name == 'canada':
        return 'canada'
    elif dem_name == 'unitedkingdom':
        return 'united kingdom'
    elif dem_name == 'unitedstates':
        return 'unitedstates'
    elif dem_name == '0.0':
        return '0.0'
    elif dem_name == '1.0':
        return '1.0'
    elif dem_name == '-1.0':
        return '-1.0'
    elif dem_name == '-10.0':
        return '-10.0'
    elif dem_name == '65-74':
        return '65-74'
    elif dem_name == '55-64':
        return '55-64'
    elif dem_name == '25-34':
     return '25-34'
    elif dem_name == '18-24':
     return '18-24'
    elif dem_name == '45-54':
        return '45-54'
    elif dem_name == '35-44':
        return '35-44'

    elif dem_name == 'male':
        return 'male'
    elif dem_name == 'female':
        return 'female'
    elif dem_name == 'liberal':
        return 'liberal'
    elif dem_name == 'veryconservative':
        return 'very conservative'
    elif dem_name == 'conservative':
        return 'conservative'
    elif dem_name == 'other':
        return 'other'
    elif dem_name == 'veryliberal':
        return 'very liberal'
    elif dem_name == 'moderate':
        return 'moderate'
    elif dem_name == 'separated':
        return 'separated'
    elif dem_name == 'widowed':
        return 'widowed'
    elif dem_name == 'divorced':
        return 'divorced'
    elif dem_name == 'married':
        return 'married'
    elif dem_name == 'living_with_partner':
        return 'living with partner'
    elif dem_name == 'single':
        return 'single'
    elif dem_name == 'infulltimeworkpermanent':
        return 'infull time work permanent'
    elif dem_name == 'retired':
        return 'retired'
    elif dem_name == 'infulltimeworktempcontract':
        return 'infull time work temp contract'
    elif dem_name == 'unemployed':
        return 'unemployed'
    elif dem_name == 'inparttimeworkpermanent':
        return 'inpart time work permanent'
    elif dem_name == 'inpart timeworktempcontract':
        return 'inparttime work temp contract'
    elif dem_name == 'studentonly':
        return 'student only'
    elif dem_name == 'parttimeworkparttimestudent':
        return 'part time work part time student'

    elif dem_name == 'selfemployed':
        return 'self employed'

def my_KS_distance(u, v):
    """
    Returns 1 minus the cosine of the angle between vectors v and u. This is
    equal to 1 - (u.v / |u||v|).
    """
    # output = 1 - (numpy.dot(u, v) / (
    #             sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))
    output = float(scipy.stats.ks_2samp(u, v)[0])
    return output


def m_clusterDemographics(demographicsDict, filename):
    for i in demographicsDict:
        total = sum(demographicsDict[i])
        demographicsDict[i] = [j / float(total) for j in demographicsDict[i]]

    demDist = []
    for i in sorted(demographicsDict.keys()):
        demDist.append(demographicsDict[i])

    X = numpy.array(demDist)

    # # To know the optimum number of clusters K,
    # # Use Silhouette Score (https://en.wikipedia.org/wiki/Silhouette_(clustering))
    #
    fp = open("silhouette_score_" + filename + ".txt", "w")
    fp.write("Number_of_Clusters (K) \\t Silhouette_Score \n")
    for numCluster in range(2, 11):
        kmeans = KMeans(n_clusters=numCluster, n_jobs=10).fit(X)
        # fp.write("{0}\t{1}\n".format(numCluster, metrics.silhouette_score(X, kmeans.labels_, metric='euclidean')))
        fp.write("{0}\t{1}\n".format(numCluster, metrics.silhouette_score(X, kmeans.labels_, metric='euclidean')))
    fp.close()
    #

    # """
    numCluster = 2
    # kclusterer = KMeansClusterer(numCluster, distance= np.mean, repeats=25)
    # kclusterer = KMeansClusterer(numCluster, distance= scipy.stats.ks_2samp, repeats=25)
    # kclusterer = KMeansClusterer(numCluster, distance= my_KS_distance, repeats=10)
    kclusterer = KMeansClusterer(numCluster, distance=my_KS_distance, conv_test=100000, avoid_empty_clusters=True,
                                 repeats=25)
    assigned_clusters = kclusterer.cluster(X, assign_clusters=True)

    clusterMemberDict = {}
    for i in range(numCluster):
        clusterMemberDict[i] = []
        for j in range(len(assigned_clusters)):
            if assigned_clusters[j] == i:
                clusterMemberDict[i].append(sorted(demographicsDict.keys())[j])
    return clusterMemberDict


# def normalized_max_min_funct(demographicsDict):
#     dict_tolist = [demographicsDict[i] for i in demographicsDict]
#     df = pd.DataFrame(dict_tolist, index=demographicsDict.keys())
#     for i in df.columns.tolist():
#         df[i] = [(el - np.min(df[i])) / float(np.max(df[i]) - np.min(df[i])) for el in df[i]]
#     for i in demographicsDict:
#         demographicsDict[i] = df.loc[i]
#     return (demographicsDict)


def normalized_max_min_funct(demographicsDict):
    output = []
    min_value = np.min(demographicsDict)
    max_value = np.max(demographicsDict)
    m = scipy.interpolate.interp1d([min_value, max_value], [-1,1] )
    for el in demographicsDict:
        output.append(m(el))
        # output.append((el - min_value) / float(max_value - min_value))
    return (output)


def normalized_max_min_funct_sikitlearn(demographicsDict):
    min_max_scaler = sklearn.preprocessing.MinMaxScaler()
    X_train_minmax = min_max_scaler.fit_transform(demographicsDict)

    return (X_train_minmax)


def rec_value_set(features_init_num=[0] * 3, value=[0, 1]):
    out_lists = []
    if len(features_init_num) == 1:
        for val_e in value:
            out_lists.append([val_e])
        return out_lists
    tmp_out_list = []
    for val in value:
        # tmp_list = []
        tmp_list = [val]
        len_list = len(features_init_num) - 1
        out_lists = rec_value_set(features_init_num=[0] * len_list, value=[0, 1])
        for m_list in out_lists:
            tmp_out_list.append(tmp_list + m_list)
    return tmp_out_list


def initial_manually(num_feat, features_init_num, value):
    initial_list = []
    res_lists = rec_value_set(features_init_num, value)
    num_arr = math.pow(len(value), len(features_init_num))
    for res_list in res_lists:
        tmp_list = [0] * num_feat
        for el in features_init_num:
            tmp_list[el - 1] = res_list[el - 1]
        initial_list.append(tmp_list)

    return initial_list


def clusterDemographics(demographicsDict, filename, number_clusters, initial_list=[], normalization_clustering=''):
    # for i in demographicsDict:
    #     total = sum(demographicsDict[i])
    #     demographicsDict[i] = [j / float(total) for j in demographicsDict[i]]
    #
    # demographicsDict = normalized_max_min_funct(demographicsDict)


    demDist = []
    for i in sorted(demographicsDict.keys()):
        demDist.append(demographicsDict[i])

    if normalization_clustering == 'minamax':
        demDist = normalized_max_min_funct_sikitlearn(demDist)

    X = numpy.array(demDist)

    # To know the optimum number of clusters K,
    # Use Silhouette Score (https://en.wikipedia.org/wiki/Silhouette_(clustering))

    # fp = open("publisher_minmax_initiallist_silhouette_score_"+filename+".txt", "w")
    # # fp = open("publisher_silhouette_score_"+filename+".txt", "w")
    # print('best k for k-means : ' + "publisher_silhouette_score_"+filename+".txt")
    # fp.write("Number_of_Clusters (K) \\t Silhouette_Score \n")
    # for numCluster in range(2, 11):
    #     kmeans = KMeans(n_clusters=numCluster, n_jobs=10).fit(X)
    #     # fp.write("{0}\t{1}\n".format(numCluster, metrics.silhouette_score(X, kmeans.labels_, metric='euclidean')))
    #     fp.write("||{0}||\t{1}||\n".format(numCluster, metrics.silhouette_score(X, kmeans.labels_, metric='euclidean')))
    # fp.close()
    #

    # """
    numCluster = number_clusters
    if len(initial_list) == 0:
        print('initial_list is empty')
        kmeans = KMeans(n_clusters=numCluster, n_init=100, n_jobs=100).fit(X)
        precision = metrics.silhouette_score(X, kmeans.labels_, metric='euclidean')
        print(precision)
    else:
        kmeans = KMeans(n_clusters=numCluster, n_init=1, init=numpy.array(initial_list), n_jobs=10).fit(X)
        print(metrics.silhouette_score(X, kmeans.labels_, metric='euclidean'))

    clusterMemberDict = {}
    # print(kmeans.cluster_centers_)
    for center in kmeans.cluster_centers_ :
        print('||')
        for cent_i in center:
            print(str(cent_i) + '||')
        # print('\n')
    for i in range(numCluster):
        clusterMemberDict[i] = []
        for j in range(len(kmeans.labels_)):
            if kmeans.labels_[j] == i:
                clusterMemberDict[i].append(sorted(demographicsDict.keys())[j])
    return clusterMemberDict, kmeans.cluster_centers_, precision


def computeDistanceBetweenDistributions(dist1, dist2):
    chiSquareDistance = 0.0
    for i in range(len(dist1)):
        val1 = dist1[i]
        val2 = dist2[i]
        chiSquareDistance += ((val1 - val2) ** 2) / (val1 + val2)
    chiSquareDistance /= 2.0

    return chiSquareDistance


def chi_sqr(dist1, dist2):
    chiSquareDistance = 0.0
    for i in range(len(dist1)):
        val1 = dist1[i]
        val2 = dist2[i]
        if val1 + val2 == 0:
            chiSquareDistance += 0
        else:
            chiSquareDistance += ((val1 - val2) ** 2) / (val1 + val2)
    chiSquareDistance /= 2.0

    return chiSquareDistance


class JSD(object):
    def __init__(self):
        self.log2 = log(2)

    def KL_divergence(self, p, q):
        """ Compute KL divergence of two vectors, K(p || q)."""
        return sum(p[x] * log((p[x]) / (q[x])) for x in range(len(p)) if p[x] != 0.0 or p[x] != 0)

    def Jensen_Shannon_divergence(self, p, q):
        """ Returns the Jensen-Shannon divergence. """
        self.JSD = 0.0
        weight = 0.5
        average = zeros(len(p))  #Average
        for x in range(len(p)):
            average[x] = weight * p[x] + (1 - weight) * q[x]
            self.JSD = (weight * self.KL_divergence(array(p), average)) + (
            (1 - weight) * self.KL_divergence(array(q), average))
        return 1 - (self.JSD / sqrt(2 * self.log2))


def preparing_output_replys_for_wiki(inp_txt, tweet_source):
    if len(inp_txt) > 2:
        output_str = '[[https://twitter.com/' + inp_txt.replace('\n', " ").split('<<**>>')[1] + '|' \
                     + inp_txt.replace('\n', "").split('<<**>>')[1] + ']]:' + '[[https://twitter.com/' \
                     + tweet_source + '/status/' \
                     + inp_txt.replace('\n', "").split('<<**>>')[2] + '|*' \
                     + inp_txt.replace('\n', "").split('<<**>>')[3].replace('\n', "") + ']]'
    else:
        output_str = ''
    return output_str


def frange(start, stop, step):
    i = start
    res_list = []
    while i < stop:
        res_list.append(i)
        i += step
    return res_list[:]


def plot_diff_dist(inp_list1, inp_list2, label_1, label_2, name):
    if 'controversial' in name:
        print"test"
    mplpl.clf()
    mplpl.rc('xtick', labelsize='medium')
    mplpl.rc('ytick', labelsize='medium')
    mplpl.rc('xtick.major', size=3, pad=3)
    mplpl.rc('xtick.minor', size=2, pad=3)
    mplpl.rc('legend', fontsize='small')  # for tweet_id in df_num_retweet_sorted['tweet_id']:

    # target_users_list = [factual_tweet_list, opinion_tweet_list]#, controversial_tweet_list,noncontroversial_tweet_list,
    #informative_tweet_list, noninformative_tweet_list]
    if len(inp_list1) > 1:
        if 'KL' not in name and 'js' not in name:
            counts, bin_edges = np.histogram(inp_list1, bins=np.arange(0, 1, 0.05), normed=True)
        else:
            counts, bin_edges = np.histogram(inp_list1, bins=np.arange(0, max(inp_list1), max(inp_list1) / 20),
                                             normed=True)

        cdf = np.cumsum(counts)
        cdf = cdf / cdf[-1]
        Plab.plot(bin_edges[0:-1], cdf, color='r', label=label_1, lw=4)

        if 'KL' not in name and 'js' not in name:
            counts, bin_edges = np.histogram(inp_list2, bins=np.arange(0, 1, 0.05), normed=True)
        else:
            counts, bin_edges = np.histogram(inp_list2, bins=np.arange(0, max(inp_list2), max(inp_list2) / 20),
                                             normed=True)
        cdf = np.cumsum(counts)
        cdf = cdf / cdf[-1]

        Plab.plot(bin_edges[0:-1], cdf, color='b', label=label_2, lw=4)

        # mplpl.hist(inp_list1, normed=1, histtype='step', cumulative=True, color='r',label=label_1)
        # mplpl.hist(inp_list2, normed=1, histtype='step', cumulative=True, color='b', label=label_2)

        mplpl.xlabel('', fontsize='large')
        mplpl.ylabel('CDF', fontsize='large')
        mplpl.title('', fontsize='large')

        mplpl.legend(loc="upper left")
        mplpl.ylim([0, 1])
        if 'KL' not in name and 'js' not in name:
            mplpl.xlim([0, 1])
        else:
            mplpl.xlim(0, max(bin_edges))

        pp = remotedir + '/new_data_set/fig/distribution_fig/'
        if not os.path.exists(pp): os.makedirs(pp)

        mplpl.savefig(pp + name, format='png')


def plot_diff_dist_1(inp_list1, inp_list2, inp_list3, inp_list4, label_1, label_2, label_3, label_4, name):
    if 'controversial' in name:
        print"test"
    mplpl.clf()
    mplpl.rc('xtick', labelsize='medium')
    mplpl.rc('ytick', labelsize='medium')
    mplpl.rc('xtick.major', size=3, pad=3)
    mplpl.rc('xtick.minor', size=2, pad=3)
    mplpl.rc('legend', fontsize='small')  # for tweet_id in df_num_retweet_sorted['tweet_id']:

    # target_users_list = [factual_tweet_list, opinion_tweet_list]#, controversial_tweet_list,noncontroversial_tweet_list,
    # informative_tweet_list, noninformative_tweet_list]
    if len(inp_list1) > 1:
        if 'KL' not in name and 'js' not in name:
            counts, bin_edges = np.histogram(inp_list1, bins=np.arange(0, 1, 0.05), normed=True)
        else:
            counts, bin_edges = np.histogram(inp_list1, bins=np.arange(0, max(inp_list1), max(inp_list1) / 20),
                                             normed=True)

        cdf = np.cumsum(counts)
        cdf = cdf / cdf[-1]
        Plab.plot(bin_edges[0:-1], cdf, color='r', label=label_1, lw=4)

        if 'KL' not in name and 'js' not in name:
            counts, bin_edges = np.histogram(inp_list2, bins=np.arange(0, 1, 0.05), normed=True)
        else:
            counts, bin_edges = np.histogram(inp_list2, bins=np.arange(0, max(inp_list2), max(inp_list2) / 20),
                                             normed=True)
        cdf = np.cumsum(counts)
        cdf = cdf / cdf[-1]

        Plab.plot(bin_edges[0:-1], cdf, color='b', label=label_2, lw=4)

        # mplpl.hist(inp_list1, normed=1, histtype='step', cumulative=True, color='r',label=label_1)
        # mplpl.hist(inp_list2, normed=1, histtype='step', cumulative=True, color='b', label=label_2)

        mplpl.xlabel('', fontsize='large')
        mplpl.ylabel('CDF', fontsize='large')
        mplpl.title('', fontsize='large')

        mplpl.legend(loc="upper left")
        mplpl.ylim([0, 1])
        if 'KL' not in name and 'js' not in name:
            mplpl.xlim([0, 1])
        else:
            mplpl.xlim(0, max(bin_edges))

        if 'KL' not in name and 'js' not in name:
            counts, bin_edges = np.histogram(inp_list3, bins=np.arange(0, 1, 0.05), normed=True)
        else:
            counts, bin_edges = np.histogram(inp_list3, bins=np.arange(0, max(inp_list3), max(inp_list3) / 20),
                                             normed=True)

        cdf = np.cumsum(counts)
        cdf = cdf / cdf[-1]
        Plab.plot(bin_edges[0:-1], cdf, color='g', label=label_3, lw=4)

        if 'KL' not in name and 'js' not in name:
            counts, bin_edges = np.histogram(inp_list4, bins=np.arange(0, 1, 0.05), normed=True)
        else:
            counts, bin_edges = np.histogram(inp_list4, bins=np.arange(0, max(inp_list4), max(inp_list4) / 20),
                                             normed=True)
        cdf = np.cumsum(counts)
        cdf = cdf / cdf[-1]

        Plab.plot(bin_edges[0:-1], cdf, color='k', label=label_4, lw=4)

        # mplpl.hist(inp_list1, normed=1, histtype='step', cumulative=True, color='r',label=label_1)
        # mplpl.hist(inp_list2, normed=1, histtype='step', cumulative=True, color='b', label=label_2)

        mplpl.xlabel('', fontsize='large')
        mplpl.ylabel('CDF', fontsize='large')
        mplpl.title('', fontsize='large')

        mplpl.legend(loc="upper left")
        mplpl.ylim([0, 1])
        if 'KL' not in name and 'js' not in name:
            mplpl.xlim([0, 1])
        else:
            mplpl.xlim(0, max(bin_edges))

        pp = remotedir + '/new_data_set/fig/distribution_fig/'
        if not os.path.exists(pp): os.makedirs(pp)

        mplpl.savefig(pp + name, format='png')


def plot_fig(tweet_dist_foll_score_dict, source_id, m_xlabel, m_ylabel, m_title, save_location):
    mplpl.clf()
    mplpl.rc('xtick', labelsize='medium')
    mplpl.rc('ytick', labelsize='medium')
    mplpl.rc('xtick.major', size=3, pad=3)
    mplpl.rc('xtick.minor', size=2, pad=3)
    mplpl.rc('legend', fontsize='small')  # for tweet_id in df_num_retweet_sorted['tweet_id']:
    # for tweet_id in df_score_sorted['tweet_id']:
    source_id = df['tweet_source_id'][index]
    tweet_id = df['tweet_id'][index]


    # n_foll, bins, patches = Plab.hist(tweet_dist_foll_score_dict[str(source_id)]
    #                                   , bins = frange(-0.5, 0.5, 0.05), normed=0)
    # mplpl.hist(tweet_dist_foll_score_dict[str(source_id)], 20, normed=1,color='green')
    skewed_data = pd.DataFrame(tweet_dist_foll_score_dict[str(source_id)])

    skewed_data.plot(kind="density", xlim=(-0.5, 0.5), color="green")
    # figsize=(10,10),
    # xlim=(-1,5))


    mplpl.vlines(skewed_data.mean(),  # Plot black line at mean
                 ymin=0,
                 ymax=2,
                 linewidth=2.0)

    mplpl.vlines(skewed_data.median(),  # Plot red line at median
                 ymin=0,
                 ymax=2,
                 linewidth=2.0,
                 color="red")

    # mplpl.xlabel('retweeters_polarization', fontsize='large')
    mplpl.xlabel(m_xlabel, fontsize='small')
    # + '\n r num rep : ' + str(np.round(df['rep_num_ret_1'][index], 4))
    # + ', r num dem : ' + str(np.round(df['dem_num_ret_1'][index], 4)), fontsize='small')
    # mplpl.ylabel('relative # of replies', fontsize='large')
    mplpl.ylabel(m_ylabel, fontsize='large')
    mplpl.title(m_title, fontsize='small')
    # +'\n sorted based on : ' + str(sorted_based_on),fontsize='small')

    # mplpl.legend(loc="lower left")
    mplpl.legend(loc="upper right")
    mplpl.ylim([0, 25])
    mplpl.xlim([-0.5, 0.5])

    pp = save_location

    mplpl.savefig(pp, format='png')


def plot_fig_list(tweet_list_value, m_xlabel, m_ylabel, m_title, save_location):
    mplpl.clf()
    mplpl.rc('xtick', labelsize='medium')
    mplpl.rc('ytick', labelsize='medium')
    mplpl.rc('xtick.major', size=3, pad=3)
    mplpl.rc('xtick.minor', size=2, pad=3)
    mplpl.rc('legend', fontsize='small')  # for tweet_id in df_num_retweet_sorted['tweet_id']:


    # n_foll, bins, patches = Plab.hist(tweet_dist_foll_score_dict[str(source_id)]
    #                                   , bins = frange(-0.5, 0.5, 0.05), normed=0)
    mplpl.hist(tweet_list_value, np.logspace(0, 4, 100), normed=1, histtype='step', cumulative='True', color='green')

    # , normed = 1, histtype = 'step', cumulative = True,
    mplpl.xlabel(m_xlabel, fontsize='small')

    mplpl.ylabel(m_ylabel, fontsize='large')
    mplpl.title(m_title, fontsize='small')

    mplpl.xscale('log')
    mplpl.yscale('log')

    # mplpl.legend(loc="lower left")
    mplpl.legend(loc="upper right")
    # mplpl.ylim([0, 25])
    # mplpl.xlim([-0.5, 0.5])

    pp = save_location

    mplpl.savefig(pp, format='png')


def replace_outliers_mean(n_dist):
    out = []
    tmp_dict = collections.defaultdict()
    i = 0
    for el in n_dist:
        tmp_dict[i] = el
        i += 1

    previous_v = 0
    dist_mean = np.mean(n_dist)
    Thresh = 5
    count = 0
    previous_el = 0
    for dist_el in range(0, len(n_dist)):
        if tmp_dict[dist_el] / dist_mean > Thresh:
            tmp_dict[dist_el] = np.max([previous_v, dist_mean])
        out.append(tmp_dict[dist_el])
        previous_v = tmp_dict[dist_el]
    return out


def plot_cdf(dist_list, x_label, y_label, title, save_location):
    mplpl.clf()
    mplpl.rc('xtick', labelsize='medium')
    mplpl.rc('ytick', labelsize='medium')
    mplpl.rc('xtick.major', size=3, pad=3)
    mplpl.rc('xtick.minor', size=2, pad=3)
    mplpl.rc('legend', fontsize='small')  # for tweet_id in df_num_retweet_sorted['tweet_id']:

    list_sorted = np.sort(dist_list)
    sum_l = np.sum(list_sorted)
    norm_list = [x / float(sum_l) for x in list_sorted]
    cdf_list = []
    cdf_v = 0
    for el in norm_list:
        cdf_v += el
        cdf_list.append(cdf_v)

    mplpl.plot(cdf_list, color='r')

    mplpl.xlabel(x_label, fontsize='small')

    mplpl.ylabel(y_label, fontsize='large')
    mplpl.title(title, fontsize='small')

    # mplpl.xscale('log')
    # mplpl.yscale('log')

    # mplpl.legend(loc="lower left")
    mplpl.legend(loc="upper right")
    # mplpl.ylim([0, 25])
    # mplpl.xlim([-0.5, 0.5])

    pp = save_location

    mplpl.savefig(pp, format='png')


def remove_outliers_dist(n_dist):
    out = []
    tmp_dict = collections.defaultdict()

    k_top = 2

    i = 0
    for el in n_dist:
        tmp_dict[i] = el
        i += 1
    i = 0

    dist_sorted = sorted(tmp_dict, key=tmp_dict.get, reverse=True)
    dist_mean = np.mean(n_dist)
    count = 0
    previous_el = 0
    dist_replace_val = np.max([tmp_dict[x] for x in dist_sorted[k_top:]])
    for dist_el in dist_sorted:
        tmp_dict[dist_el] = dist_replace_val
        if count >= k_top:
            break
        count += 1

    for dist_el in range(0, len(n_dist)):
        out.append(tmp_dict[dist_el])

    return out


def AMT_tweet_cont_fact_scores():
    processing_data = 'media_sites'

    local_dir_saving = '/preprocess/new_data_set/'
    remotedir = '/home/babaei/Desktop/SVN/streaming_api/with_tweepy/fake_news_data'

    pp = remotedir + local_dir_saving
    inpFtmp = open(pp + 'wiki_output_diff_ks_with_results.txt', 'r')
    outputFile = open(pp + 'set_0', 'w')
    count = 0
    cc = 0
    out_list = []
    ks_dict_tweet = collections.defaultdict()
    tweet_source_dict = collections.defaultdict()
    tweet_text_dict = collections.defaultdict()
    AMT_tweet_id_list = []
    for line in inpFtmp:
        try:
            if '==' in line or '||Index||tweet||tweet_id||source||' in line:
                # outputFile.write(line)
                count += 1
                continue
            if count < 7:
                continue
            cc += 1
            line = line.replace('\n', '')
            line_splt = line.split('||')
            tweet = line_splt[2]
            tweet_txt = tweet.split('<<BR>>')[0]
            tweet_link = tweet.split('<<BR>>')[1]
            tweet_link = tweet_link.replace('[[', '')
            tweet_link = tweet_link.replace(']]', '')
            tweet_link = tweet_link.split('|')[0]
            tweet_id = int(line_splt[3])
            source = tweet_link.split('/')[3].replace('@', '')
            ks_value = float(line_splt[6])

            tmp_list = [tweet_id, source, tweet_txt, tweet_link]
            ks_dict_tweet[tweet_id] = ks_value
            tweet_source_dict[tweet_id] = source
            tweet_text_dict[tweet_id] = tweet_txt
            AMT_tweet_id_list.append(tweet_id)
            # out_list.append(tmp_list)
        except:
            continue

    tweet_sorted_ks = sorted(ks_dict_tweet, key=ks_dict_tweet.get, reverse=False)
    tweets_ks_values_sorted = []
    for tweet_e in tweet_sorted_ks:
        tweets_ks_values_sorted.append(np.round(ks_dict_tweet[tweet_e], 3))

    # valid users dict
    Vuser_dict = collections.defaultdict(int)
    # query1 = "SELECT text from all_expert_tweets_parsed WHERE tweet_id = %d"%tweet_id
    query1 = "select workerid, count(*) from mturk_crowd_signals_tweet_response_1 group by workerid;"

    cursor.execute(query1)
    res_all = cursor.fetchall()
    for el in res_all:
        if el[1] == 21:
            Vuser_dict[el[0]] = 1

    query2 = "select workerid, tweet_id, ra, rb, rc, text from mturk_crowd_signals_tweet_response_1;"

    cursor.execute(query2)
    res_all = cursor.fetchall()

    query3 = "select workerid, ra from mturk_crowd_signals_tweet_response_1 where tweet_id=1;"

    cursor.execute(query3)
    res_leaning = cursor.fetchall()
    leaning_dict = collections.defaultdict()
    for el in res_leaning:
        leaning_dict[el[0]] = el[1]

    workerid_list = collections.defaultdict(list)
    tweetid_list = collections.defaultdict(list)
    ra_list = collections.defaultdict(list)
    rb_list = collections.defaultdict(list)
    rc_list = collections.defaultdict(list)
    txt_list = collections.defaultdict(list)
    index_list = collections.defaultdict(list)

    workerid_list_all = collections.defaultdict(list)
    tweetid_list_all = collections.defaultdict(list)
    ra_list_all = collections.defaultdict(list)
    rb_list_all = collections.defaultdict(list)
    rc_list_all = collections.defaultdict(list)
    txt_list_all = collections.defaultdict(list)
    index_list_all = collections.defaultdict(list)

    workerid_list_m = []
    tweetid_list_m = []
    ra_list_m = []
    rb_list_m = []
    rc_list_m = []
    txt_list_m = []
    index_list_m = []
    user_leaning_list_m = []
    count = 0
    # df_category = [0,1,2,3]*[1 as with author and 2 without author]
    category_author_laning = ['withoutauthor_lean_1', 'withauthor_lean_1', 'withourauthor_lean2', 'withauthor_lean2',
                              'withoutauthor_lean3', 'withauthor_lean3', 'withoutauthor_lean4', 'withauthor_lean4']
    for el in res_all:
        if el[0] in Vuser_dict:
            workerid = int(el[0])
            tweetid = int(el[1])
            ra = int(el[2])
            rb = int(el[3])
            rc = int(el[4])
            if tweetid != 1:
                ra = ra % 2
                rb = rb % 2
                rc = rc % 2
            txt = el[5]

            user_leaning = leaning_dict[el[0]]

            if workerid % 2 == 1:
                var_sum = 1
            else:
                var_sum = 0
            user_leaning_1 = user_leaning * 2 + var_sum
            workerid_list[user_leaning_1].append(workerid)
            tweetid_list[user_leaning_1].append(tweetid)
            ra_list[user_leaning_1].append(ra)
            rb_list[user_leaning_1].append(rb)
            rc_list[user_leaning_1].append(rc)
            txt_list[user_leaning_1].append(txt)
            index_list[user_leaning_1].append(count)

            workerid_list_all[user_leaning].append(workerid)
            tweetid_list_all[user_leaning].append(tweetid)
            ra_list_all[user_leaning].append(ra)
            rb_list_all[user_leaning].append(rb)
            rc_list_all[user_leaning].append(rc)
            txt_list_all[user_leaning].append(txt)
            index_list_all[user_leaning].append(count)

            workerid_list_m.append(workerid)
            tweetid_list_m.append(tweetid)
            ra_list_m.append(ra)
            rb_list_m.append(rb)
            rc_list_m.append(rc)
            txt_list_m.append(txt)
            index_list_m.append(count)
            user_leaning_list_m.append(user_leaning)
            count += 1

    index_list_m = range(len(workerid_list_m))
    df = pd.DataFrame({'workerid': Series(workerid_list_m, index=index_list_m),
                       'tweetid': Series(tweetid_list_m, index=index_list_m),
                       'ra': Series(ra_list_m, index=index_list_m),
                       'rb': Series(rb_list_m, index=index_list_m),
                       'rc': Series(rc_list_m, index=index_list_m),
                       'text': Series(txt_list_m, index=index_list_m),
                       'leaning': Series(user_leaning_list_m, index=index_list_m)})

    df_cat = collections.defaultdict()
    for i in range(0, 8):
        index_list[i] = range(len(workerid_list[i]))
        df_cat[i] = pd.DataFrame({'workerid': Series(workerid_list[i], index=index_list[i]),
                                  'tweetid': Series(tweetid_list[i], index=index_list[i]),
                                  'ra': Series(ra_list[i], index=index_list[i]),
                                  'rb': Series(rb_list[i], index=index_list[i]),
                                  'rc': Series(rc_list[i], index=index_list[i]),
                                  'text': Series(txt_list[i], index=index_list[i])})
        # 'leaning': Series(user_leaning_list[i], index=index_list_m)})

    df_all_aut_cat = collections.defaultdict()
    for i in range(1, 4):
        index_list_all[i] = range(len(workerid_list_all[i]))
        df_all_aut_cat[i] = pd.DataFrame({'workerid': Series(workerid_list_all[i], index=index_list_all[i]),
                                          'tweetid': Series(tweetid_list_all[i], index=index_list_all[i]),
                                          'ra': Series(ra_list_all[i], index=index_list_all[i]),
                                          'rb': Series(rb_list_all[i], index=index_list_all[i]),
                                          'rc': Series(rc_list_all[i], index=index_list_all[i]),
                                          'text': Series(txt_list_all[i], index=index_list_all[i])})

    with_author = 0;
    without_author = 0;
    workerid_set = set(df['workerid'])
    with_author_list = []
    without_author_list = []
    for wid in workerid_set:
        if wid % 2 == 1:
            with_author += 1
            with_author_list.append(wid)
        else:
            without_author += 1
            without_author_list.append(wid)
    print('number of workers who sees the surveys without author is : ' + str(without_author))
    print('number of workers who sees the surveys with author is : ' + str(with_author))

    df['ra'][df['ra'] == 2] = 0
    df['rb'][df['rb'] == 2] = 0
    df['rc'][df['rc'] == 2] = 0

    df_author = df[df['workerid'] % 2 == 1]
    df_w_author = df[df['workerid'] % 2 == 0]

    groupby_ftr = 'tweetid'
    grouped = df.groupby(groupby_ftr, sort=False)
    sum_feat = df.groupby(groupby_ftr).sum()

    category_author_laning = ['withoutauthor_dem', 'withauthor_dem', 'withourauthor_rep', 'withauthor_rep',
                              'withoutauthor_neut', 'withauthor_neut', 'withoutauthor_dont_know',
                              'withauthor_dont_know']

    tweet_id_ra_dict = collections.defaultdict(float)
    tweet_id_rb_dict = collections.defaultdict(float)
    tweet_id_rc_dict = collections.defaultdict(float)

    groupby_ftr = 'tweetid'
    grouped = df.groupby(groupby_ftr, sort=False)
    sum_feat = df.groupby(groupby_ftr).sum()
    count_feat = df.groupby(groupby_ftr).count()
    outF = open(remotedir + local_dir_saving + 'amt_output_wiki_clustering.txt', 'w')
    for tweet_id_e in tweet_sorted_ks:
        relative_ra = float(sum_feat['ra'][tweet_id_e]) / count_feat['ra'][tweet_id_e]
        relative_rb = float(sum_feat['rb'][tweet_id_e]) / count_feat['rb'][tweet_id_e]
        relative_rc = float(sum_feat['rc'][tweet_id_e]) / count_feat['rc'][tweet_id_e]
        tweet_id_ra_dict[tweet_id_e] = relative_ra
        tweet_id_rb_dict[tweet_id_e] = relative_rb
        tweet_id_rc_dict[tweet_id_e] = relative_rc

        ##############################
    author = 0
    ##############################


    tweet_id_ra_dict_w = collections.defaultdict(float)
    tweet_id_rb_dict_w = collections.defaultdict(float)
    tweet_id_rc_dict_w = collections.defaultdict(float)

    tweet_id_ra_dict_nw = collections.defaultdict(float)
    tweet_id_rb_dict_nw = collections.defaultdict(float)
    tweet_id_rc_dict_nw = collections.defaultdict(float)

    groupby_ftr = 'tweetid'
    grouped = df_author.groupby(groupby_ftr, sort=False)
    sum_feat = df_author.groupby(groupby_ftr).sum()
    count_feat = df_author.groupby(groupby_ftr).count()
    for tweet_id_e in tweet_sorted_ks:
        tweet_id_ra_dict_w[tweet_id_e] = float(sum_feat['ra'][tweet_id_e]) / count_feat['ra'][tweet_id_e]
        tweet_id_rb_dict_w[tweet_id_e] = float(sum_feat['rb'][tweet_id_e]) / count_feat['rb'][tweet_id_e]
        tweet_id_rc_dict_w[tweet_id_e] = float(sum_feat['rc'][tweet_id_e]) / count_feat['rc'][tweet_id_e]

    groupby_ftr = 'tweetid'
    grouped = df_w_author.groupby(groupby_ftr, sort=False)
    sum_feat = df_w_author.groupby(groupby_ftr).sum()
    count_feat = df_w_author.groupby(groupby_ftr).count()

    for tweet_id_e in tweet_sorted_ks:
        tweet_id_ra_dict_nw[tweet_id_e] = float(sum_feat['ra'][tweet_id_e]) / count_feat['ra'][tweet_id_e]
        tweet_id_rb_dict_nw[tweet_id_e] = float(sum_feat['rb'][tweet_id_e]) / count_feat['rb'][tweet_id_e]
        tweet_id_rc_dict_nw[tweet_id_e] = float(sum_feat['rc'][tweet_id_e]) / count_feat['rc'][tweet_id_e]

    return [tweet_id_ra_dict, tweet_id_rb_dict, tweet_id_rc_dict, tweet_id_ra_dict_w, tweet_id_rb_dict_w,
            tweet_id_rc_dict_w, tweet_id_ra_dict_nw, tweet_id_rb_dict_nw, tweet_id_rc_dict_nw, AMT_tweet_id_list]

# ================================ main

if __name__ == '__main__':
    global globflag
    global out_list
    out_list = []
    # currently we are testing these arguments
    # for simplicity and accesibility we list them here
    if len(sys.argv) == 1:
        # sys.argv += ["-t","create_db"]

        # sys.argv += ["-t", "prepare_AMT_dataset_publishers_tweets_audiences_polarization_consensus"]
        # sys.argv+=["-t", "prepare_AMT_dataset_reliable_news"]
        # sys.argv+=["-t", "AMT_dataset_reliable_news_processing"]
        # sys.argv += ["-t", "AMT_dataset_reliable_news_processing_diff_topic"]
        # sys.argv+=["-t", "AMT_dataset_reliable_news_processing_exp2"]
        # sys.argv+=["-t", "AMT_dataset_reliable_news_processing_exp2_weighted"]
        # sys.argv += ["-t", "AMT_dataset_reliable_news_processing_snopes_weighted"]
        # sys.argv += ["-t", "AMT_dataset_reliable_news_processing_all_dataset_weighted"]
        # sys.argv += ["-t", "AMT_dataset_reliable_news_processing_snopes_weighted_visualisation"]
        # sys.argv += ["-t", "AMT_dataset_reliable_news_processing_all_dataset_weighted_time_analysis"]
        # sys.argv += ["-t", "AMT_dataset_reliable_news_processing_snopes_weighted_demographic_visualisation"]
        # sys.argv += ["-t", "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation"]
        # sys.argv += ["-t", "AMT_dataset_reliable_news_processing_all_data_weighted_demographic_visualisation"]
        # sys.argv += ["-t", "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_corr"]
        # sys.argv += ["-t","AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics"]
        # sys.argv += ["-t","AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_demographics"]
        # sys.argv += ["-t","AMT_dataset_reliable_user-level_processing_all_dataset_weighted_stastistics_top-bottom_accurate_readers_demographics"]
        # sys.argv += ["-t","AMT_dataset_reliable_user-level_processing_all_dataset_weighted_stastistics_top-bottom_accurate_readers"]
        # sys.argv += ["-t","AMT_dataset_reliable_user-level_processing_all_dataset_weighted_stastistics_top-bottom_accurate_readers_together"]
        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_labeld_news_ktop_nptl"]
        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_labeld_news_ktop_nptl_together"]
        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_true-false(gt-pt)_news_ktop_nptl"]
        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_true-false(gt-pt)_news_ktop_nptl_scatter"]


        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_true-false(gt-pt)_news_ktop_nptl_scatter_fig1"]
        # sys.argv += ["-t", "AMT_dataset_reliable_processing_all_dataset_weighted_stastistics_top-disputability-NAPB"]
        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_CDF_fig"]
        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_PDF_CDF_MPB_fig"]
        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_PDF_CDF_APB_fig"]
        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_PDF_CDF_disp_fig"]
        # sys.argv += ["-t", "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_acc_cdf_toghether"]
        # sys.argv += ["-t", "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_mpb_cdf_toghether"]
        sys.argv += ["-t", "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_apb_cdf_toghether"]
        # sys.argv += ["-t", "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_disp_cdf_toghether"]

        # sys.argv += ["-t",
        #              "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_ideological_disp_cdf_toghether"]
        # sys.argv += ["-t", "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_ideological_mpb_cdf_toghether"]
        # sys.argv += ["-t", "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_ideological_apb_cdf_toghether"]
        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_stacked_bar_true-false_fig"]

        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_DISP_TPB_scatter_fig"]


        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_time_fig"]
        # sys.argv += ["-t", "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_time_cdf_toghether"]
        # sys.argv += ["-t",
        #              "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_corr_weighted_answering_nonweighted"]
        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_corr_gradient_descent"]
        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_true-false_accuracy"]
        # sys.argv += ["-t","AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_creating_features_prediction"]
        # sys.argv += ["-t", "AMT_dataset_reliable_news_processing_exp2_weighted_rel_leaning"]
        # sys.argv += ["-t", "AMT_dataset_reliable_news_processing_diff_topic_exp2"]
        # sys.argv += ["-t", "AMT_dataset_reliable_news_processing_exp2_news_users_dist_visulisation"]
        # sys.argv+=["-t","publishers_tweets_audiences_polarization_visiulization"]
        # sys.argv+=["-t","html_parser"]
        # sys.argv += ["-t", "html_parser_snopes"]
        # sys.argv += ["-t", "politifact_data_preprocess"]
        # sys.argv += ["-t", "snopes_data_preprocess"]
        # sys.argv += ["-t", "snopes_data_preprocess_topics"]
        # sys.argv += ["-t", "snopes_data_preprocess_topics_AMT"]
        # sys.argv += ["-t", "snopes_data_preprocess_topics_AMT_nonpolitic"]
        # sys.argv += ["-t", "politifact_data_preprocess_topics_AMT"]
        # sys.argv += ["-t", "snopes_data_preprocess_topics_AMT_preparing"]
        # sys.argv += ["-t", "politifact_data_preprocess_topics_AMT_preparing"]
        # sys.argv += ["-t", "word_counting"]
        print "The list od arguments modified to include:", sys.argv[1:]

    parser = argparse.ArgumentParser()
    parser.add_argument('inputpaths', nargs='*',
                        default=glob.glob("tweets-bilal/20*.gz"),
                        help='paths to gzipped files with tweets in json format')
    parser.add_argument('-t',
                        default="extract-userinfo-usercrawl",
                        help='task name')
    parser.add_argument('-c',
                        default="set3-en",
                        # default="set2-all",
                        # default="nov3m",
                        # default="all",
                        help='crawlname')
    parser.add_argument('-ct',
                        # default="tweets-retweeters",
                        default="tweets-expall",
                        help='crawltype')
    parser.add_argument('-usn',
                        default="set3-en",
                        help="usersetnm")
    parser.add_argument('-muzzled',
                        default=False,
                        action='store_true')
    parser.add_argument('-nj',
                        default=None,
                        help='n_jobs_remote')
    parser.add_argument('-pn',
                        default="before-feb2016-1m",
                        help='periodname')
    parser.add_argument('-f',
                        default="0",
                        help='num_followers_bin')

    args = parser.parse_args()
    crawlname = os.path.basename(args.c)
    crawltype = os.path.basename(args.ct)
    usersetnm = os.path.basename(args.usn)
    periodname = os.path.basename(args.pn)
    f_g_bin = int(args.f)
    # print(f_g_bin)
    inputpaths = args.inputpaths

    localdir, remotedir = tweetstxt_basic.get_dirs()

    # if tweetstxt_basic.is_local_machine():
    # 	nrows = 1e5
    # 	nsamples = int(1e4)
    # 	args.gs = True
    # else:
    # 	nrows = None
    # 	nsamples = int(1e5)
    # 	nsamples = int(2e4)

    # conn = pgsql.connect(
    #     "host='postgresql00.mpi-sws.org' dbname='twitter_data' user='twitter_data' password='twitter@mpi'")
    # cursor = conn.cursor()





    if args.t == "prepare_AMT_dataset_reliable_news":
        publisher_leaning = 1
        threshold = 10
        local_dir_saving = ''
        sp = 'sp_all_'
        # sp = 'sp_'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/data/'

        # normalization_clustering = ''
        input_rumor = open(remotedir + local_dir_saving + 'rumer_tweets', 'r')
        input_non_rumor = open(remotedir + local_dir_saving + 'non_rumer_tweets','r')




        final_output_exp1 = open(remotedir + local_dir_saving
                                 + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'w')

        tweet_id = 100010
        publisher_name = 110
        sample_tweets_exp1 = []

        for input_file in [input_rumor, input_non_rumor]:
            for line in input_file:
                # tweet_id = 100001
                # publisher_name = 100001
                line_splt = line.split('\t')
                tweet_txt = line_splt[1]
                tweet_link = line_splt[1]
                tweet_id+=1
                publisher_name+=1
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]

                sample_tweets_exp1.append(tmp_list)

        random.shuffle(sample_tweets_exp1)

        json.dump(sample_tweets_exp1[:80], final_output_exp1)


    if args.t == "AMT_dataset_reliable_news_processing":
        publisher_leaning = 1
        threshold = 10
        local_dir_saving = ''
        sp = 'sp_all_'
        # sp = 'sp_'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/data/'


        final_inp_exp1 = open(remotedir + local_dir_saving
                                 + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

        sample_tweets_exp1 = json.load(final_inp_exp1)

        input_rumor = open(remotedir + local_dir_saving + 'rumer_tweets', 'r')
        input_non_rumor = open(remotedir + local_dir_saving + 'non_rumer_tweets','r')





        tweet_id = 100010
        publisher_name = 110
        tweet_popularity = {}
        tweet_text_dic = {}
        for input_file in [input_rumor, input_non_rumor]:
            for line in input_file:
                line.replace('\n','')
                line_splt = line.split('\t')
                tweet_txt = line_splt[1]
                tweet_link = line_splt[1]
                tweet_id+=1
                publisher_name+=1
                tweet_popularity[tweet_id] = int(line_splt[2])
                tweet_text_dic[tweet_id] = tweet_txt

        # query1 = "select workerid, count(*) from mturk_crowd_signal_consensu_tweet_response_test group by workerid;"
        # query2 = "update mturk_crowd_signal_consensu_tweet_response_test set " \
        #          "workerid = mturk_crowd_signal_consensu_tweet_response_test.workerid + 100 from mturk_crowd_signal_consensu_tweet_response_test"

        # cursor.execute(query2)
        # test = cursor.fetchall()
        # mturk_crowd_signal_consensu_tweet_response_test



        run = 'plot'
        # run = 'analysis'
        exp1_list = sample_tweets_exp1


        out_list = []
        cnn_list = []
        foxnews_list = []
        ap_list = []
        tweet_txt_dict = {}
        tweet_link_dict = {}
        tweet_publisher_dict = {}
        tweet_rumor= {}
        tweet_non_rumor = {}
        pub_dict = collections.defaultdict(list)
        for tweet in exp1_list:

            tweet_id = tweet[0]
            publisher_name = tweet[1]
            tweet_txt = tweet[2]
            tweet_link = tweet[3]
            tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
            tweet_txt_dict[tweet_id] = tweet_txt
            tweet_link_dict[tweet_id] = tweet_link
            tweet_publisher_dict[tweet_id] = publisher_name
            if int(tweet_id)<100060:
                tweet_rumor[tweet_id]=-1
            else:
                tweet_non_rumor[tweet_id]=1

        if run == 'analysis':




            dem_list = []
            rep_list = []
            neut_list = []
            # valid users dict
            Vuser_dict = collections.defaultdict(int)
            query1 = "select workerid, count(*) from mturk_reliable_tweet_response_test2 group by workerid;"

            cursor.execute(query1)
            res_exp2 = cursor.fetchall()
            for el in res_exp2:
                if el[1] == 81:
                    Vuser_dict[el[0]] = 1

            query2 = "select workerid, tweet_id, ra, rb, rc, text from mturk_reliable_tweet_response_test2;"

            cursor.execute(query2)
            res_exp2 = cursor.fetchall()

            res_exp1_l = []
            for el in res_exp2:
                res_exp1_l.append((el[0], el[1], el[2], el[3], el[4]))
            query3 = "select workerid, ra from mturk_reliable_tweet_response_test2 where tweet_id=1;"

            cursor.execute(query3)
            res_leaning = cursor.fetchall()
            leaning_dict = collections.defaultdict()
            dem_l = [];
            rep_l = [];
            neut_l = []
            for el in res_leaning:
                leaning_dict[el[0]] = el[1]
                if el[1] == 1:
                    dem_l.append(el[0])
                elif el[1] == 2:
                    rep_l.append(el[0])
                elif el[1] == 3:
                    neut_l.append(el[0])

            min_len = np.min([len(dem_l), len(rep_l), len(neut_l)])
            random.shuffle(dem_l)
            random.shuffle(rep_l)
            random.shuffle(neut_l)

            # dem_list += dem_l[:min_len]
            # rep_list += rep_l[:min_len]
            # neut_list += neut_l[:min_len]


            dem_list += dem_l
            rep_list += rep_l
            neut_list += neut_l

            res_all = []
            res_all += res_exp1_l

            workerid_list = collections.defaultdict(list)
            tweetid_list = collections.defaultdict(list)
            ra_list = collections.defaultdict(list)
            rb_list = collections.defaultdict(list)
            rc_list = collections.defaultdict(list)
            txt_list = collections.defaultdict(list)
            index_list = collections.defaultdict(list)

            workerid_list_all = collections.defaultdict(list)
            tweetid_list_all = collections.defaultdict(list)
            ra_list_all = collections.defaultdict(list)
            rb_list_all = collections.defaultdict(list)
            rc_list_all = collections.defaultdict(list)
            txt_list_all = collections.defaultdict(list)
            index_list_all = collections.defaultdict(list)

            workerid_list_m = []
            tweetid_list_m = []
            ra_list_m = []
            rb_list_m = []
            rc_list_m = []
            txt_list_m = []
            index_list_m = []
            user_leaning_list_m = []
            count = 0
            # df_category = [0,1,2,3]*[1 as with author and 2 without author]
            category_author_laning = ['lean_1', 'lean2', 'lean3']
            for el in res_all:
                if el[0] in Vuser_dict:
                    workerid = int(el[0])
                    tweetid = int(el[1])
                    ra = int(el[2])
                    rb = int(el[3])
                    rc = int(el[4])
                    # if tweetid!=1:
                    #     ra = ra % 2
                    #     rb = rb % 2
                    #     rc = rc % 2
                    txt = el[5]

                    user_leaning = leaning_dict[el[0]]

                    workerid_list_m.append(workerid)
                    tweetid_list_m.append(tweetid)
                    ra_list_m.append(ra)
                    rb_list_m.append(rb)
                    rc_list_m.append(rc)
                    txt_list_m.append(txt)
                    index_list_m.append(count)
                    user_leaning_list_m.append(user_leaning)
                    count += 1


            index_list_m = range(len(workerid_list_m))
            df = pd.DataFrame({'worker_id': Series(workerid_list_m, index=index_list_m),
                               'tweet_id': Series(tweetid_list_m, index=index_list_m),
                               'ra': Series(ra_list_m, index=index_list_m),
                               'rb': Series(rb_list_m, index=index_list_m),
                               # 'rc': Series(rc_list_m, index=index_list_m),
                               'text': Series(txt_list_m, index=index_list_m),
                               'leaning': Series(user_leaning_list_m, index=index_list_m)})

            worker_id_list = []


            df.to_csv(remotedir + local_dir_saving +'amt_answers_reliable_news.csv',
                      columns=df.columns, sep="\t", index=False)



        else:
            # outF = open(remotedir+local_dir_saving+'amt_consensus_output_wiki_offensive_2.txt','w')

            input = remotedir + local_dir_saving  + 'amt_answers_reliable_news.csv'
            df = pd.read_csv(input, sep="\t")
            tweet_dem_score = {};
            tweet_rep_score = {};
            tweet_cons_score = {}
            tweet_cons_score_1 = {};
            tweet_cons_score_2 = {};
            tweet_cons_score_3 = {}


            # sum_feat = df.groupby(groupby_ftr).sum()
            # count_feat = df.groupby(groupby_ftr).count()

            df.loc[:, 'rel_score'] = df['tweet_id'] * 0.0
            df.loc[:, 'intst_score'] = df['tweet_id'] * 0.0
            df.loc[:, 'lable'] = df['tweet_id'] * 0.0

            df.loc[:, 'gull'] = df['tweet_id'] * 0.0
            df.loc[:, 'cyn'] = df['tweet_id'] * 0.0
            tweet_rel_dict = collections.defaultdict(list)
            tweet_intst_dict = collections.defaultdict(list)

            for index in df.index.tolist():
                tweet_id = df['tweet_id'][index]
                if tweet_id == 1:
                    continue

                ra = df['ra'][index]
                if ra<3:
                    rel = 1
                else:
                    rel = 0
                df['rel_score'][index] = rel


                rb = df['rb'][index]
                if rb<3:
                    intst = 1
                else:
                    intst = 0
                df['intst_score'][index]=intst

                tweet_rel_dict[tweet_id].append(rel)
                tweet_intst_dict[tweet_id].append(intst)

                if tweet_id in tweet_rumor:
                    df['lable'][index] = 0
                    df['gull'][index] = rel
                    df['cyn'][index] = 0
                else:
                    df['lable'][index] = 1
                    df['gull'][index] = 0
                    df['cyn'][index] = 1 - rel

            groupby_ftr = 'worker_id'
            grouped = df.groupby(groupby_ftr, sort=False)



            groupby_ftr = 'worker_id'
            grouped = df.groupby(groupby_ftr, sort=False)
            grouped_sum = df.groupby(groupby_ftr, sort=False).sum()

            # for workerid in grouped.groups.keys():
            #     tweet_id_l = grouped.groups[workerid]
            #     for tweetid in tweet_id_l:
            #         index = df[df['tweet_id'] == tweet_id].index.tolist()[0]
            #         if df['lable'][index]==0:
            #             gull +=

            df_dem = df[df['leaning']==1]
            df_rep = df[df['leaning']==2]
            df_neut = df[df['leaning']==3]
            rep_workers_list = list(set(df_rep['worker_id']))
            dem_workers_list = list(set(df_dem['worker_id']))
            neut_workers_list = list(set(df_neut['worker_id']))


            random.shuffle(dem_workers_list)
            dem_workers_list = dem_workers_list[:5]
            df_dem = df_dem[df_dem['worker_id'].isin(dem_workers_list)]

            grouped_sum_dem = df_dem.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_rep = df_rep.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_neut = df_neut.groupby(groupby_ftr, sort=False).sum()

            df_rumor = df[df['lable']==0]
            df_non_rumor = df[df['lable']==1]

            df_dem_rumor = df_dem[df_dem['lable']==0]
            df_dem_non_rumor = df_dem[df_dem['lable']==1]
            df_rep_rumor = df_rep[df_rep['lable']==0]
            df_rep_non_rumor = df_rep[df_rep['lable']==1]
            df_neut_rumor = df_neut[df_neut['lable']==0]
            df_neut_non_rumor = df_neut[df_neut['lable']==1]

            all_gull_list = [x/float(50) for x in list(grouped_sum['gull'])]
            all_cyn_list = [x/float(30) for x in list(grouped_sum['cyn'])]

            mplpl.hist(all_gull_list, bins=10, color='b')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('Frequency')
            mplpl.title('All users')
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig/all_gullibility'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            mplpl.hist(all_cyn_list, bins=10, color='r')
            mplpl.xlabel('Cynicality')
            mplpl.ylabel('Frequency')
            mplpl.title('All users')
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig/all_cynically'
            mplpl.savefig(pp, format='png')
            mplpl.figure()



            all_gull_list = [x/float(50) for x in list(grouped_sum_dem['gull'])]
            all_cyn_list = [x/float(30) for x in list(grouped_sum_dem['cyn'])]

            mplpl.hist(all_gull_list, bins=10, color='b')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('Frequency')
            mplpl.title('Dem users')
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig/dem_gullibility'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            mplpl.hist(all_cyn_list, bins=10, color='r')
            mplpl.xlabel('Cynicality')
            mplpl.ylabel('Frequency')
            mplpl.title('Dem users')
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig/dem_cynically'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            all_gull_list = [x/float(50) for x in list(grouped_sum_rep['gull'])]
            all_cyn_list = [x/float(30) for x in list(grouped_sum_rep['cyn'])]

            mplpl.hist(all_gull_list, bins=10, color='b')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('Frequency')
            mplpl.title('Rep users')
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig/rep_gullibility'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            mplpl.hist(all_cyn_list, bins=10, color='r')
            mplpl.xlabel('Cynically')
            mplpl.ylabel('Frequency')
            mplpl.title('Rep users')
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig/rep_cynicality'
            mplpl.savefig(pp, format='png')
            mplpl.figure()



            all_gull_list = [x/float(50) for x in list(grouped_sum_neut['gull'])]
            all_cyn_list = [x/float(30) for x in list(grouped_sum_neut['cyn'])]

            mplpl.hist(all_gull_list, bins=10, color='b')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('Frequency')
            mplpl.title('Neut users')
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig/neut_gullibility'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            mplpl.hist(all_cyn_list, bins=10, color='r')
            mplpl.xlabel('Cynically')
            mplpl.ylabel('Frequency')
            mplpl.title('Neut users')
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig/neut_cynicality'
            mplpl.savefig(pp, format='png')
            mplpl.figure()



            print(np.mean(grouped_sum['gull']))
            print(np.mean(grouped_sum['cyn']))


            print(np.mean(grouped_sum_dem['gull']))
            print(np.mean(grouped_sum_dem['cyn']))


            print(np.mean(grouped_sum_rep['gull']))
            print(np.mean(grouped_sum_rep['cyn']))


            print(np.mean(grouped_sum_neut['gull']))
            print(np.mean(grouped_sum_neut['cyn']))


            print(set(df_dem['worker_id']))
            print(set(df_rep['worker_id']))
            print(set(df_neut['worker_id']))
            # exit()

            tweet_rel_score = collections.defaultdict()
            tweet_intst_score = collections.defaultdict()
            tweet_rel_score_l = []
            tweet_intst_score_l = []
            tweet_non_rumor_rel_score_l = []
            tweet_non_rumor_intst_score_l = []
            tweet_rumor_rel_score_l = []
            tweet_rumor_intst_score_l = []
            tweet_pop = []
            tweet_pop_rumor = []
            tweet_pop_non_rumor = []


            # users = 'dem'
            # users = 'rep'
            users = 'neut'


            if users == 'all':
                df_m = df.copye()
            elif users == 'dem':
                df_m = df_dem.copy()
            elif users == 'rep':
                df_m = df_rep.copy()
            elif users == 'neut':
                df_m = df_neut.copy()

            groupby_ftr = 'tweet_id'
            grouped = df_m.groupby(groupby_ftr, sort=False)
            grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

            groupby_ftr = 'worker_id'
            grouped = df_m.groupby(groupby_ftr, sort=False)
            grouped_sum_w = df_m.groupby(groupby_ftr, sort=False).sum()


            for tweet_id in tweet_rel_dict:
                # tweet_rel_score[tweet_id] =   np.sum(tweet_rel_dict[tweet_id])/float(len(tweet_rel_dict[tweet_id]))
                # tweet_intst_score[tweet_id] = np.sum(tweet_intst_dict[tweet_id]) / float(len(tweet_intst_dict[tweet_id]))

                tweet_rel_score[tweet_id] = grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w))
                tweet_intst_score[tweet_id] = grouped_sum['intst_score'][tweet_id] / float(len(grouped_sum_w))

            tweet_id_sorted_list = sorted(tweet_rel_score, key=tweet_rel_score.get,reverse=False)
            for tweet_id in tweet_id_sorted_list:

                # tweet_rel_score_l.append(np.sum(tweet_rel_dict[tweet_id])/float(len(tweet_rel_dict[tweet_id])))
                # tweet_intst_score_l.append(np.sum(tweet_intst_dict[tweet_id]) / float(len(tweet_intst_dict[tweet_id])))

                tweet_rel_score_l.append(grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w)))
                tweet_intst_score_l.append(grouped_sum['intst_score'][tweet_id] / float(len(grouped_sum_w)))

                tweet_pop.append(tweet_popularity[tweet_id])

                if tweet_id in tweet_rumor:
                    tweet_rumor_rel_score_l.append(grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w)))
                    tweet_rumor_intst_score_l.append(grouped_sum['intst_score'][tweet_id] / float(len(grouped_sum_w)))
                    tweet_pop_rumor.append(tweet_popularity[tweet_id])

                if tweet_id in tweet_non_rumor:
                    tweet_non_rumor_rel_score_l.append(grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w)))
                    tweet_non_rumor_intst_score_l.append(grouped_sum['intst_score'][tweet_id] / float(len(grouped_sum_w)))
                    tweet_pop_non_rumor.append(tweet_popularity[tweet_id])

            # rumor_outF = open(remotedir + local_dir_saving + 'rumor_outF', 'w')
            # non_rumor_outF = open(remotedir + local_dir_saving + 'non-rumor_outF', 'w')
            #
            # for tweet_id in tweet_id_sorted_list:
            #     if tweet_id in tweet_rumor:
            #         rumor_outF.write('|| ' + tweet_text_dic[tweet_id] + '||' +'Rumor' + '||'+ str(tweet_rel_score[tweet_id]) + '||'+
            #               str(tweet_intst_score[tweet_id]) + '||' + str(tweet_popularity[tweet_id])+'||\n')
            #
            #     else:
            #         non_rumor_outF.write('|| ' + tweet_text_dic[tweet_id] + '||' +'Non-Rumor' + '||'+ str(tweet_rel_score[tweet_id]) + '||'+
            #               str(tweet_intst_score[tweet_id]) + '||' + str(tweet_popularity[tweet_id])+'||\n')


            mplpl.scatter(tweet_rel_score_l,tweet_intst_score_l)
            z = np.polyfit(tweet_rel_score_l, tweet_intst_score_l, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_rel_score_l, p(tweet_rel_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('Interesting score')
            mplpl.xlabel('Reliability score')
            mplpl.title('Rumors and Non-Rumors topics : ' + str(np.corrcoef(tweet_rel_score_l,tweet_intst_score_l)[0][1]))
            # mplpl.show()
            mplpl.legend(loc="upper right")
            pp = remotedir + local_dir_saving + '/fig/all_Interesting_Reliability_'+users
            mplpl.savefig(pp, format='png')

            mplpl.figure()
            mplpl.scatter(tweet_rumor_rel_score_l, tweet_rumor_intst_score_l)
            z = np.polyfit(tweet_rumor_rel_score_l, tweet_rumor_intst_score_l, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_rumor_rel_score_l, p(tweet_rumor_rel_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('Interesting score')
            mplpl.xlabel('Reliability score')
            mplpl.title('Rumors topics : ' + str(np.corrcoef(tweet_rumor_rel_score_l,tweet_rumor_intst_score_l)[0][1]))
            # mplpl.show()
            mplpl.legend(loc="upper right")
            pp = remotedir + local_dir_saving + '/fig/Rumors_Interesting_Reliability_'+users
            mplpl.savefig(pp, format='png')

            mplpl.figure()

            mplpl.scatter(tweet_non_rumor_rel_score_l, tweet_non_rumor_intst_score_l)
            z = np.polyfit(tweet_non_rumor_rel_score_l, tweet_non_rumor_intst_score_l, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_non_rumor_rel_score_l, p(tweet_non_rumor_rel_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('Interesting score')
            mplpl.xlabel('Reliability score')
            mplpl.title('Non-Rumors topics : ' + str(np.corrcoef(tweet_non_rumor_rel_score_l,tweet_non_rumor_intst_score_l)[0][1]))
            # mplpl.show()
            mplpl.legend(loc="upper right")
            pp = remotedir + local_dir_saving + '/fig/Non-Rumors_Interesting_Reliability_'+users
            mplpl.savefig(pp, format='png')




            mplpl.figure()

            mplpl.scatter(tweet_rel_score_l, tweet_pop)
            z = np.polyfit(tweet_rel_score_l, tweet_pop, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_rel_score_l, p(tweet_rel_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('popularity')
            mplpl.xlabel('Reliability score')
            mplpl.title('Rumors and Non-Rumors topics : ' + str(np.corrcoef(tweet_rel_score_l,tweet_pop)[0][1]))
            mplpl.yscale('log')
            # mplpl.show()
            mplpl.legend(loc="upper right")
            pp = remotedir + local_dir_saving + '/fig/all_popularity_Reliability_'+users
            mplpl.savefig(pp, format='png')


            mplpl.figure()

            mplpl.scatter(tweet_rumor_rel_score_l, tweet_pop_rumor)
            z = np.polyfit(tweet_rumor_rel_score_l, tweet_pop_rumor, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_rumor_rel_score_l, p(tweet_rumor_rel_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('popularity')
            mplpl.xlabel('Reliability score')
            mplpl.title('Rumors topics : ' + str(np.corrcoef(tweet_rumor_rel_score_l,tweet_pop_rumor)[0][1]))
            mplpl.yscale('log')
            # mplpl.show()
            mplpl.legend(loc="upper right")
            pp = remotedir + local_dir_saving + '/fig/Rumors_popularity_Reliability_'+users
            mplpl.savefig(pp, format='png')

            mplpl.figure()

            mplpl.scatter(tweet_non_rumor_rel_score_l, tweet_pop_non_rumor)
            z = np.polyfit(tweet_non_rumor_rel_score_l, tweet_pop_non_rumor, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_non_rumor_rel_score_l, p(tweet_non_rumor_rel_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('popularity')
            mplpl.xlabel('Reliability score')
            mplpl.title('Non-Rumors topics : ' + str(np.corrcoef(tweet_non_rumor_rel_score_l,tweet_pop_non_rumor)[0][1]))
            mplpl.yscale('log')
            # mplpl.show()
            mplpl.legend(loc="upper right")
            pp = remotedir + local_dir_saving + '/fig/Non-Rumors_popularity_Reliability_'+users
            mplpl.savefig(pp, format='png')





            mplpl.figure()

            mplpl.scatter(tweet_intst_score_l,tweet_pop)
            z = np.polyfit(tweet_intst_score_l,tweet_pop, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_intst_score_l, p(tweet_intst_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('Popularity')
            mplpl.xlabel('Interesting score')
            mplpl.title('Rumors and Non-Rumors topics : ' + str(np.corrcoef(tweet_intst_score_l,tweet_pop)[0][1]))
            mplpl.yscale('log')
            # mplpl.show()
            mplpl.legend(loc="upper right")
            pp = remotedir + local_dir_saving + '/fig/all_Popularity_Interesting_'+users
            mplpl.savefig(pp, format='png')

            mplpl.figure()

            mplpl.scatter(tweet_rumor_intst_score_l,tweet_pop_rumor)
            z = np.polyfit(tweet_rumor_intst_score_l,tweet_pop_rumor, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_rumor_intst_score_l, p(tweet_rumor_intst_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('Popularity')
            mplpl.xlabel('Interesting score')
            mplpl.title('Rumors topics : ' + str(np.corrcoef(tweet_rumor_intst_score_l,tweet_pop_rumor)[0][1]))
            mplpl.yscale('log')
            # mplpl.show()
            mplpl.legend(loc="upper right")
            pp = remotedir + local_dir_saving + '/fig/Rumors_Popularity_Interesting_'+users
            mplpl.savefig(pp, format='png')


            mplpl.figure()

            mplpl.scatter(tweet_non_rumor_intst_score_l,tweet_pop_non_rumor)
            mplpl.ylabel('Popularity')
            mplpl.xlabel('Interesting score')
            mplpl.title('Non-Rumors topics : ' + str(np.corrcoef(tweet_non_rumor_intst_score_l,tweet_pop_non_rumor)[0][1]))
            mplpl.yscale('log')
            z = np.polyfit(tweet_non_rumor_intst_score_l,tweet_pop_non_rumor, 1)
            p = np.poly1d(z)
            # mplpl.plot(tweet_non_rumor_intst_score_l, p(tweet_non_rumor_intst_score_l), 'r-', linewidth=4.0)
            # mplpl.show()
            mplpl.legend(loc="upper right")
            pp = remotedir + local_dir_saving + '/fig/Non-Rumors_Popularity_Interesting_'+users
            mplpl.savefig(pp, format='png')

            print(np.mean(tweet_rumor_rel_score_l))
            print(np.mean(tweet_non_rumor_rel_score_l))

    if args.t == "AMT_dataset_reliable_news_processing_diff_topic":

        publisher_leaning = 1
        threshold = 10
        local_dir_saving = ''
        sp = 'sp_all_'
        # sp = 'sp_'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/data/'


        final_inp_exp1 = open(remotedir + local_dir_saving
                                 + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

        sample_tweets_exp1 = json.load(final_inp_exp1)

        input_rumor = open(remotedir + local_dir_saving + 'rumer_tweets', 'r')
        input_non_rumor = open(remotedir + local_dir_saving + 'non_rumer_tweets','r')





        tweet_id = 100010
        publisher_name = 110
        tweet_popularity = {}
        tweet_text_dic = {}
        for input_file in [input_rumor, input_non_rumor]:
            for line in input_file:
                line.replace('\n','')
                line_splt = line.split('\t')
                tweet_txt = line_splt[1]
                tweet_link = line_splt[1]
                tweet_id+=1
                publisher_name+=1
                tweet_popularity[tweet_id] = int(line_splt[2])
                tweet_text_dic[tweet_id] = tweet_txt

        # query1 = "select workerid, count(*) from mturk_crowd_signal_consensu_tweet_response_test group by workerid;"
        # query2 = "update mturk_crowd_signal_consensu_tweet_response_test set " \
        #          "workerid = mturk_crowd_signal_consensu_tweet_response_test.workerid + 100 from mturk_crowd_signal_consensu_tweet_response_test"

        # cursor.execute(query2)
        # test = cursor.fetchall()
        # mturk_crowd_signal_consensu_tweet_response_test



        run = 'plot'
        # run = 'analysis'
        exp1_list = sample_tweets_exp1


        out_list = []
        cnn_list = []
        foxnews_list = []
        ap_list = []
        tweet_txt_dict = {}
        tweet_link_dict = {}
        tweet_publisher_dict = {}
        tweet_rumor= {}
        tweet_non_rumor = {}
        pub_dict = collections.defaultdict(list)
        for tweet in exp1_list:

            tweet_id = tweet[0]
            publisher_name = tweet[1]
            tweet_txt = tweet[2]
            tweet_link = tweet[3]
            tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
            tweet_txt_dict[tweet_id] = tweet_txt
            tweet_link_dict[tweet_id] = tweet_link
            tweet_publisher_dict[tweet_id] = publisher_name
            if int(tweet_id)<100060:
                tweet_rumor[tweet_id]=-1
            else:
                tweet_non_rumor[tweet_id]=1

        if run == 'analysis':

            print("analysis")


        else:

            rumor_outF = open(remotedir + local_dir_saving + 'rumor_outF.txt', 'r')
            non_rumor_outF = open(remotedir + local_dir_saving + 'non-rumor_outF.txt', 'r')
            non_rumor_tid_l = []
            rumor_tid_l = []
            for line in rumor_outF:
                line_splt = line.split('||')
                rumor_tid_l.append(int(line_splt[1]))

            for line in non_rumor_outF:
                line_splt = line.split('||')
                non_rumor_tid_l.append(int(line_splt[1]))

            rumor_inF = open('rumor_news.txt', 'r')
            non_rumor_inF = open('non-rumor_news.txt', 'r')

            tweet_id_l = []
            tweet_txt_l = []
            rel_score_l = []
            intst_score_l = []
            popularity_l = []
            politic_l = []
            helath_l = []
            tech_l = []
            myth_l = []
            death_l = []
            work_l = []
            personal_l = []
            entertainment_l = []
            food_l = []
            andimal_l = []
            rumor_l = []
            f_count=0
            for inpF in [rumor_inF, non_rumor_inF]:
                cc=0
                for line in inpF:
                    line_splt = line.split('||')
                    if cc>0:
                        # print(cc)
                        if f_count==0:
                            tweet_id = rumor_tid_l[cc-1]
                        else:
                            tweet_id = non_rumor_tid_l[cc-1]

                        tweet_txt = line_splt[1]

                        if 'Non' in line_splt[2]:
                            rumor = 0
                        else:
                            rumor=1
                        rel_score = float(line_splt[3])
                        intst_score = float(line_splt[4])
                        popularity = int(line_splt[5])
                        politic = int(line_splt[6])
                        health = int(line_splt[7])
                        tech = int(line_splt[8])
                        myth = int(line_splt[9])
                        death = int(line_splt[10])
                        work = int(line_splt[11])
                        personal = int(line_splt[12])
                        entertainment = int(line_splt[13])
                        food = int(line_splt[14])
                        animal = int(line_splt[15])


                        tweet_id_l.append(tweet_id)
                        tweet_txt_l.append(tweet_txt)
                        rumor_l.append(rumor)
                        rel_score_l.append(rel_score)
                        intst_score_l.append(intst_score)
                        popularity_l.append(popularity)
                        politic_l.append(politic)
                        helath_l.append(health)
                        tech_l.append(tech)
                        myth_l.append(myth)
                        death_l.append(death)
                        work_l.append(work)
                        personal_l.append(personal)
                        entertainment_l.append(entertainment)
                        food_l.append(food)
                        andimal_l.append(animal)
                    cc+=1
                f_count+=1

            index_list_m = range(80)
            df_topics = pd.DataFrame({'tweet_id': Series(tweet_id_l, index=index_list_m),
                               'text': Series(tweet_txt_l, index=index_list_m),
                               'rel_score': Series(rel_score_l, index=index_list_m),
                               'rumor': Series(rumor_l, index=index_list_m),
                               'intst_score': Series(intst_score_l, index=index_list_m),
                                'popularity' : Series(popularity_l, index=index_list_m),
                                'politic' : Series(politic_l, index=index_list_m),
                                'helath':Series(helath_l, index=index_list_m),
                                'tech' : Series(tech_l, index=index_list_m),
                                'myth' : Series(myth_l, index=index_list_m),
                                'death' : Series(death_l, index=index_list_m),
                                'work' : Series(work_l, index=index_list_m),
                                'personal' : Series(personal_l, index=index_list_m),
                                'entertainment' : Series(entertainment_l, index=index_list_m),
                                'food' : Series(food_l, index=index_list_m),
                                'animal' : Series(andimal_l, index=index_list_m)})

            topic_list = ['politic', 'helath', 'tech', 'myth', 'death', 'work', 'personal', 'entertainment', 'food', 'animal']


            input = remotedir + local_dir_saving  + 'amt_answers_reliable_news.csv'
            df = pd.read_csv(input, sep="\t")
            tweet_dem_score = {};
            tweet_rep_score = {};
            tweet_cons_score = {}
            tweet_cons_score_1 = {};
            tweet_cons_score_2 = {};
            tweet_cons_score_3 = {}


            # sum_feat = df.groupby(groupby_ftr).sum()
            # count_feat = df.groupby(groupby_ftr).count()

            df.loc[:, 'rel_score'] = df['tweet_id'] * 0.0
            df.loc[:, 'intst_score'] = df['tweet_id'] * 0.0
            df.loc[:, 'lable'] = df['tweet_id'] * 0.0

            df.loc[:, 'gull'] = df['tweet_id'] * 0.0
            df.loc[:, 'cyn'] = df['tweet_id'] * 0.0
            tweet_rel_dict = collections.defaultdict(list)
            tweet_intst_dict = collections.defaultdict(list)

            for index in df.index.tolist():
                tweet_id = df['tweet_id'][index]
                if tweet_id == 1:
                    continue

                ra = df['ra'][index]
                if ra<3:
                    rel = 1
                else:
                    rel = 0
                df['rel_score'][index] = rel


                rb = df['rb'][index]
                if rb<3:
                    intst = 1
                else:
                    intst = 0
                df['intst_score'][index]=intst

                tweet_rel_dict[tweet_id].append(rel)
                tweet_intst_dict[tweet_id].append(intst)

                if tweet_id in tweet_rumor:
                    df['lable'][index] = 0
                    df['gull'][index] = rel
                    df['cyn'][index] = 0
                else:
                    df['lable'][index] = 1
                    df['gull'][index] = 0
                    df['cyn'][index] = 1 - rel

            groupby_ftr = 'worker_id'
            grouped = df.groupby(groupby_ftr, sort=False)



            groupby_ftr = 'worker_id'
            grouped = df.groupby(groupby_ftr, sort=False)
            grouped_sum = df.groupby(groupby_ftr, sort=False).sum()

            # for workerid in grouped.groups.keys():
            #     tweet_id_l = grouped.groups[workerid]
            #     for tweetid in tweet_id_l:
            #         index = df[df['tweet_id'] == tweet_id].index.tolist()[0]
            #         if df['lable'][index]==0:
            #             gull +=

            df_dem = df[df['leaning']==1]
            df_rep = df[df['leaning']==2]
            df_neut = df[df['leaning']==3]
            rep_workers_list = list(set(df_rep['worker_id']))
            dem_workers_list = list(set(df_dem['worker_id']))
            neut_workers_list = list(set(df_neut['worker_id']))


            random.shuffle(dem_workers_list)
            dem_workers_list = dem_workers_list[:5]
            df_dem = df_dem[df_dem['worker_id'].isin(dem_workers_list)]

            grouped_sum_dem = df_dem.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_rep = df_rep.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_neut = df_neut.groupby(groupby_ftr, sort=False).sum()

            df_rumor = df[df['lable']==0]
            df_non_rumor = df[df['lable']==1]

            df_dem_rumor = df_dem[df_dem['lable']==0]
            df_dem_non_rumor = df_dem[df_dem['lable']==1]
            df_rep_rumor = df_rep[df_rep['lable']==0]
            df_rep_non_rumor = df_rep[df_rep['lable']==1]
            df_neut_rumor = df_neut[df_neut['lable']==0]
            df_neut_non_rumor = df_neut[df_neut['lable']==1]

            all_gull_list = [x/float(50) for x in list(grouped_sum['gull'])]
            all_cyn_list = [x/float(30) for x in list(grouped_sum['cyn'])]

            # mplpl.hist(all_gull_list, bins=10, color='b')
            # mplpl.xlabel('Gullibility')
            # mplpl.ylabel('Frequency')
            # mplpl.title('All users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/all_gullibility'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            # mplpl.hist(all_cyn_list, bins=10, color='r')
            # mplpl.xlabel('Cynicality')
            # mplpl.ylabel('Frequency')
            # mplpl.title('All users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/all_cynically'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            #
            #
            # all_gull_list = [x/float(50) for x in list(grouped_sum_dem['gull'])]
            # all_cyn_list = [x/float(30) for x in list(grouped_sum_dem['cyn'])]
            #
            # mplpl.hist(all_gull_list, bins=10, color='b')
            # mplpl.xlabel('Gullibility')
            # mplpl.ylabel('Frequency')
            # mplpl.title('Dem users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/dem_gullibility'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            # mplpl.hist(all_cyn_list, bins=10, color='r')
            # mplpl.xlabel('Cynicality')
            # mplpl.ylabel('Frequency')
            # mplpl.title('Dem users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/dem_cynically'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            #
            # all_gull_list = [x/float(50) for x in list(grouped_sum_rep['gull'])]
            # all_cyn_list = [x/float(30) for x in list(grouped_sum_rep['cyn'])]
            #
            # mplpl.hist(all_gull_list, bins=10, color='b')
            # mplpl.xlabel('Gullibility')
            # mplpl.ylabel('Frequency')
            # mplpl.title('Rep users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/rep_gullibility'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            # mplpl.hist(all_cyn_list, bins=10, color='r')
            # mplpl.xlabel('Cynically')
            # mplpl.ylabel('Frequency')
            # mplpl.title('Rep users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/rep_cynicality'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            #
            #
            # all_gull_list = [x/float(50) for x in list(grouped_sum_neut['gull'])]
            # all_cyn_list = [x/float(30) for x in list(grouped_sum_neut['cyn'])]
            #
            # mplpl.hist(all_gull_list, bins=10, color='b')
            # mplpl.xlabel('Gullibility')
            # mplpl.ylabel('Frequency')
            # mplpl.title('Neut users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/neut_gullibility'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            # mplpl.hist(all_cyn_list, bins=10, color='r')
            # mplpl.xlabel('Cynically')
            # mplpl.ylabel('Frequency')
            # mplpl.title('Neut users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/neut_cynicality'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()



            print(np.mean(grouped_sum['gull'])/float(50))
            print(np.mean(grouped_sum['cyn'])/float(30))


            print(np.mean(grouped_sum_dem['gull'])/float(50))
            print(np.mean(grouped_sum_dem['cyn'])/float(30))


            print(np.mean(grouped_sum_rep['gull'])/float(50))
            print(np.mean(grouped_sum_rep['cyn'])/float(30))


            print(np.mean(grouped_sum_neut['gull'])/float(50))
            print(np.mean(grouped_sum_neut['cyn'])/float(30))
            #
            #
            # print(set(df_dem['worker_id']))
            # print(set(df_rep['worker_id']))
            # print(set(df_neut['worker_id']))
            # exit()

            tweet_rel_score = collections.defaultdict()
            tweet_intst_score = collections.defaultdict()
            tweet_rel_score_l = []
            tweet_intst_score_l = []
            tweet_non_rumor_rel_score_l = []
            tweet_non_rumor_intst_score_l = []
            tweet_rumor_rel_score_l = []
            tweet_rumor_intst_score_l = []
            tweet_pop = []
            tweet_pop_rumor = []
            tweet_pop_non_rumor = []


            users = 'all'
            # users = 'dem'
            # users = 'rep'
            # users = 'neut'



            df_rumor = df[df['lable']==0]
            df_non_rumor = df[df['lable']==1]

            df_dem_rumor = df_dem[df_dem['lable']==0]
            df_dem_non_rumor = df_dem[df_dem['lable']==1]
            df_rep_rumor = df_rep[df_rep['lable']==0]
            df_rep_non_rumor = df_rep[df_rep['lable']==1]
            df_neut_rumor = df_neut[df_neut['lable']==0]
            df_neut_non_rumor = df_neut[df_neut['lable']==1]


            if users == 'all':
                df_r_m = df_rumor.copy()
                df_nr_m = df_non_rumor.copy()
            elif users == 'dem':
                df_r_m = df_dem_rumor.copy()
                df_nr_m = df_dem_non_rumor.copy()
            elif users == 'rep':
                df_r_m = df_rep_rumor.copy()
                df_nr_m = df_rep_non_rumor.copy()
            elif users == 'neut':
                df_r_m = df_neut_rumor.copy()
                df_nr_m = df_neut_non_rumor.copy()

            outF_gull = open(remotedir + local_dir_saving  + 'gullibility_users_topics.txt', 'w')
            outF_cyn = open(remotedir + local_dir_saving  + 'cynical_users_topics.txt', 'w')

            for topic in topic_list:
                outF_gull.write('||' + topic + '||')
                outF_cyn.write('||' + topic + '||')

                for users in ['all', 'dem', 'rep', 'neut']:
                    if users == 'all':
                        df_r_m = df_rumor.copy()
                        df_nr_m = df_non_rumor.copy()
                    elif users == 'dem':
                        df_r_m = df_dem_rumor.copy()
                        df_nr_m = df_dem_non_rumor.copy()
                    elif users == 'rep':
                        df_r_m = df_rep_rumor.copy()
                        df_nr_m = df_rep_non_rumor.copy()
                    elif users == 'neut':
                        df_r_m = df_neut_rumor.copy()
                        df_nr_m = df_neut_non_rumor.copy()


                    groupby_ftr = topic
                    grouped = df_topics.groupby(groupby_ftr, sort=False)
                    # grouped_sum = df_topics.groupby(groupby_ftr, sort=False).sum()
                    tweet_id_list = df_topics['tweet_id'][grouped.groups[1]]
                    num_r_topic = len(set(df_rumor[df_rumor['tweet_id'].isin(tweet_id_list)]))
                    num_nr_topic = len(set(df_non_rumor[df_non_rumor['tweet_id'].isin(tweet_id_list)]))
                    df_r_tmp = df_r_m[df_r_m['tweet_id'].isin(tweet_id_list)]

                    df_nr_tmp = df_nr_m[df_nr_m['tweet_id'].isin(tweet_id_list)]

                    groupby_ftr = 'worker_id'
                    grouped_sum_r = df_r_tmp.groupby(groupby_ftr, sort=False).sum()
                    grouped_sum_nr = df_nr_tmp.groupby(groupby_ftr, sort=False).sum()

                    if num_r_topic>0:
                        outF_gull.write(str(np.round(np.mean(grouped_sum_r['gull'])/float(num_r_topic), 4)) + '||')
                    else:
                        outF_gull.write('?'+ '||')
                    if num_nr_topic>0:
                        outF_cyn.write(str(np.round(np.mean(grouped_sum_nr['cyn'])/float(num_nr_topic),4)) + '||')
                    else:
                        outF_cyn.write('?'+ '||')
                outF_gull.write('\n')
                outF_cyn.write('\n')
                # print("salam")



            # groupby_ftr = 'worker_id'
            # grouped = df_m.groupby(groupby_ftr, sort=False)
            # grouped_sum_w = df_m.groupby(groupby_ftr, sort=False).sum()

    #### considering time
    if args.t == "AMT_dataset_reliable_news_processing_exp2":
        publisher_leaning = 1
        threshold = 10
        local_dir_saving = ''
        sp = 'sp_all_'
        # sp = 'sp_'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/data/'
        # remotedir = 'reliable_news/data/'


        final_inp_exp1 = open(remotedir + local_dir_saving
                                 + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

        sample_tweets_exp1 = json.load(final_inp_exp1)

        input_rumor = open(remotedir + local_dir_saving + 'rumer_tweets', 'r')
        input_non_rumor = open(remotedir + local_dir_saving + 'non_rumer_tweets','r')

        input = remotedir + local_dir_saving + 'news_labling_time.csv'
        df_time = pd.read_csv(input, sep="\t")



        tweet_id = 100010
        publisher_name = 110
        tweet_popularity = {}
        tweet_text_dic = {}
        for input_file in [input_rumor, input_non_rumor]:
            for line in input_file:
                line.replace('\n','')
                line_splt = line.split('\t')
                tweet_txt = line_splt[1]
                tweet_link = line_splt[1]
                tweet_id+=1
                publisher_name+=1
                tweet_popularity[tweet_id] = int(line_splt[2])
                tweet_text_dic[tweet_id] = tweet_txt

        # query1 = "select workerid, count(*) from mturk_crowd_signal_consensu_tweet_response_test group by workerid;"
        # query2 = "update mturk_crowd_signal_consensu_tweet_response_test set " \
        #          "workerid = mturk_crowd_signal_consensu_tweet_response_test.workerid + 100 from mturk_crowd_signal_consensu_tweet_response_test"

        # cursor.execute(query2)
        # test = cursor.fetchall()
        # mturk_crowd_signal_consensu_tweet_response_test



        run = 'plot'
        # run = 'analysis'
        exp1_list = sample_tweets_exp1


        out_list = []
        cnn_list = []
        foxnews_list = []
        ap_list = []
        tweet_txt_dict = {}
        tweet_link_dict = {}
        tweet_publisher_dict = {}
        tweet_rumor= {}
        tweet_non_rumor = {}
        pub_dict = collections.defaultdict(list)
        for tweet in exp1_list:

            tweet_id = tweet[0]
            publisher_name = tweet[1]
            tweet_txt = tweet[2]
            tweet_link = tweet[3]
            tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
            tweet_txt_dict[tweet_id] = tweet_txt
            tweet_link_dict[tweet_id] = tweet_link
            tweet_publisher_dict[tweet_id] = publisher_name
            if int(tweet_id)<100060:
                tweet_rumor[tweet_id]=-1
            else:
                tweet_non_rumor[tweet_id]=1

        if run == 'analysis':




            dem_list = []
            rep_list = []
            neut_list = []
            # valid users dict
            Vuser_dict = collections.defaultdict(int)
            query1 = "select workerid, count(*) from mturk_reliable_tweet_response_exp2_2 group by workerid;"

            cursor.execute(query1)
            res_exp2 = cursor.fetchall()
            for el in res_exp2:
                if el[1] == 81:
                    Vuser_dict[el[0]] = 1

            query2 = "select workerid, tweet_id, ra, rb, rc, at, text from mturk_reliable_tweet_response_exp2_2;"

            cursor.execute(query2)
            res_exp2 = cursor.fetchall()

            res_exp1_l = []
            for el in res_exp2:
                res_exp1_l.append((el[0], el[1], el[2], el[3], el[4], el[5], el[6]))
            query3 = "select workerid, ra from mturk_reliable_tweet_response_exp2_2 where tweet_id=1;"

            cursor.execute(query3)
            res_leaning = cursor.fetchall()
            leaning_dict = collections.defaultdict()
            dem_l = [];
            rep_l = [];
            neut_l = []
            for el in res_leaning:
                leaning_dict[el[0]] = el[1]
                if el[1] == 1:
                    dem_l.append(el[0])
                elif el[1] == 2:
                    rep_l.append(el[0])
                elif el[1] == 3:
                    neut_l.append(el[0])

            min_len = np.min([len(dem_l), len(rep_l), len(neut_l)])
            random.shuffle(dem_l)
            random.shuffle(rep_l)
            random.shuffle(neut_l)

            dem_list += dem_l[:min_len]
            rep_list += rep_l[:min_len]
            neut_list += neut_l[:min_len]


            # dem_list += dem_l
            # rep_list += rep_l
            # neut_list += neut_l

            res_all = []
            res_all += res_exp1_l

            workerid_list = collections.defaultdict(list)
            tweetid_list = collections.defaultdict(list)
            ra_list = collections.defaultdict(list)
            rb_list = collections.defaultdict(list)
            rc_list = collections.defaultdict(list)
            txt_list = collections.defaultdict(list)
            index_list = collections.defaultdict(list)

            workerid_list_all = collections.defaultdict(list)
            tweetid_list_all = collections.defaultdict(list)
            ra_list_all = collections.defaultdict(list)
            rb_list_all = collections.defaultdict(list)
            rc_list_all = collections.defaultdict(list)
            txt_list_all = collections.defaultdict(list)
            index_list_all = collections.defaultdict(list)

            workerid_list_m = []
            tweetid_list_m = []
            ra_list_m = []
            rb_list_m = []
            rc_list_m = []
            txt_list_m = []
            index_list_m = []
            user_leaning_list_m = []
            time_list_m = []
            count = 0
            # df_category = [0,1,2,3]*[1 as with author and 2 without author]
            category_author_laning = ['lean_1', 'lean2', 'lean3']
            for el in res_all:
                if el[0] in Vuser_dict:
                    workerid = int(el[0])
                    tweetid = int(el[1])
                    ra = int(el[2])
                    rb = int(el[3])
                    rc = int(el[4])
                    # if tweetid!=1:
                    #     ra = ra % 2
                    #     rb = rb % 2
                    #     rc = rc % 2
                    time = int(el[5])
                    txt = el[6]

                    user_leaning = leaning_dict[el[0]]

                    workerid_list_m.append(workerid)
                    tweetid_list_m.append(tweetid)
                    ra_list_m.append(ra)
                    # rb_list_m.append(rb)
                    # rc_list_m.append(rc)
                    txt_list_m.append(txt)
                    time_list_m.append(time)
                    index_list_m.append(count)
                    user_leaning_list_m.append(user_leaning)
                    count += 1


            index_list_m = range(len(workerid_list_m))
            df = pd.DataFrame({'worker_id': Series(workerid_list_m, index=index_list_m),
                               'tweet_id': Series(tweetid_list_m, index=index_list_m),
                               'ra': Series(ra_list_m, index=index_list_m),
                               # 'rb': Series(rb_list_m, index=index_list_m),
                               # 'rc': Series(rc_list_m, index=index_list_m),
                               'time': Series(time_list_m, index=index_list_m),
                               'text': Series(txt_list_m, index=index_list_m),
                               'leaning': Series(user_leaning_list_m, index=index_list_m)})

            worker_id_list = []


            df.to_csv(remotedir + local_dir_saving +'amt_answers_reliable_news_exp2.csv',
                      columns=df.columns, sep="\t", index=False)



        else:
            # outF = open(remotedir+local_dir_saving+'amt_consensus_output_wiki_offensive_2.txt','w')

            input = remotedir + local_dir_saving  + 'amt_answers_reliable_news_exp2.csv'
            df = pd.read_csv(input, sep="\t")
            tweet_dem_score = {};
            tweet_rep_score = {};
            tweet_cons_score = {}
            tweet_cons_score_1 = {};
            tweet_cons_score_2 = {};
            tweet_cons_score_3 = {}


            # sum_feat = df.groupby(groupby_ftr).sum()
            # count_feat = df.groupby(groupby_ftr).count()

            df.loc[:, 'rel_score'] = df['tweet_id'] * 0.0
            df.loc[:, 'intst_score'] = df['tweet_id'] * 0.0
            df.loc[:, 'lable'] = df['tweet_id'] * 0.0
            df.loc[:, 'delta_time'] = df['tweet_id'] * 0.0

            df.loc[:, 'gull'] = df['tweet_id'] * 0.0
            df.loc[:, 'cyn'] = df['tweet_id'] * 0.0
            tweet_rel_dict = collections.defaultdict(list)
            tweet_intst_dict = collections.defaultdict(list)
            rel_4 = 0
            for index in df.index.tolist():
                tweet_id = df['tweet_id'][index]
                if tweet_id == 1:
                    continue

                ra = df['ra'][index]
                if ra<=4:
                    rel = 0
                elif ra>4:
                    rel = 1
                # else:
                #     rel_4+=1
                df['rel_score'][index] = rel

                tweet_rel_dict[tweet_id].append(rel)

                if tweet_id in tweet_rumor:
                    df['lable'][index] = 0
                    df['gull'][index] = rel
                    df['cyn'][index] = 0
                else:
                    df['lable'][index] = 1
                    df['gull'][index] = 0
                    df['cyn'][index] = 1 - rel




            groupby_ftr = 'worker_id'
            grouped = df.groupby(groupby_ftr, sort=False)
            grouped_sum = df.groupby(groupby_ftr, sort=False).sum()

            # for workerid in grouped.groups.keys():
            #     tweet_id_l = grouped.groups[workerid]
            #     for tweetid in tweet_id_l:
            #         index = df[df['tweet_id'] == tweet_id].index.tolist()[0]
            #         if df['lable'][index]==0:
            #             gull +=

            df_dem = df[df['leaning']==1]
            df_rep = df[df['leaning']==2]
            df_neut = df[df['leaning']==3]
            rep_workers_list = list(set(df_rep['worker_id']))
            dem_workers_list = list(set(df_dem['worker_id']))
            neut_workers_list = list(set(df_neut['worker_id']))


            df_gr = df.groupby('worker_id', sort=False)
            worker_id_l = df_gr.groups.keys()
            cc = 0
            pr_tim = 0
            cur_tim = 0
            # for workerid in worker_id_l:
            #     cc = 0
            #     pr_tim = 0
            #     cur_tim = 0
            #     ind_l = df_gr.groups[workerid]
            #     df_wid = df.iloc[ind_l]
            #     df_wid = df_wid.sort('time', ascending=True)
            #     for tmp_ind in df_wid.index.tolist():
            #         cur_tim = df_wid['time'][tmp_ind]
            #         if cc==0:
            #             pr_tim=cur_tim
            #         cc+=1
            #         delta = cur_tim - pr_tim
            #         pr_tim = cur_tim
            #         df['delta_time'][tmp_ind] = delta
            #     np.max(df_wid['time']) - np.min(df_wid['time'])

            # news_time_labling_F = open(remotedir + local_dir_saving + 'news_labling_time.txt','w')
            # news_time_labling_csv = open(remotedir + local_dir_saving + 'news_labling_time.csv','w')
            #
            # df_gr = df.groupby('tweet_id', sort=False)
            #
            # news_time_labling_F.write('|| tweet text || tweet id ||')
            # news_time_labling_csv.write('tweet_id\ttweet_text\t')
            # # news_time_labling_csv.write('tweet_id\t')
            # for w_id in worker_id_l:
            #     news_time_labling_F.write(str(w_id) + '||')
            #     news_time_labling_csv.write(str(w_id) + '\t')
            #
            # news_time_labling_F.write('AVG||')
            # news_time_labling_csv.write('AVG\t')
            # news_time_labling_F.write('\n')
            # news_time_labling_csv.write('\n')
            #
            # my_ind = -1
            # for tweetid in df_gr.groups:
            #     ts = 0
            #     my_ind += 1
            #     if tweetid==1:
            #         continue
            #     ind_l = df_gr.groups[tweetid]
            #     df_wid = df.iloc[ind_l]
            #     # df_wid = df_wid.sort('time', ascending=True)
            #     news_time_labling_F.write( '||' + tweet_text_dic[tweetid].replace("?", "'") +'||' + str(tweetid) + '||')
            #     news_time_labling_csv.write(str(my_ind)+'\t')
            #     m_text = tweet_text_dic[tweetid].replace("?", "'")
            #     m_text = m_text.replace("\t", " ")
            #     news_time_labling_csv.write(str(tweetid) + '\t' + m_text +'\t' )
            #     # news_time_labling_csv.write(str(tweetid) + '\t' )
            #
            #     for w_id in worker_id_l:
            #         ind_t = df_wid[df_wid['worker_id'] == w_id].index[0]
            #         ts+= int(df['delta_time'][ind_t])
            #         news_time_labling_F.write(str(df['delta_time'][ind_t])+'||')
            #         news_time_labling_csv.write(str(df['delta_time'][ind_t]) + '\t')
            #
            #     news_time_labling_F.write((str(ts/float(len(worker_id_l)))) + '||')
            #     news_time_labling_F.write('\n')
            #
            #     news_time_labling_csv.write((str(ts/float(len(worker_id_l)))) + '\t')
            #     news_time_labling_csv.write('\n')


            # news_time_labling_csv.close()
            input = remotedir + local_dir_saving  + 'news_labling_time.csv'
            df_time = pd.read_csv(input, sep="\t")

            # print(np.max(df['time']) - np.min(df['time']))

            # random.shuffle(dem_workers_list,random.random)
            random.shuffle(dem_workers_list)
            dem_workers_list = dem_workers_list[:6]
            df_dem = df_dem[df_dem['worker_id'].isin(dem_workers_list)]

            # random.shuffle(neut_workers_list,random.random)
            random.shuffle(neut_workers_list)
            neut_workers_list = neut_workers_list[:6]
            df_neut = df_neut[df_neut['worker_id'].isin(neut_workers_list)]

            all_workers_list = []
            all_workers_list += dem_workers_list
            all_workers_list += rep_workers_list
            all_workers_list += neut_workers_list
            df = df[df['worker_id'].isin(all_workers_list)]


            grouped_sum = df.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_dem = df_dem.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_rep = df_rep.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_neut = df_neut.groupby(groupby_ftr, sort=False).sum()

            df_rumor = df[df['lable']==0]
            df_non_rumor = df[df['lable']==1]

            df_dem_rumor = df_dem[df_dem['lable']==0]
            df_dem_non_rumor = df_dem[df_dem['lable']==1]
            df_rep_rumor = df_rep[df_rep['lable']==0]
            df_rep_non_rumor = df_rep[df_rep['lable']==1]
            df_neut_rumor = df_neut[df_neut['lable']==0]
            df_neut_non_rumor = df_neut[df_neut['lable']==1]

            all_gull_list = [x/float(50) for x in list(grouped_sum['gull'])]
            all_cyn_list = [x/float(30) for x in list(grouped_sum['cyn'])]

            all_gull_list_sort = sorted(all_gull_list)
            all_cyn_list_sort = sorted(all_cyn_list)

            mplpl.scatter(range(len(all_gull_list_sort)),all_gull_list_sort,color='k')
            mplpl.xlabel('Workers id')
            mplpl.ylabel('Gulliblity')
            mplpl.title('All users, Mean : ' + str(np.mean(all_gull_list_sort)))
            mplpl.ylim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/all_gullibility_workers'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            mplpl.scatter(range(len(all_cyn_list_sort)), all_cyn_list_sort,color='k')
            mplpl.xlabel('Workers id')
            mplpl.ylabel('Cynicality')
            mplpl.title('All users, Mean : ' + str(np.mean(all_cyn_list_sort)))
            mplpl.ylim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/all_cynicality_workers'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            mplpl.hist(all_gull_list, bins=10, color='k')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('Frequency')
            mplpl.title('All users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/all_gullibility'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            # n, bins, patches = mplpl.hist(all_cyn_list_sort,normed=1, histtype='step', cumulative=True,
            #            color='b')
            num_bins = len(all_gull_list)
            counts, bin_edges = np.histogram(all_gull_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='k', lw=3, label='Gullibility')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('All users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/all_gullibility_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()



            mplpl.hist(all_cyn_list, bins=10, color='k')
            mplpl.xlabel('Cynicality')
            mplpl.ylabel('Frequency')
            mplpl.title('All users, Mean : ' + str(np.mean(all_cyn_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/all_cynicality'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            num_bins = len(all_cyn_list)
            counts, bin_edges = np.histogram(all_cyn_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='k', lw=3, label='Cynicality')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('All users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/all_cynicality_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            all_gull_list = [x/float(50) for x in list(grouped_sum_dem['gull'])]
            all_cyn_list = [x/float(30) for x in list(grouped_sum_dem['cyn'])]

            dem_gull_list_sort = sorted(all_gull_list)
            dem_cyn_list_sort = sorted(all_cyn_list)


            mplpl.hist(all_gull_list, bins=10, color='b')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('Frequency')
            mplpl.title('Dem users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/dem_gullibility'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_gull_list)
            counts, bin_edges = np.histogram(all_gull_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='b', lw=3, label='Gullibility')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('Dem users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/dem_gullibility_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            mplpl.hist(all_cyn_list, bins=10, color='b')
            mplpl.xlabel('Cynicality')
            mplpl.ylabel('Frequency')
            mplpl.title('Dem users, Mean : ' + str(np.mean(all_cyn_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/dem_cynically'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_cyn_list)
            counts, bin_edges = np.histogram(all_cyn_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='b', lw=3, label='Cynicality')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('Dem users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/dem_cynicality_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()




            all_gull_list = [x/float(50) for x in list(grouped_sum_rep['gull'])]
            all_cyn_list = [x/float(30) for x in list(grouped_sum_rep['cyn'])]


            rep_gull_list_sort = sorted(all_gull_list)
            rep_cyn_list_sort = sorted(all_cyn_list)


            mplpl.hist(all_gull_list, bins=10, color='r')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('Frequency')
            mplpl.title('Rep users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/rep_gullibility'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_gull_list)
            counts, bin_edges = np.histogram(all_gull_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='r', lw=3, label='Gullibility')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('Rep users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/rep_gullibility_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            mplpl.hist(all_cyn_list, bins=10, color='r')
            mplpl.xlabel('Cynically')
            mplpl.ylabel('Frequency')
            mplpl.title('Rep users, Mean : ' + str(np.mean(all_cyn_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/rep_cynicality'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_cyn_list)
            counts, bin_edges = np.histogram(all_cyn_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='r', lw=3, label='Cynicality')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('Rep users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/rep_cynicality_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            all_gull_list = [x/float(50) for x in list(grouped_sum_neut['gull'])]
            all_cyn_list = [x/float(30) for x in list(grouped_sum_neut['cyn'])]


            neut_gull_list_sort = sorted(all_gull_list)
            neut_cyn_list_sort = sorted(all_cyn_list)


            mplpl.hist(all_gull_list, bins=10, color='g')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('Frequency')
            mplpl.title('Neut users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/neut_gullibility'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_gull_list)
            counts, bin_edges = np.histogram(all_gull_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='g', lw=3, label='Gullibility')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('Neut users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/neut_gullibility_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            mplpl.hist(all_cyn_list, bins=10, color='g')
            mplpl.xlabel('Cynically')
            mplpl.ylabel('Frequency')
            mplpl.title('Neut users, Mean : ' + str(np.mean(all_cyn_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/neut_cynicality'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            num_bins = len(all_cyn_list)
            counts, bin_edges = np.histogram(all_cyn_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='g', lw=3, label='Cynicality')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('Neut users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            pp = remotedir + local_dir_saving + '/fig_exp2/neut_cynicality_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()









            mplpl.scatter(range(len(dem_gull_list_sort)),dem_gull_list_sort, s=40,color='b',marker='o', label='Democrats')
            mplpl.scatter(range(len(rep_gull_list_sort)),rep_gull_list_sort, s=40,color='r',marker='s', label='Republicans')
            mplpl.scatter(range(len(neut_gull_list_sort)),neut_gull_list_sort, s=40,color='g',marker='<', label='Neutrals')
            mplpl.xlabel('Workers rank')
            mplpl.ylabel('Gulliblity')
            # mplpl.title('All users, Mean : ' + str(np.mean(dem_gull_list_sort)))
            mplpl.ylim([0,1])
            mplpl.legend(loc="upper left")
            pp = remotedir + local_dir_saving + '/fig_exp2/dem_rep_neut_gullibility_workers'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            mplpl.scatter(range(len(dem_cyn_list_sort)), dem_cyn_list_sort, s=40,color='b',marker='o', label='Democrats')
            mplpl.plot(range(len(dem_cyn_list_sort)), dem_cyn_list_sort, color='b')
            mplpl.scatter(range(len(rep_cyn_list_sort)), rep_cyn_list_sort, s=40,color='r',marker='s', label='Republicans')
            mplpl.plot(range(len(rep_cyn_list_sort)), rep_cyn_list_sort,color='r')
            mplpl.scatter(range(len(neut_cyn_list_sort)), neut_cyn_list_sort, s=40,color='g',marker='<', label='Neutrals')
            mplpl.plot(range(len(neut_cyn_list_sort)), neut_cyn_list_sort, color='g')
            mplpl.xlabel('Workers rank')
            mplpl.ylabel('Cynicality')
            # mplpl.title('All users, Mean : ' + str(np.mean(dem_cyn_list_sort)))
            mplpl.ylim([0,1])
            mplpl.legend(loc="upper left")

            pp = remotedir + local_dir_saving + '/fig_exp2/dem_rep_neut_synicality_workers'
            mplpl.savefig(pp, format='png')
            mplpl.figure()




            # print(np.mean(grouped_sum['gull']))
            # print(np.mean(grouped_sum['cyn']))
            #
            #
            # print(np.mean(grouped_sum_dem['gull']))
            # print(np.mean(grouped_sum_dem['cyn']))
            #
            #
            # print(np.mean(grouped_sum_rep['gull']))
            # print(np.mean(grouped_sum_rep['cyn']))
            #
            #
            # print(np.mean(grouped_sum_neut['gull']))
            # print(np.mean(grouped_sum_neut['cyn']))
            #
            #
            # print(set(df_dem['worker_id']))
            # print(set(df_rep['worker_id']))
            # print(set(df_neut['worker_id']))
            # exit()

            tweet_rel_score = collections.defaultdict()
            tweet_intst_score = collections.defaultdict()
            tweet_rel_score_l = []
            tweet_intst_score_l = []
            tweet_non_rumor_rel_score_l = []
            tweet_non_rumor_intst_score_l = []
            tweet_rumor_rel_score_l = []
            tweet_rumor_intst_score_l = []
            tweet_pop = []
            tweet_pop_rumor = []
            tweet_pop_non_rumor = []


            # users='all'
            # users = 'dem'
            # users = 'rep'
            users = 'neut'


            if users == 'all':
                df_m = df.copy()
                col = 'k'
            elif users == 'dem':
                df_m = df_dem.copy()
                col = 'b'
            elif users == 'rep':
                df_m = df_rep.copy()
                col = 'r'
            elif users == 'neut':
                df_m = df_neut.copy()
                col = 'g'

            groupby_ftr = 'tweet_id'
            grouped = df_m.groupby(groupby_ftr, sort=False)
            grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

            groupby_ftr = 'worker_id'
            grouped = df_m.groupby(groupby_ftr, sort=False)
            grouped_sum_w = df_m.groupby(groupby_ftr, sort=False).sum()


            for tweet_id in tweet_rel_dict:
                # tweet_rel_score[tweet_id] =   np.sum(tweet_rel_dict[tweet_id])/float(len(tweet_rel_dict[tweet_id]))
                # tweet_intst_score[tweet_id] = np.sum(tweet_intst_dict[tweet_id]) / float(len(tweet_intst_dict[tweet_id]))

                tweet_rel_score[tweet_id] = grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w))

            tweet_id_sorted_list = sorted(tweet_rel_score, key=tweet_rel_score.get,reverse=False)
            for tweet_id in tweet_id_sorted_list:

                # tweet_rel_score_l.append(np.sum(tweet_rel_dict[tweet_id])/float(len(tweet_rel_dict[tweet_id])))
                # tweet_intst_score_l.append(np.sum(tweet_intst_dict[tweet_id]) / float(len(tweet_intst_dict[tweet_id])))

                tweet_rel_score_l.append(grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w)))

                tweet_pop.append(tweet_popularity[tweet_id])

                if tweet_id in tweet_rumor:
                    tweet_rumor_rel_score_l.append(grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w)))
                    tweet_pop_rumor.append(tweet_popularity[tweet_id])

                if tweet_id in tweet_non_rumor:
                    tweet_non_rumor_rel_score_l.append(grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w)))
                    tweet_pop_non_rumor.append(tweet_popularity[tweet_id])

            # rumor_outF = open(remotedir + local_dir_saving + 'rumor_outF_exp2.txt', 'w')
            # non_rumor_outF = open(remotedir + local_dir_saving + 'non-rumor_outF_exp2.txt', 'w')
            #
            # for tweet_id in tweet_id_sorted_list:
            #     if tweet_id in tweet_rumor:
            #         rumor_outF.write('|| ' + str(tweet_id) + '|| ' + tweet_text_dic[tweet_id] + '||' +'Rumor' + '||'+ str(tweet_rel_score[tweet_id]) + '||'+
            #               str(tweet_popularity[tweet_id])+'||\n')
            #
            #     else:
            #         non_rumor_outF.write('|| ' + str(tweet_id) + '|| ' +tweet_text_dic[tweet_id] + '||' +'Non-Rumor' + '||'+ str(tweet_rel_score[tweet_id]) + '||'+
            #               str(tweet_popularity[tweet_id])+'||\n')



            mplpl.figure()

            mplpl.scatter(tweet_rel_score_l, tweet_pop)
            z = np.polyfit(tweet_rel_score_l, tweet_pop, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_rel_score_l, p(tweet_rel_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('popularity')
            mplpl.xlabel('Reliability score')
            mplpl.title('Rumors and Non-Rumors topics : ' + str(np.corrcoef(tweet_rel_score_l,tweet_pop)[0][1]))
            mplpl.yscale('log')
            # mplpl.show()
            mplpl.legend(loc="upper right")
            pp = remotedir + local_dir_saving + '/fig_exp2/all_popularity_Reliability_'+users
            mplpl.savefig(pp, format='png')


            print(np.mean(tweet_rel_score_l))

            mplpl.figure()

            mplpl.scatter(tweet_rumor_rel_score_l, tweet_pop_rumor)
            z = np.polyfit(tweet_rumor_rel_score_l, tweet_pop_rumor, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_rumor_rel_score_l, p(tweet_rumor_rel_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('popularity')
            mplpl.xlabel('Reliability score')
            mplpl.title('Rumors topics : ' + str(np.corrcoef(tweet_rumor_rel_score_l,tweet_pop_rumor)[0][1]))
            mplpl.yscale('log')
            # mplpl.show()
            mplpl.legend(loc="upper right")
            pp = remotedir + local_dir_saving + '/fig_exp2/Rumors_popularity_Reliability_'+users
            mplpl.savefig(pp, format='png')

            print(np.mean(tweet_rumor_rel_score_l))
            mplpl.figure()

            mplpl.scatter(tweet_non_rumor_rel_score_l, tweet_pop_non_rumor)
            z = np.polyfit(tweet_non_rumor_rel_score_l, tweet_pop_non_rumor, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_non_rumor_rel_score_l, p(tweet_non_rumor_rel_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('popularity')
            mplpl.xlabel('Reliability score')
            mplpl.title('Non-Rumors topics : ' + str(np.corrcoef(tweet_non_rumor_rel_score_l,tweet_pop_non_rumor)[0][1]))
            mplpl.yscale('log')
            # mplpl.show()
            mplpl.legend(loc="upper right")
            pp = remotedir + local_dir_saving + '/fig_exp2/Non-Rumors_popularity_Reliability_'+users
            mplpl.savefig(pp, format='png')
            print(np.mean(tweet_non_rumor_rel_score_l))

            mplpl.figure()

            tweet_non_rumor_rel_score_l_sort = sorted(tweet_non_rumor_rel_score_l)
            tweet_rumor_rel_score_l_sort = sorted(tweet_rumor_rel_score_l)
            tweet_rel_score_l_sort = sorted(tweet_rel_score_l)



            mplpl.scatter(range(len(tweet_rel_score_l_sort)),tweet_rel_score_l_sort, s=40,color='k',marker='o', label='All news')
            mplpl.plot(range(len(tweet_rel_score_l_sort)), tweet_rel_score_l_sort, color='k')
            mplpl.scatter(range(len(tweet_rumor_rel_score_l_sort)),tweet_rumor_rel_score_l_sort, s=40,color='r',marker='s', label='Rumors')
            mplpl.plot(range(len(tweet_rumor_rel_score_l_sort)), tweet_rumor_rel_score_l_sort, color='r')
            mplpl.scatter(range(len(tweet_non_rumor_rel_score_l_sort)),tweet_non_rumor_rel_score_l_sort, s=40,color='g',marker='<', label='Non-Rumors')
            mplpl.plot(range(len(tweet_non_rumor_rel_score_l_sort)), tweet_non_rumor_rel_score_l_sort, color='g')
            mplpl.xlabel('News')
            mplpl.ylabel('Reliability')
            # mplpl.title('All users, Mean : ' + str(np.mean(dem_gull_list_sort)))
            # mplpl.ylim([0,1])
            mplpl.legend(loc="upper left")
            pp = remotedir + local_dir_saving + '/fig_exp2/'+users+'_reliability_news'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            num_bins = len(tweet_rel_score_l_sort)
            counts, bin_edges = np.histogram(tweet_rel_score_l_sort, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='k', lw=3, label='All news')


            num_bins = len(tweet_rumor_rel_score_l_sort)
            counts, bin_edges = np.histogram(tweet_rumor_rel_score_l_sort, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='r', lw=3, label='Rumors')

            num_bins = len(tweet_non_rumor_rel_score_l_sort)
            counts, bin_edges = np.histogram(tweet_non_rumor_rel_score_l_sort, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='g', lw=3, label='Non-Rumors')

            mplpl.xlabel('Reliability')
            mplpl.ylabel('CDF')
            # mplpl.title('Neut users, Mean : ' + str(np.mean(all_gull_list)))
            mplpl.xlim([0,1])
            mplpl.ylim([0,1])
            mplpl.legend(loc="upper left")
            pp = remotedir + local_dir_saving + '/fig_exp2/'+users+'_reliability_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


    if args.t == "AMT_dataset_reliable_news_processing_exp2_weighted":
        publisher_leaning = 1
        threshold = 10
        local_dir_saving = ''
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/data/'


        final_inp_exp1 = open(remotedir + local_dir_saving
                                 + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

        sample_tweets_exp1 = json.load(final_inp_exp1)

        input_rumor = open(remotedir + local_dir_saving + 'rumer_tweets', 'r')
        input_non_rumor = open(remotedir + local_dir_saving + 'non_rumer_tweets','r')

        input = remotedir + local_dir_saving + 'news_labling_time.csv'
        df_time = pd.read_csv(input, sep="\t")



        tweet_id = 100010
        publisher_name = 110
        tweet_popularity = {}
        tweet_text_dic = {}
        for input_file in [input_rumor, input_non_rumor]:
            for line in input_file:
                line.replace('\n','')
                line_splt = line.split('\t')
                tweet_txt = line_splt[1]
                tweet_link = line_splt[1]
                tweet_id+=1
                publisher_name+=1
                tweet_popularity[tweet_id] = int(line_splt[2])
                tweet_text_dic[tweet_id] = tweet_txt

        # query1 = "select workerid, count(*) from mturk_crowd_signal_consensu_tweet_response_test group by workerid;"
        # query2 = "update mturk_crowd_signal_consensu_tweet_response_test set " \
        #          "workerid = mturk_crowd_signal_consensu_tweet_response_test.workerid + 100 from mturk_crowd_signal_consensu_tweet_response_test"

        # cursor.execute(query2)
        # test = cursor.fetchall()
        # mturk_crowd_signal_consensu_tweet_response_test



        run = 'plot'
        # run = 'analysis'
        exp1_list = sample_tweets_exp1


        out_list = []
        cnn_list = []
        foxnews_list = []
        ap_list = []
        tweet_txt_dict = {}
        tweet_link_dict = {}
        tweet_publisher_dict = {}
        tweet_rumor= {}
        tweet_non_rumor = {}
        pub_dict = collections.defaultdict(list)
        for tweet in exp1_list:

            tweet_id = tweet[0]
            publisher_name = tweet[1]
            tweet_txt = tweet[2]
            tweet_link = tweet[3]
            tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
            tweet_txt_dict[tweet_id] = tweet_txt
            tweet_link_dict[tweet_id] = tweet_link
            tweet_publisher_dict[tweet_id] = publisher_name
            if int(tweet_id)<100060:
                tweet_rumor[tweet_id]=-1
            else:
                tweet_non_rumor[tweet_id]=1

        if run == 'analysis':




            dem_list = []
            rep_list = []
            neut_list = []
            # valid users dict
            Vuser_dict = collections.defaultdict(int)
            query1 = "select workerid, count(*) from mturk_reliable_tweet_response_exp2_2 group by workerid;"

            cursor.execute(query1)
            res_exp2 = cursor.fetchall()
            for el in res_exp2:
                if el[1] == 81:
                    Vuser_dict[el[0]] = 1

            query2 = "select workerid, tweet_id, ra, rb, rc, at, text from mturk_reliable_tweet_response_exp2_2;"

            cursor.execute(query2)
            res_exp2 = cursor.fetchall()

            res_exp1_l = []
            for el in res_exp2:
                res_exp1_l.append((el[0], el[1], el[2], el[3], el[4], el[5], el[6]))
            query3 = "select workerid, ra from mturk_reliable_tweet_response_exp2_2 where tweet_id=1;"

            cursor.execute(query3)
            res_leaning = cursor.fetchall()
            leaning_dict = collections.defaultdict()
            dem_l = [];
            rep_l = [];
            neut_l = []
            for el in res_leaning:
                leaning_dict[el[0]] = el[1]
                if el[1] == 1:
                    dem_l.append(el[0])
                elif el[1] == 2:
                    rep_l.append(el[0])
                elif el[1] == 3:
                    neut_l.append(el[0])

            min_len = np.min([len(dem_l), len(rep_l), len(neut_l)])
            random.shuffle(dem_l)
            random.shuffle(rep_l)
            random.shuffle(neut_l)

            dem_list += dem_l[:min_len]
            rep_list += rep_l[:min_len]
            neut_list += neut_l[:min_len]


            # dem_list += dem_l
            # rep_list += rep_l
            # neut_list += neut_l

            res_all = []
            res_all += res_exp1_l

            workerid_list = collections.defaultdict(list)
            tweetid_list = collections.defaultdict(list)
            ra_list = collections.defaultdict(list)
            rb_list = collections.defaultdict(list)
            rc_list = collections.defaultdict(list)
            txt_list = collections.defaultdict(list)
            index_list = collections.defaultdict(list)

            workerid_list_all = collections.defaultdict(list)
            tweetid_list_all = collections.defaultdict(list)
            ra_list_all = collections.defaultdict(list)
            rb_list_all = collections.defaultdict(list)
            rc_list_all = collections.defaultdict(list)
            txt_list_all = collections.defaultdict(list)
            index_list_all = collections.defaultdict(list)

            workerid_list_m = []
            tweetid_list_m = []
            ra_list_m = []
            rb_list_m = []
            rc_list_m = []
            txt_list_m = []
            index_list_m = []
            user_leaning_list_m = []
            time_list_m = []
            count = 0
            # df_category = [0,1,2,3]*[1 as with author and 2 without author]
            category_author_laning = ['lean_1', 'lean2', 'lean3']
            for el in res_all:
                if el[0] in Vuser_dict:
                    workerid = int(el[0])
                    tweetid = int(el[1])
                    ra = int(el[2])
                    rb = int(el[3])
                    rc = int(el[4])
                    # if tweetid!=1:
                    #     ra = ra % 2
                    #     rb = rb % 2
                    #     rc = rc % 2
                    time = int(el[5])
                    txt = el[6]

                    user_leaning = leaning_dict[el[0]]

                    workerid_list_m.append(workerid)
                    tweetid_list_m.append(tweetid)
                    ra_list_m.append(ra)
                    # rb_list_m.append(rb)
                    # rc_list_m.append(rc)
                    txt_list_m.append(txt)
                    time_list_m.append(time)
                    index_list_m.append(count)
                    user_leaning_list_m.append(user_leaning)
                    count += 1


            index_list_m = range(len(workerid_list_m))
            df = pd.DataFrame({'worker_id': Series(workerid_list_m, index=index_list_m),
                               'tweet_id': Series(tweetid_list_m, index=index_list_m),
                               'ra': Series(ra_list_m, index=index_list_m),
                               # 'rb': Series(rb_list_m, index=index_list_m),
                               # 'rc': Series(rc_list_m, index=index_list_m),
                               'time': Series(time_list_m, index=index_list_m),
                               'text': Series(txt_list_m, index=index_list_m),
                               'leaning': Series(user_leaning_list_m, index=index_list_m)})

            worker_id_list = []


            df.to_csv(remotedir + local_dir_saving +'amt_answers_reliable_news_exp2.csv',
                      columns=df.columns, sep="\t", index=False)



        else:
            # outF = open(remotedir+local_dir_saving+'amt_consensus_output_wiki_offensive_2.txt','w')
            approach = 'weighted'
            # approach = 'non-weighted'
            input = remotedir + local_dir_saving  + 'amt_answers_reliable_news_exp2.csv'
            df = pd.read_csv(input, sep="\t")
            tweet_dem_score = {};
            tweet_rep_score = {};
            tweet_cons_score = {}
            tweet_cons_score_1 = {};
            tweet_cons_score_2 = {};
            tweet_cons_score_3 = {}


            # sum_feat = df.groupby(groupby_ftr).sum()
            # count_feat = df.groupby(groupby_ftr).count()

            df.loc[:, 'rel_score'] = df['tweet_id'] * 0.0
            df.loc[:, 'intst_score'] = df['tweet_id'] * 0.0
            df.loc[:, 'lable'] = df['tweet_id'] * 0.0
            df.loc[:, 'delta_time'] = df['tweet_id'] * 0.0

            df.loc[:, 'gull'] = df['tweet_id'] * 0.0
            df.loc[:, 'cyn'] = df['tweet_id'] * 0.0
            df.loc[:, 'susc'] = df['tweet_id'] * 0.0
            tweet_rel_dict = collections.defaultdict(list)
            tweet_intst_dict = collections.defaultdict(list)
            rel_4 = 0
            for index in df.index.tolist():
                tweet_id = df['tweet_id'][index]
                if tweet_id == 1:
                    continue

                ra = df['ra'][index]
                if approach == 'weighted':

                    if ra==1:
                        rel = -3
                    elif ra==2:
                        rel=-2
                    elif ra==3:
                        rel=-1
                    elif ra==4:
                        rel=0
                    elif ra==5:
                        rel = 1
                    elif ra==6:
                        rel = 2
                    elif ra==7:
                        rel = 3
                #
                elif approach == 'non-weighted':

                    if ra==1:
                        rel = -1
                    elif ra==2:
                        rel=-1
                    elif ra==3:
                        rel=-1
                    elif ra==4:
                        rel=0
                    elif ra==5:
                        rel = 1
                    elif ra==6:
                        rel = 1
                    elif ra==7:
                        rel = 1



                df['rel_score'][index] = rel

                tweet_rel_dict[tweet_id].append(rel)

                if tweet_id in tweet_rumor:
                    df['lable'][index] = -1
                    # df['cyn'][index] = -1
                    # df['gull'][index] = 0
                    # if rel > 0:
                    # df['gull'][index] = rel/float(3)

                else:
                    df['lable'][index] = 1
                    # df['gull'][index] = -1
                    # df['cyn'][index] = 0
                    # if rel < 0:
                    # df['cyn'][index] = -1* rel/float(3)

                l_scr = df['lable'][index]
                k_1 = 1
                k_2 = 3

                gull_val = rel / float(k_2) - l_scr / float(k_1)
                gull_val = gull_val * ((np.sign(gull_val) + 1) / float(2))

                cyn_val = l_scr / float(k_1) - rel / float(k_2)
                cyn_val = cyn_val * ((np.sign(cyn_val) + 1) / float(2))

                df['gull'][index] = gull_val / float(2)
                df['cyn'][index] = cyn_val / float(2)
                df['susc'][index] = (df['cyn'][index] + df['gull'][index])/float(2)

            df_dem = df[df['leaning'] == 1]
            df_rep = df[df['leaning'] == 2]
            df_neut = df[df['leaning'] == 3]
            rep_workers_list = list(set(df_rep['worker_id']))
            dem_workers_list = list(set(df_dem['worker_id']))
            neut_workers_list = list(set(df_neut['worker_id']))

            # random.shuffle(dem_workers_list,random.random)
            random.shuffle(dem_workers_list)
            dem_workers_list = dem_workers_list[:6]
            df_dem = df_dem[df_dem['worker_id'].isin(dem_workers_list)]

            # random.shuffle(neut_workers_list,random.random)
            random.shuffle(neut_workers_list)
            neut_workers_list = neut_workers_list[:6]
            df_neut = df_neut[df_neut['worker_id'].isin(neut_workers_list)]

            all_workers_list = []
            all_workers_list += dem_workers_list
            all_workers_list += rep_workers_list
            all_workers_list += neut_workers_list
            df = df[df['worker_id'].isin(all_workers_list)]

            news_stories = ['all', 'Rumors', 'Non-Rumors']
            users_l = ['all_18', 'dem', 'rep', 'neut']

            table_F = open(remotedir+local_dir_saving + 'table_out.txt' , 'w')

            for news_story in news_stories:
                table_F.write('\n')
                for users in users_l:

                    if news_story=='all':
                        df_m = df.copy()
                    elif news_story == 'Rumors':
                        df_m = df[df['lable']==-1]
                    elif news_story == 'Non-Rumors':
                        df_m = df[df['lable']==1]

                    df_dem = df_m[df_m['leaning']==1]
                    df_rep = df_m[df_m['leaning']==2]
                    df_neut = df_m[df_m['leaning']==3]
                    rep_workers_list = list(set(df_rep['worker_id']))
                    dem_workers_list = list(set(df_dem['worker_id']))
                    neut_workers_list = list(set(df_neut['worker_id']))

                    # if users == 'all':
                    #     df_m = df.copy()
                    # elif users == 'dem':
                    #     df_m = df_m[df_dem['worker_id'].isin(dem_workers_list)]
                    # elif users == 'rep':
                    #     df_m = df_rep[df_rep['worker_id'].isin(rep_workers_list)]
                    # elif users == 'neut':
                    #     df_m = df_neut[df_neut['worker_id'].isin(neut_workers_list)]



                    if users == 'all_18':
                        df_m = df_m.copy()
                        col = 'c'
                    elif users == 'dem':
                        df_m = df_dem.copy()
                        col = 'b'
                    elif users == 'rep':
                        df_m = df_rep.copy()
                        col = 'r'
                    elif users == 'neut':
                        df_m = df_neut.copy()
                        col = 'g'
                    tmp_list = list(df_m['ra'][:])
                    rumor_relative_n = len(df_m[df_m['ra'].isin([1,2,3])])/float(len(df_m))
                    nonrumor_relative_n = len(df_m[df_m['ra'].isin([5,6,7])])/float(len(df_m))
                    neut_relative_n = len(df_m[df_m['ra'].isin([4])])/float(len(df_m))

                    table_F.write('||' +str(rumor_relative_n)+ '||'
                                  + str(neut_relative_n) + '||'
                                  + str(nonrumor_relative_n) + '||\n')
                    mplpl.hist(list(df_m['ra'][:]), bins=7, normed=1, color=col)
                    # mplpl.xlabel('Rel score')
                    labels = ['Confirm it to be false', 'Very likely to be a ', 'Possibly rumor',
                              'Can\'t tell','Possibly real','Very likely to be real','Confirm it to be real']
                    x = [1, 2, 3, 4, 5, 6, 7]
                    mplpl.xticks(x, labels, rotation='45')
                    mplpl.ylabel('Frequency')
                    mplpl.title(users + ' users')
                    mplpl.subplots_adjust(bottom=0.25)
                    mplpl.ylim([0,0.5])
                    if approach == 'weighted':
                        pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_'+users+'_' + news_story+'_rel_dist'
                    elif approach == 'non-weighted':
                        pp = remotedir + local_dir_saving + '/fig_exp2/'+users+'_' + news_story+'_rel_dist'

                    # mplpl.savefig(pp, format='png')
                    mplpl.figure()


            # exit()
            groupby_ftr = 'worker_id'
            grouped = df.groupby(groupby_ftr, sort=False)
            grouped_sum = df.groupby(groupby_ftr, sort=False).sum()

            # for workerid in grouped.groups.keys():
            #     tweet_id_l = grouped.groups[workerid]
            #     for tweetid in tweet_id_l:
            #         index = df[df['tweet_id'] == tweet_id].index.tolist()[0]
            #         if df['lable'][index]==0:
            #             gull +=

            df_dem = df[df['leaning']==1]
            df_rep = df[df['leaning']==2]
            df_neut = df[df['leaning']==3]
            rep_workers_list = list(set(df_rep['worker_id']))
            dem_workers_list = list(set(df_dem['worker_id']))
            neut_workers_list = list(set(df_neut['worker_id']))


            df_gr = df.groupby('worker_id', sort=False)
            worker_id_l = df_gr.groups.keys()
            cc = 0
            pr_tim = 0
            cur_tim = 0
            # for workerid in worker_id_l:
            #     cc = 0
            #     pr_tim = 0
            #     cur_tim = 0
            #     ind_l = df_gr.groups[workerid]
            #     df_wid = df.iloc[ind_l]
            #     df_wid = df_wid.sort('time', ascending=True)
            #     for tmp_ind in df_wid.index.tolist():
            #         cur_tim = df_wid['time'][tmp_ind]
            #         if cc==0:
            #             pr_tim=cur_tim
            #         cc+=1
            #         delta = cur_tim - pr_tim
            #         pr_tim = cur_tim
            #         df['delta_time'][tmp_ind] = delta
            #     np.max(df_wid['time']) - np.min(df_wid['time'])

            # news_time_labling_F = open(remotedir + local_dir_saving + 'news_labling_time.txt','w')
            # news_time_labling_csv = open(remotedir + local_dir_saving + 'news_labling_time.csv','w')
            #
            # df_gr = df.groupby('tweet_id', sort=False)
            #
            # news_time_labling_F.write('|| tweet text || tweet id ||')
            # news_time_labling_csv.write('tweet_id\ttweet_text\t')
            # # news_time_labling_csv.write('tweet_id\t')
            # for w_id in worker_id_l:
            #     news_time_labling_F.write(str(w_id) + '||')
            #     news_time_labling_csv.write(str(w_id) + '\t')
            #
            # news_time_labling_F.write('AVG||')
            # news_time_labling_csv.write('AVG\t')
            # news_time_labling_F.write('\n')
            # news_time_labling_csv.write('\n')
            #
            # my_ind = -1
            # for tweetid in df_gr.groups:
            #     ts = 0
            #     my_ind += 1
            #     if tweetid==1:
            #         continue
            #     ind_l = df_gr.groups[tweetid]
            #     df_wid = df.iloc[ind_l]
            #     # df_wid = df_wid.sort('time', ascending=True)
            #     news_time_labling_F.write( '||' + tweet_text_dic[tweetid].replace("?", "'") +'||' + str(tweetid) + '||')
            #     news_time_labling_csv.write(str(my_ind)+'\t')
            #     m_text = tweet_text_dic[tweetid].replace("?", "'")
            #     m_text = m_text.replace("\t", " ")
            #     news_time_labling_csv.write(str(tweetid) + '\t' + m_text +'\t' )
            #     # news_time_labling_csv.write(str(tweetid) + '\t' )
            #
            #     for w_id in worker_id_l:
            #         ind_t = df_wid[df_wid['worker_id'] == w_id].index[0]
            #         ts+= int(df['delta_time'][ind_t])
            #         news_time_labling_F.write(str(df['delta_time'][ind_t])+'||')
            #         news_time_labling_csv.write(str(df['delta_time'][ind_t]) + '\t')
            #
            #     news_time_labling_F.write((str(ts/float(len(worker_id_l)))) + '||')
            #     news_time_labling_F.write('\n')
            #
            #     news_time_labling_csv.write((str(ts/float(len(worker_id_l)))) + '\t')
            #     news_time_labling_csv.write('\n')


            # news_time_labling_csv.close()
            input = remotedir + local_dir_saving  + 'news_labling_time.csv'
            df_time = pd.read_csv(input, sep="\t")



            # print(np.max(df['time']) - np.min(df['time']))
            sum_t = 0
            for col in all_workers_list:
                sum_t += len(df_time[df_time[str(col)]<=20])/float(80)

            print(sum_t/float(len(all_workers_list)))
            groupby_ftr = 'worker_id'
            grouped_sum = df.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_dem = df_dem.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_rep = df_rep.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_neut = df_neut.groupby(groupby_ftr, sort=False).sum()

            grouped_mean = df.groupby(groupby_ftr, sort=False).mean()
            grouped_mean_dem = df_dem.groupby(groupby_ftr, sort=False).mean()
            grouped_mean_rep = df_rep.groupby(groupby_ftr, sort=False).mean()
            grouped_mean_neut = df_neut.groupby(groupby_ftr, sort=False).mean()


            df_rumor = df[df['lable']==-1]
            df_non_rumor = df[df['lable']==1]

            df_dem_rumor = df_dem[df_dem['lable']==-1]
            df_dem_non_rumor = df_dem[df_dem['lable']==1]
            df_rep_rumor = df_rep[df_rep['lable']==-1]
            df_rep_non_rumor = df_rep[df_rep['lable']==1]
            df_neut_rumor = df_neut[df_neut['lable']==-1]
            df_neut_non_rumor = df_neut[df_neut['lable']==1]

            # all_gull_list = [x/float(50) for x in list(grouped_sum['gull'])]
            # all_cyn_list = [x/float(30) for x in list(grouped_sum['cyn'])]
            all_gull_list = list(grouped_mean['gull'])
            all_cyn_list = list(grouped_mean['cyn'])
            all_susc_list = list(grouped_mean['susc'])

            all_gull_list_sort = sorted(all_gull_list)
            all_cyn_list_sort = sorted(all_cyn_list)
            all_susc_list_sort = sorted(all_susc_list)


            mplpl.scatter(range(len(all_susc_list_sort)),all_susc_list_sort,color='k')
            mplpl.xlabel('Workers id')
            mplpl.ylabel('Susceptibility')
            mplpl.title('All users, Mean : ' + str(np.mean(all_susc_list_sort)))
            mplpl.ylim([0,1])
            if approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/all_susc_workers'
            elif approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_all_susc_workers'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            mplpl.scatter(range(len(all_gull_list_sort)),all_gull_list_sort,color='k')
            mplpl.xlabel('Workers id')
            mplpl.ylabel('Gulliblity')
            mplpl.title('All users, Mean : ' + str(np.mean(all_gull_list_sort)))
            mplpl.ylim([0,1])
            if approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/all_gullibility_workers'
            elif approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_all_gullibility_workers'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            mplpl.scatter(range(len(all_cyn_list_sort)), all_cyn_list_sort,color='k')
            mplpl.xlabel('Workers id')
            mplpl.ylabel('Cynicality')
            mplpl.title('All users, Mean : ' + str(np.mean(all_cyn_list_sort)))
            mplpl.ylim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_all_cynicality_workers'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/all_cynicality_workers'
            mplpl.savefig(pp, format='png')
            mplpl.figure()



            mplpl.hist(all_susc_list, bins=10, color='k')
            mplpl.xlabel('Susceptibility')
            mplpl.ylabel('Frequency')
            mplpl.title('All users, Mean : ' + str(np.mean(all_susc_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_all_susc'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/all_susc'
            # mplpl.savefig(pp, format='png')
            mplpl.figure()


            mplpl.hist(all_gull_list, bins=10, color='k')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('Frequency')
            mplpl.title('All users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_all_gullibility'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/all_gullibility'
            # mplpl.savefig(pp, format='png')
            mplpl.figure()

            # n, bins, patches = mplpl.hist(all_cyn_list_sort,normed=1, histtype='step', cumulative=True,
            #            color='b')
            num_bins = len(all_gull_list)
            counts, bin_edges = np.histogram(all_gull_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='k', lw=3, label='Gullibility')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('All users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_all_gullibility_CDF'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/all_gullibility_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()



            mplpl.hist(all_cyn_list, bins=10, color='k')
            mplpl.xlabel('Cynicality')
            mplpl.ylabel('Frequency')
            mplpl.title('All users, Mean : ' + str(np.mean(all_cyn_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_all_cynicality'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/all_cynicality'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            num_bins = len(all_cyn_list)
            counts, bin_edges = np.histogram(all_cyn_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='k', lw=3, label='Cynicality')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('All users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_all_cynicality_CDF'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/all_cynicality_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            all_gull_list = list(grouped_mean_dem['gull'])
            all_cyn_list = list(grouped_mean_dem['cyn'])
            all_susc_list = list(grouped_mean_dem['susc'])

            dem_gull_list_sort = sorted(all_gull_list)
            dem_cyn_list_sort = sorted(all_cyn_list)
            dem_susc_list_sort = sorted(all_susc_list)



            mplpl.hist(all_susc_list, bins=10, color='b')
            mplpl.xlabel('Susceptibility')
            mplpl.ylabel('Frequency')
            mplpl.title('Dem users, Mean : ' + str(np.mean(all_susc_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_dem_susc'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/dem_susc'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_susc_list)
            counts, bin_edges = np.histogram(all_susc_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='b', lw=3, label='Susceptibility')
            mplpl.xlabel('Susceptibility')
            mplpl.ylabel('CDF')
            mplpl.title('Dem users, Mean : ' + str(np.mean(all_susc_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_dem_susc_CDF'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/dem_susc_CDF'

            mplpl.savefig(pp, format='png')
            mplpl.figure()





            mplpl.hist(all_gull_list, bins=10, color='b')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('Frequency')
            mplpl.title('Dem users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_dem_gullibility'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/dem_gullibility'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_gull_list)
            counts, bin_edges = np.histogram(all_gull_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='b', lw=3, label='Gullibility')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('Dem users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_dem_gullibility_CDF'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/dem_gullibility_CDF'

            mplpl.savefig(pp, format='png')
            mplpl.figure()

            mplpl.hist(all_cyn_list, bins=10, color='b')
            mplpl.xlabel('Cynicality')
            mplpl.ylabel('Frequency')
            mplpl.title('Dem users, Mean : ' + str(np.mean(all_cyn_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_dem_cynically'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/dem_cynically'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_cyn_list)
            counts, bin_edges = np.histogram(all_cyn_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='b', lw=3, label='Cynicality')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('Dem users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_dem_cynicality_CDF'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/dem_cynicality_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()




            all_gull_list = list(grouped_mean_rep['gull'])
            all_cyn_list = list(grouped_mean_rep['cyn'])
            all_susc_list = list(grouped_mean_rep['susc'])


            rep_gull_list_sort = sorted(all_gull_list)
            rep_cyn_list_sort = sorted(all_cyn_list)
            rep_susc_list_sort = sorted(all_susc_list)


            mplpl.hist(all_susc_list, bins=10, color='r')
            mplpl.xlabel('Susceptibility')
            mplpl.ylabel('Frequency')
            mplpl.title('Rep users, Mean : ' + str(np.mean(all_susc_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_rep_susc'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/rep_susc'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_susc_list)
            counts, bin_edges = np.histogram(all_susc_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='r', lw=3, label='Susceptibility')
            mplpl.xlabel('Susceptibility')
            mplpl.ylabel('CDF')
            mplpl.title('Rep users, Mean : ' + str(np.mean(all_susc_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_rep_susc_CDF'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/rep_susc_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()



            mplpl.hist(all_gull_list, bins=10, color='r')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('Frequency')
            mplpl.title('Rep users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_rep_gullibility'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/rep_gullibility'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_gull_list)
            counts, bin_edges = np.histogram(all_gull_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='r', lw=3, label='Gullibility')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('Rep users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_rep_gull_CDF'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/rep_gull_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            mplpl.hist(all_cyn_list, bins=10, color='r')
            mplpl.xlabel('Cynically')
            mplpl.ylabel('Frequency')
            mplpl.title('Rep users, Mean : ' + str(np.mean(all_cyn_list)))
            mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_rep_cynicality'
            pp = remotedir + local_dir_saving + '/fig_exp2/rep_cynicality'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_cyn_list)
            counts, bin_edges = np.histogram(all_cyn_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='r', lw=3, label='Cynicality')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('Rep users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_rep_cynicality_CDF'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/rep_cynicality_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            all_gull_list = list(grouped_mean_neut['gull'])
            all_cyn_list = list(grouped_mean_neut['cyn'])
            all_susc_list = list(grouped_mean_neut['susc'])


            neut_gull_list_sort = sorted(all_gull_list)
            neut_cyn_list_sort = sorted(all_cyn_list)
            neut_susc_list_sort = sorted(all_susc_list)




            mplpl.hist(all_susc_list, bins=10, color='g')
            mplpl.xlabel('Susceptibility')
            mplpl.ylabel('Frequency')
            mplpl.title('Neut users, Mean : ' + str(np.mean(all_susc_list)))
            # mplpl.xlim([0,1])
            if approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/neut_susc'
            elif approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_neut_susc'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_susc_list)
            counts, bin_edges = np.histogram(all_susc_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='g', lw=3, label='Susceptibility')
            mplpl.xlabel('Susceptibility')
            mplpl.ylabel('CDF')
            mplpl.title('Neut users, Mean : ' + str(np.mean(all_susc_list)))
            # mplpl.xlim([0,1])
            if approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/neut_susc_CDF'
            elif approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_neut_susc_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            mplpl.hist(all_gull_list, bins=10, color='g')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('Frequency')
            mplpl.title('Neut users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            if approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/neut_gullibility'
            elif approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_neut_gullibility'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            num_bins = len(all_gull_list)
            counts, bin_edges = np.histogram(all_gull_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='g', lw=3, label='Gullibility')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('Neut users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            if approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/neut_gullibility_CDF'
            elif approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_neut_gullibility_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            mplpl.hist(all_cyn_list, bins=10, color='g')
            mplpl.xlabel('Cynically')
            mplpl.ylabel('Frequency')
            mplpl.title('Neut users, Mean : ' + str(np.mean(all_cyn_list)))
            # mplpl.xlim([0,1])
            if approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/neut_cynicality'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_neut_cynicality'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            num_bins = len(all_cyn_list)
            counts, bin_edges = np.histogram(all_cyn_list, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='g', lw=3, label='Cynicality')
            mplpl.xlabel('Gullibility')
            mplpl.ylabel('CDF')
            mplpl.title('Neut users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_neut_cynicality_CDF'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/neut_cynicality_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()









            mplpl.scatter(range(len(dem_susc_list_sort)),dem_susc_list_sort, s=40,color='b',marker='o', label='Democrats : ' + str(np.round(np.mean(dem_susc_list_sort),3)))
            mplpl.plot(range(len(dem_susc_list_sort)), dem_susc_list_sort, color='b')
            mplpl.scatter(range(len(rep_susc_list_sort)),rep_susc_list_sort, s=40,color='r',marker='s', label='Republicans : ' + str(np.round(np.mean(rep_susc_list_sort),3)))
            mplpl.plot(range(len(rep_susc_list_sort)), rep_susc_list_sort, color='r')
            mplpl.scatter(range(len(neut_susc_list_sort)),neut_susc_list_sort, s=40,color='g',marker='<', label='Neutrals : ' + str(np.round(np.mean(neut_susc_list_sort),3)))
            mplpl.plot(range(len(neut_susc_list_sort)), neut_susc_list_sort, color='g')
            mplpl.xlabel('Workers rank')
            mplpl.ylabel('Susceptibility')
            # mplpl.title('All users, Mean : ' + str(np.mean(dem_gull_list_sort)))
            # mplpl.ylim([0,1])
            mplpl.legend(loc="upper left")
            pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_dem_rep_neut_susc_workers'
            # pp = remotedir + local_dir_saving + '/fig_exp2/dem_rep_neut_gullibility_workers'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            mplpl.scatter(range(len(dem_gull_list_sort)),dem_gull_list_sort, s=40,color='b',marker='o', label='Democrats : ' + str(np.round(np.mean(dem_gull_list_sort),3)))
            mplpl.plot(range(len(dem_gull_list_sort)), dem_gull_list_sort, color='b')
            mplpl.scatter(range(len(rep_gull_list_sort)),rep_gull_list_sort, s=40,color='r',marker='s', label='Republicans : ' + str(np.round(np.mean(rep_gull_list_sort),3)))
            mplpl.plot(range(len(rep_gull_list_sort)), rep_gull_list_sort, color='r')
            mplpl.scatter(range(len(neut_gull_list_sort)),neut_gull_list_sort, s=40,color='g',marker='<', label='Neutrals : ' + str(np.round(np.mean(neut_gull_list_sort),3)))
            mplpl.plot(range(len(neut_gull_list_sort)), neut_gull_list_sort, color='g')
            mplpl.xlabel('Workers rank')
            mplpl.ylabel('Gulliblity')
            # mplpl.title('All users, Mean : ' + str(np.mean(dem_gull_list_sort)))
            # mplpl.ylim([0,1])
            mplpl.legend(loc="upper left")
            pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_dem_rep_neut_gullibility_workers'
            # pp = remotedir + local_dir_saving + '/fig_exp2/dem_rep_neut_gullibility_workers'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            mplpl.scatter(range(len(dem_cyn_list_sort)), dem_cyn_list_sort, s=40,color='b',marker='o', label='Democrats : ' + str(np.round(np.mean(dem_cyn_list_sort),3)))
            mplpl.plot(range(len(dem_cyn_list_sort)), dem_cyn_list_sort, color='b')
            mplpl.scatter(range(len(rep_cyn_list_sort)), rep_cyn_list_sort, s=40,color='r',marker='s', label='Republicans : ' + str(np.round(np.mean(rep_cyn_list_sort),3)))
            mplpl.plot(range(len(rep_cyn_list_sort)), rep_cyn_list_sort,color='r')
            mplpl.scatter(range(len(neut_cyn_list_sort)), neut_cyn_list_sort, s=40,color='g',marker='<', label='Neutrals : ' + str(np.round(np.mean(neut_cyn_list_sort),3)))
            mplpl.plot(range(len(neut_cyn_list_sort)), neut_cyn_list_sort, color='g')
            mplpl.xlabel('Workers rank')
            mplpl.ylabel('Cynicality')
            # mplpl.title('All users, Mean : ' + str(np.mean(dem_cyn_list_sort)))
            # mplpl.ylim([0,1])
            mplpl.legend(loc="upper left")

            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_dem_rep_neut_synicality_workers'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/dem_rep_neut_synicality_workers'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

            exit()


            # print(np.mean(grouped_sum['gull']))
            # print(np.mean(grouped_sum['cyn']))
            #
            #
            # print(np.mean(grouped_sum_dem['gull']))
            # print(np.mean(grouped_sum_dem['cyn']))
            #
            #
            # print(np.mean(grouped_sum_rep['gull']))
            # print(np.mean(grouped_sum_rep['cyn']))
            #
            #
            # print(np.mean(grouped_sum_neut['gull']))
            # print(np.mean(grouped_sum_neut['cyn']))
            #
            #
            # print(set(df_dem['worker_id']))
            # print(set(df_rep['worker_id']))
            # print(set(df_neut['worker_id']))
            # exit()

            tweet_rel_score = collections.defaultdict()
            tweet_intst_score = collections.defaultdict()
            tweet_rel_score_l = []
            tweet_intst_score_l = []
            tweet_non_rumor_rel_score_l = []
            tweet_non_rumor_intst_score_l = []
            tweet_rumor_rel_score_l = []
            tweet_rumor_intst_score_l = []
            tweet_pop = []
            tweet_pop_rumor = []
            tweet_pop_non_rumor = []


            users='all'
            # users = 'dem'
            # users = 'rep'
            # users = 'neut'


            if users == 'all':
                df_m = df.copy()
                col = 'k'
            elif users == 'dem':
                df_m = df_dem.copy()
                col = 'b'
            elif users == 'rep':
                df_m = df_rep.copy()
                col = 'r'
            elif users == 'neut':
                df_m = df_neut.copy()
                col = 'g'

            groupby_ftr = 'tweet_id'
            grouped = df_m.groupby(groupby_ftr, sort=False)
            grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

            groupby_ftr = 'worker_id'
            grouped = df_m.groupby(groupby_ftr, sort=False)
            grouped_sum_w = df_m.groupby(groupby_ftr, sort=False).sum()


            for tweet_id in tweet_rel_dict:
                # tweet_rel_score[tweet_id] =   np.sum(tweet_rel_dict[tweet_id])/float(len(tweet_rel_dict[tweet_id]))
                # tweet_intst_score[tweet_id] = np.sum(tweet_intst_dict[tweet_id]) / float(len(tweet_intst_dict[tweet_id]))

                tweet_rel_score[tweet_id] = grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w))

            tweet_id_sorted_list = sorted(tweet_rel_score, key=tweet_rel_score.get,reverse=False)
            for tweet_id in tweet_id_sorted_list:

                # tweet_rel_score_l.append(np.sum(tweet_rel_dict[tweet_id])/float(len(tweet_rel_dict[tweet_id])))
                # tweet_intst_score_l.append(np.sum(tweet_intst_dict[tweet_id]) / float(len(tweet_intst_dict[tweet_id])))

                tweet_rel_score_l.append(grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w)))

                tweet_pop.append(tweet_popularity[tweet_id])

                if tweet_id in tweet_rumor:
                    tweet_rumor_rel_score_l.append(grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w)))
                    tweet_pop_rumor.append(tweet_popularity[tweet_id])

                if tweet_id in tweet_non_rumor:
                    tweet_non_rumor_rel_score_l.append(grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w)))
                    tweet_pop_non_rumor.append(tweet_popularity[tweet_id])

            if approach == 'weighted':
                rumor_outF = open(remotedir + local_dir_saving + 'weighted_rumor_outF_exp2.txt', 'w')
                non_rumor_outF = open(remotedir + local_dir_saving + 'weighted_non-rumor_outF_exp2.txt', 'w')
            elif approach == 'non-weighted':
                rumor_outF = open(remotedir + local_dir_saving + 'rumor_outF_exp2.txt', 'w')
                non_rumor_outF = open(remotedir + local_dir_saving + 'non-rumor_outF_exp2.txt', 'w')

            for tweet_id in tweet_id_sorted_list:
                if tweet_id in tweet_rumor:
                    rumor_outF.write('|| ' + str(tweet_id) + '|| ' + tweet_text_dic[tweet_id] + '||' +'Rumor' + '||'+ str(tweet_rel_score[tweet_id]) + '||'+
                          str(tweet_popularity[tweet_id])+'||\n')

                else:
                    non_rumor_outF.write('|| ' + str(tweet_id) + '|| ' +tweet_text_dic[tweet_id] + '||' +'Non-Rumor' + '||'+ str(tweet_rel_score[tweet_id]) + '||'+
                          str(tweet_popularity[tweet_id])+'||\n')

            # exit()

            mplpl.figure()

            mplpl.scatter(tweet_rel_score_l, tweet_pop)
            z = np.polyfit(tweet_rel_score_l, tweet_pop, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_rel_score_l, p(tweet_rel_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('popularity')
            mplpl.xlabel('Reliability score')
            mplpl.title('Rumors and Non-Rumors topics : ' + str(np.corrcoef(tweet_rel_score_l,tweet_pop)[0][1]))
            mplpl.yscale('log')
            # mplpl.show()
            mplpl.legend(loc="upper right")
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_all_popularity_Reliability_'+users
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/all_popularity_Reliability_'+users
            mplpl.savefig(pp, format='png')


            print(np.mean(tweet_rel_score_l))

            mplpl.figure()

            mplpl.scatter(tweet_rumor_rel_score_l, tweet_pop_rumor)
            z = np.polyfit(tweet_rumor_rel_score_l, tweet_pop_rumor, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_rumor_rel_score_l, p(tweet_rumor_rel_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('popularity')
            mplpl.xlabel('Reliability score')
            mplpl.title('Rumors topics : ' + str(np.corrcoef(tweet_rumor_rel_score_l,tweet_pop_rumor)[0][1]))
            mplpl.yscale('log')
            # mplpl.show()
            mplpl.legend(loc="upper right")
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_Rumors_popularity_Reliability_'+users
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/Rumors_popularity_Reliability_'+users
            mplpl.savefig(pp, format='png')

            print(np.mean(tweet_rumor_rel_score_l))
            mplpl.figure()

            mplpl.scatter(tweet_non_rumor_rel_score_l, tweet_pop_non_rumor)
            z = np.polyfit(tweet_non_rumor_rel_score_l, tweet_pop_non_rumor, 1)
            p = np.poly1d(z)
            mplpl.plot(tweet_non_rumor_rel_score_l, p(tweet_non_rumor_rel_score_l), 'r-', linewidth=4.0)
            mplpl.ylabel('popularity')
            mplpl.xlabel('Reliability score')
            mplpl.title('Non-Rumors topics : ' + str(np.corrcoef(tweet_non_rumor_rel_score_l,tweet_pop_non_rumor)[0][1]))
            mplpl.yscale('log')
            # mplpl.show()
            mplpl.legend(loc="upper right")
            # pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_Non-Rumors_popularity_Reliability_'+users
            pp = remotedir + local_dir_saving + '/fig_exp2/Non-Rumors_popularity_Reliability_'+users
            mplpl.savefig(pp, format='png')
            print(np.mean(tweet_non_rumor_rel_score_l))

            mplpl.figure()

            tweet_non_rumor_rel_score_l_sort = sorted(tweet_non_rumor_rel_score_l)
            tweet_rumor_rel_score_l_sort = sorted(tweet_rumor_rel_score_l)
            tweet_rel_score_l_sort = sorted(tweet_rel_score_l)



            mplpl.scatter(range(len(tweet_rel_score_l_sort)),tweet_rel_score_l_sort, s=40,color='k',marker='o', label='All news')
            mplpl.plot(range(len(tweet_rel_score_l_sort)), tweet_rel_score_l_sort, color='k')
            mplpl.scatter(range(len(tweet_rumor_rel_score_l_sort)),tweet_rumor_rel_score_l_sort, s=40,color='r',marker='s', label='Rumors')
            mplpl.plot(range(len(tweet_rumor_rel_score_l_sort)), tweet_rumor_rel_score_l_sort, color='r')
            mplpl.scatter(range(len(tweet_non_rumor_rel_score_l_sort)),tweet_non_rumor_rel_score_l_sort, s=40,color='g',marker='<', label='Non-Rumors')
            mplpl.plot(range(len(tweet_non_rumor_rel_score_l_sort)), tweet_non_rumor_rel_score_l_sort, color='g')
            mplpl.xlabel('News')
            mplpl.ylabel('Reliability')
            # mplpl.title('All users, Mean : ' + str(np.mean(dem_gull_list_sort)))
            # mplpl.ylim([0,1])
            mplpl.legend(loc="upper left")
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_'+users+'_reliability_news'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/'+users+'_reliability_news'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            num_bins = len(tweet_rel_score_l_sort)
            counts, bin_edges = np.histogram(tweet_rel_score_l_sort, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='k', lw=3, label='All news')


            num_bins = len(tweet_rumor_rel_score_l_sort)
            counts, bin_edges = np.histogram(tweet_rumor_rel_score_l_sort, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='r', lw=3, label='Rumors')

            num_bins = len(tweet_non_rumor_rel_score_l_sort)
            counts, bin_edges = np.histogram(tweet_non_rumor_rel_score_l_sort, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='g', lw=3, label='Non-Rumors')

            mplpl.xlabel('Reliability')
            mplpl.ylabel('CDF')
            # mplpl.title('Neut users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            # mplpl.ylim([0,1])
            mplpl.legend(loc="upper left")
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_'+users+'_reliability_CDF'
            if approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/'+users+'_reliability_CDF'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

    if args.t == "AMT_dataset_reliable_news_processing_snopes_weighted":
        publisher_leaning = 1
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
        inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
        print(inp_all)
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)

        news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
        news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

        line_count = 0
        tmp_dict = {}
        claims_list = []
        for i in range(0, 5):
            df_cat = news_cat_list[i]
            df_cat_f = news_cat_list_f[i]
            inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
            cat_count = 0
            for line in inF:
                claims_list.append(line)
                cat_count += 1

        sample_tweets_exp1 = []

        tweet_txt_dict = {}
        tweet_date_dict = {}
        tweet_lable_dict = {}

        for line in claims_list:
            line_splt = line.split('<<||>>')
            publisher_name = int(line_splt[2])
            tweet_txt = line_splt[3]
            tweet_id = publisher_name
            cat_lable = line_splt[4]
            dat = line_splt[5]
            dt_splt = dat.split(' ')[0].split('-')
            m_day = int(dt_splt[2])
            m_month = int(dt_splt[1])
            m_year = int(dt_splt[0])
            m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
            tweet_txt_dict[tweet_id] = tweet_txt
            tweet_date_dict[tweet_id] = m_date
            tweet_lable_dict[tweet_id] = cat_lable

        # run = 'plot'
        run = 'analysis'
        run = 'second-analysis'
        exp1_list = sample_tweets_exp1


        if run == 'analysis':
            experiment = 1
            # experiment = 2
            # experiment = 3

            dem_list = []
            rep_list = []
            neut_list = []
            # valid users dict
            Vuser_dict = collections.defaultdict(int)
            query1 = "select workerid, count(*) from mturk_sp_claim_response_exp1_"+str(experiment)+"_recovery group by workerid;"

            cursor.execute(query1)
            res_exp2 = cursor.fetchall()
            for el in res_exp2:
                if el[1] == 51:
                    Vuser_dict[1000 + el[0]] = 1

            query2 = "select workerid, tweet_id, ra, rb, rc, at, text from mturk_sp_claim_response_exp1_"+str(experiment)+"_recovery;"

            cursor.execute(query2)
            res_exp2 = cursor.fetchall()

            res_exp1_l = []
            for el in res_exp2:
                if Vuser_dict[1000 + el[0]]==1:
                    res_exp1_l.append((1000 + el[0], el[1], el[2], el[3], el[4], el[5], el[6]))

            query3 = "select worker_id, demographic_nationality, demographic_residence, demographic_gender," \
                      " demographic_age, demographic_degree, demographic_employment, demographic_income," \
                      " demographic_political_view, demographic_race," \
                      " demographic_marital_status from mturk_sp_claim_demographics_"+str(experiment)+"_recovery"


            cursor.execute(query3)
            res_leaning = cursor.fetchall()
            leaning_dict = collections.defaultdict()
            dem_l = [];
            rep_l = [];
            neut_l = []

            w_nationality = {};w_residence = {};w_gender = {};w_age = {}
            w_degree = {};w_employment = {};w_income = {};w_political_view = {}
            w_race = {};w_marital_status = {}

            workerid_list_m  = []
            w_nationality_l = [];w_residence_l = [];w_gender_l = []
            w_age_l = [];w_degree_l = [];w_employment_l = []
            w_income_l = [];w_political_view_l = [];w_race_l = [];w_marital_status_l = []
            tweet_id_l = []; ra_l=[]; time_l=[];txt_list_m=[]
            for el in res_leaning:
                w_nationality[1000+el[0]] = el[1]
                w_residence[1000 + el[0]] = el[2]
                w_gender[1000 + el[0]] = el[3]
                w_age[1000 + el[0]] = el[4]
                w_degree[1000 + el[0]] = el[5]
                w_employment[1000 + el[0]] = el[6]
                w_income[1000 + el[0]] = el[7]
                w_political_view[1000 + el[0]] = el[8]
                w_race[1000 + el[0]] = el[9]
                w_marital_status[1000 + el[0]] = el[10]


                workerid_list_m.append(el[0]+1000)
                w_nationality_l.append(el[1])
                w_residence_l.append(el[2])
                w_gender_l.append(el[3])
                w_age_l.append(el[4])
                w_degree_l.append(el[5])
                w_employment_l.append(el[6])
                w_income_l.append(el[7])
                w_political_view_l.append(el[8])
                w_race_l.append(el[9])
                w_marital_status_l.append(el[10])

            index_list_m = range(len(w_nationality_l))
            df_w = pd.DataFrame({'worker_id': Series(workerid_list_m, index=index_list_m),
                                'nationality': Series(w_nationality_l, index=index_list_m),
                                'residence': Series(w_residence_l, index=index_list_m),
                                'gender': Series(w_gender_l, index=index_list_m),
                                'age': Series(w_age_l, index=index_list_m),
                                'degree': Series(w_degree_l, index=index_list_m),
                                'employment': Series(w_employment_l, index=index_list_m),
                                'income': Series(w_income_l, index=index_list_m),
                                'political_view': Series(w_political_view_l, index=index_list_m),
                                'race': Series(w_race_l, index=index_list_m),
                                'marital_status': Series(w_marital_status_l, index=index_list_m),})

            workerid_list_m  = []
            w_nationality_l = [];w_residence_l = [];w_gender_l = []
            w_age_l = [];w_degree_l = [];w_employment_l = []
            w_income_l = [];w_political_view_l = [];w_race_l = [];w_marital_status_l = []
            tweet_id_l = []; ra_l=[]; time_l=[];txt_list_m=[];ra_gt_l=[]


            df_w.to_csv(remotedir  +'worker_amt_answers_sp_claims_exp'+str(experiment)+'.csv',
                      columns=df_w.columns, sep="\t", index=False)


            outF = open(remotedir + 'amt_st.txt' , 'w')
            for dem_att in df_w.columns:
                outF.write('=== ' +  dem_att +' ===\n')
                sum_feat = df_w.groupby(dem_att).sum()
                count_feat = df_w.groupby(dem_att).count()
                grouped = df_w.groupby(dem_att)

                for key in grouped.groups.keys():
                    outF.write('|| ' + str(key))
                outF.write('||\n')
                for key in grouped.groups.keys():
                    out_l = grouped.groups[key]
                    outF.write('|| ' + str(len(out_l)))
                outF.write('||\n')
            outF.close()
            # workerid, tweet_id, ra, rb, rc, at, text

            for el in res_exp1_l:
                if el[1]==1:
                    continue
                tweet_id_l.append(el[1])
                ra_l.append(el[2])
                time_l.append(el[5])
                txt_list_m.append(tweet_txt_dict[el[1]])
                ra_gt_l.append(tweet_lable_dict[el[1]])

                workerid_list_m.append(el[0])
                w_nationality_l.append(w_nationality[el[0]])
                w_residence_l.append(w_residence[el[0]])
                w_gender_l.append(w_gender[el[0]])
                w_age_l.append(w_age[el[0]])
                w_degree_l.append(w_degree[el[0]])
                w_employment_l.append(w_employment[el[0]])
                w_income_l.append(w_income[el[0]])
                w_political_view_l.append(w_political_view[el[0]])
                w_race_l.append(w_race[el[0]])
                w_marital_status_l.append(w_marital_status[el[0]])

            index_list_m = range(len(w_nationality_l))
            df = pd.DataFrame({'worker_id': Series(workerid_list_m, index=index_list_m),
                               'tweet_id': Series(tweet_id_l, index=index_list_m),
                               'ra': Series(ra_l, index=index_list_m),
                               'ra_gt': Series(ra_gt_l, index=index_list_m),
                               'time': Series(time_l, index=index_list_m),
                               'text': Series(txt_list_m, index=index_list_m),
                                'nationality': Series(w_nationality_l, index=index_list_m),
                                'residence': Series(w_residence_l, index=index_list_m),
                                'gender': Series(w_gender_l, index=index_list_m),
                                'age': Series(w_age_l, index=index_list_m),
                                'degree': Series(w_degree_l, index=index_list_m),
                                'employment': Series(w_employment_l, index=index_list_m),
                                'income': Series(w_income_l, index=index_list_m),
                                'political_view': Series(w_political_view_l, index=index_list_m),
                                'race': Series(w_race_l, index=index_list_m),
                                'marital_status': Series(w_marital_status_l, index=index_list_m),})

            worker_id_list = []


            df.to_csv(remotedir  +'amt_answers_sp_claims_exp'+str(experiment)+'.csv',
                      columns=df.columns, sep="\t", index=False)



        else:
            # outF = open(remotedir+local_dir_saving+'amt_consensus_output_wiki_offensive_2.txt','w')
            approach = 'weighted'
            # approach = 'non-weighted'
            df = collections.defaultdict()
            df_w = collections.defaultdict()
            inp1 = remotedir  +'amt_answers_sp_claims_exp1.csv'
            inp1_w = remotedir  +'worker_amt_answers_sp_claims_exp1.csv'
            df[1] = pd.read_csv(inp1, sep="\t")
            df_w[1] = pd.read_csv(inp1_w, sep="\t")

            inp1 = remotedir  +'amt_answers_sp_claims_exp2.csv'
            inp1_w = remotedir  +'worker_amt_answers_sp_claims_exp2.csv'
            df[2] = pd.read_csv(inp1, sep="\t")
            df_w[2] = pd.read_csv(inp1_w, sep="\t")


            inp1 = remotedir  +'amt_answers_sp_claims_exp3.csv'
            inp1_w = remotedir  +'worker_amt_answers_sp_claims_exp3.csv'
            df[3] = pd.read_csv(inp1, sep="\t")
            df_w[3] = pd.read_csv(inp1_w, sep="\t")




            # sum_feat = df.groupby(groupby_ftr).sum()
            # count_feat = df.groupby(groupby_ftr).count()

            for ind in [1, 2, 3]:

                df[ind].loc[:,'rel_v'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'rel_gt_v'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'delta_time'] = df[ind]['tweet_id'] * 0.0

                df[ind].loc[:, 'err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'gull'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'cyn'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'susc'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'acc'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'leaning'] = df[ind]['tweet_id'] * 0.0
                tweet_rel_dict = collections.defaultdict(list)
                tweet_intst_dict = collections.defaultdict(list)
                rel_4 = 0
                for index in df[ind].index.tolist():
                    tweet_id = df[ind]['tweet_id'][index]
                    if tweet_id == 1:
                        continue

                    ra = df[ind]['ra'][index]
                    ra_gt = df[ind]['ra_gt'][index]

                    if ra==1:
                        rel = -3
                    elif ra==2:
                        rel=-2
                    elif ra==3:
                        rel=-1


                    elif ra==4:
                        rel=0
                    elif ra==5:
                        rel = 1
                    elif ra==6:
                        rel = 2
                    elif ra==7:
                        rel = 3



                    rel = rel / float(3)
                    # df[ind]['rel_v'][index] = np.abs(rel)
                    df[ind]['rel_v'][index] = rel

                    tweet_rel_dict[tweet_id].append(rel)

                    # news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                    # news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                    if ra_gt == 'FALSE':
                        rel_gt = -2
                    if ra_gt == 'MOSTLY FALSE':
                        rel_gt = -1
                    if ra_gt == 'MIXTURE':
                        rel_gt = 0
                    if ra_gt == 'MOSTLY TRUE':
                        rel_gt = 1
                    if ra_gt == 'TRUE':
                        rel_gt = 2

                    df[ind]['rel_gt_v'][index] = rel_gt / float(2)


                    l_scr = df[ind]['rel_gt_v'][index]

                    err_val = rel  - l_scr

                    gull_val = rel  - l_scr
                    gull_val = gull_val * ((np.sign(gull_val) + 1) / float(2))

                    cyn_val = l_scr  - rel
                    cyn_val = cyn_val * ((np.sign(cyn_val) + 1) / float(2))

                    df[ind]['err'][index] = err_val
                    df[ind]['gull'][index] = gull_val / float(2)
                    df[ind]['cyn'][index] = cyn_val / float(2)
                    df[ind]['susc'][index] = (df[ind]['cyn'][index] + df[ind]['gull'][index])/float(2)

                    # if rel > 0  and rel_gt > 0:
                    #     df[ind]['acc'][index]  = 1
                    # elif rel < 0 and rel_gt < 0:
                    #     df[ind]['acc'][index]  = 1
                    # elif rel == 0 and rel_gt == 0:
                    #     df[ind]['acc'][index]  = 1
                    # else:
                    #     df[ind]['acc'][index]  = 0

                    if rel > 0 and rel_gt > 0:
                        df[ind]['acc'][index] = 1
                    elif rel == 0 and rel_gt > 0:
                        df[ind]['acc'][index] = -1
                    elif rel < 0 and rel_gt < 0:
                        df[ind]['acc'][index] = 1
                    elif rel == 0 and rel_gt < 0:
                        df[ind]['acc'][index] = -1
                    elif rel == 0 and rel_gt == 0:
                        df[ind]['acc'][index] = 1
                    else:
                        df[ind]['acc'][index] = 0


                    if 'liberal' in df[ind]['political_view'][index]:
                        df[ind]['leaning'][index]=1
                    elif 'conservative' in df[ind]['political_view'][index]:
                        df[ind]['leaning'][index]=-1
                    elif 'moderate' in df[ind]['political_view'][index]:
                        df[ind]['leaning'][index] = 0
                    else:
                        df[ind]['leaning'][index] = -10


            for ind in [1,2,3]:
                df[ind].to_csv(remotedir  +'amt_answers_sp_claims_exp'+str(ind)+'_final.csv',
                          columns=df[ind].columns, sep="\t", index=False)



    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted":

        weighted=False
        # dataset = 'snopes'
        # dataset = 'mia'
        dataset = 'politifact'
        # dataset = 'snopes_nonpol'
        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                #     tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'snopes_nonpol':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'


            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            print(inp_all)
            source_dict = {}
            text_dict = {}
            date_dict = {}
            # outF = open(remotedir + 'politifact_last_100_news.txt', 'w')
            # F = open(remotedir + 'snopes_latest_20_news_per_lable_non_politics.txt', 'r')


            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable

        # run = 'plot'
        # run = 'analysis'
        run = 'second-analysis'
        exp1_list = sample_tweets_exp1

        if run == 'analysis':
            experiment = 1
            # experiment = 2
            # experiment = 3

            dem_list = []
            rep_list = []
            neut_list = []
            # valid users dict
            Vuser_dict = collections.defaultdict(int)
            if dataset=='snopes':
                query1 = "select workerid, count(*) from mturk_sp_claim_response_exp1_"+str(experiment)+"_recovery group by workerid;"
            elif dataset=='snopes_nonpol':
                query1 = "select workerid, count(*) from mturk_sp_claim_nonpol_response_exp1_recovery group by workerid;"
            elif dataset=='politifact':
                query1 = "select workerid, count(*) from mturk_pf_claim_response_exp1_" + str(experiment) + "_recovery group by workerid;"
            elif dataset=='mia':
                query1 = "select workerid, count(*) from mturk_m_claim_response_exp1_recovery group by workerid;"

            cursor.execute(query1)
            res_exp2 = cursor.fetchall()
            for el in res_exp2:
                if el[1] == 101:
                    Vuser_dict[1000*experiment + el[0]] = 1

            if dataset=='snopes':
                query2 = "select workerid, tweet_id, ra, rb, rc, at, text from mturk_sp_claim_response_exp1_"+str(experiment)+"_recovery;"
            elif dataset == 'snopes_nonpol':
                query2 = "select workerid, tweet_id, ra, rb, rc, at, text from mturk_sp_claim_nonpol_response_exp1_recovery;"
            elif dataset=='politifact':
                query2 = "select workerid, tweet_id, ra, rb, rc, at, text from mturk_pf_claim_response_exp1_"+str(experiment)+"_recovery;"

            elif dataset == 'mia':
                query2 = "select workerid, tweet_id, ra, rb, rc, at, text from mturk_m_claim_response_exp1_recovery;"




            cursor.execute(query2)
            res_exp2 = cursor.fetchall()

            res_exp1_l = []
            for el in res_exp2:
                if Vuser_dict[1000*experiment + el[0]]==1:
                    res_exp1_l.append((1000*experiment + el[0], el[1], el[2], el[3], el[4], el[5], el[6]))

            if dataset=='snopes':
                query3 = "select worker_id, demographic_nationality, demographic_residence, demographic_gender," \
                          " demographic_age, demographic_degree, demographic_employment, demographic_income," \
                          " demographic_political_view, demographic_race," \
                          " demographic_marital_status from mturk_sp_claim_demographics_"+str(experiment)+"_recovery"
            elif dataset == 'snopes_nonpol':
                query3 = "select worker_id, demographic_nationality, demographic_residence, demographic_gender," \
                          " demographic_age, demographic_degree, demographic_employment, demographic_income," \
                          " demographic_political_view, demographic_race," \
                          " demographic_marital_status from mturk_sp_claim_nonpol_demographics1_recovery"


            elif dataset=='politifact':
                query3 = "select worker_id, demographic_nationality, demographic_residence, demographic_gender," \
                          " demographic_age, demographic_degree, demographic_employment, demographic_income," \
                          " demographic_political_view, demographic_race," \
                          " demographic_marital_status from mturk_pf_claim_demographics_"+str(experiment)+"_recovery"

            elif dataset == 'mia':
                query3 = "select worker_id, demographic_nationality, demographic_residence, demographic_gender," \
                          " demographic_age, demographic_degree, demographic_employment, demographic_income," \
                          " demographic_political_view, demographic_race," \
                          " demographic_marital_status from mturk_m_claim_demographics_recovery"


            cursor.execute(query3)
            res_leaning = cursor.fetchall()
            leaning_dict = collections.defaultdict()
            dem_l = [];
            rep_l = [];
            neut_l = []

            w_nationality = {};w_residence = {};w_gender = {};w_age = {}
            w_degree = {};w_employment = {};w_income = {};w_political_view = {}
            w_race = {};w_marital_status = {}

            workerid_list_m  = []
            w_nationality_l = [];w_residence_l = [];w_gender_l = []
            w_age_l = [];w_degree_l = [];w_employment_l = []
            w_income_l = [];w_political_view_l = [];w_race_l = [];w_marital_status_l = []
            tweet_id_l = []; ra_l=[]; time_l=[];txt_list_m=[]
            for el in res_leaning:
                w_nationality[1000*experiment+el[0]] = el[1]
                w_residence[1000*experiment + el[0]] = el[2]
                w_gender[1000*experiment + el[0]] = el[3]
                w_age[1000*experiment + el[0]] = el[4]
                w_degree[1000*experiment + el[0]] = el[5]
                w_employment[1000*experiment + el[0]] = el[6]
                w_income[1000*experiment + el[0]] = el[7]
                w_political_view[1000*experiment + el[0]] = el[8]
                w_race[1000*experiment + el[0]] = el[9]
                w_marital_status[1000*experiment + el[0]] = el[10]


                workerid_list_m.append(el[0]+1000*experiment)
                w_nationality_l.append(el[1])
                w_residence_l.append(el[2])
                w_gender_l.append(el[3])
                w_age_l.append(el[4])
                w_degree_l.append(el[5])
                w_employment_l.append(el[6])
                w_income_l.append(el[7])
                w_political_view_l.append(el[8])
                w_race_l.append(el[9])
                w_marital_status_l.append(el[10])

            index_list_m = range(len(w_nationality_l))
            df_w = pd.DataFrame({'worker_id': Series(workerid_list_m, index=index_list_m),
                                'nationality': Series(w_nationality_l, index=index_list_m),
                                'residence': Series(w_residence_l, index=index_list_m),
                                'gender': Series(w_gender_l, index=index_list_m),
                                'age': Series(w_age_l, index=index_list_m),
                                'degree': Series(w_degree_l, index=index_list_m),
                                'employment': Series(w_employment_l, index=index_list_m),
                                'income': Series(w_income_l, index=index_list_m),
                                'political_view': Series(w_political_view_l, index=index_list_m),
                                'race': Series(w_race_l, index=index_list_m),
                                'marital_status': Series(w_marital_status_l, index=index_list_m),})

            workerid_list_m  = []
            w_nationality_l = [];w_residence_l = [];w_gender_l = []
            w_age_l = [];w_degree_l = [];w_employment_l = []
            w_income_l = [];w_political_view_l = [];w_race_l = [];w_marital_status_l = []
            tweet_id_l = []; ra_l=[]; time_l=[];txt_list_m=[];ra_gt_l=[]


            if dataset=='snopes':
                df_w.to_csv(remotedir  +'worker_amt_answers_sp_claims_exp'+str(experiment)+'.csv',
                          columns=df_w.columns, sep="\t", index=False)
            elif dataset == 'snopes_nonpol':
                df_w.to_csv(remotedir + 'worker_amt_answers_sp_claims_nonpol_exp' + str(experiment) + '.csv',
                            columns=df_w.columns, sep="\t", index=False)
            elif dataset=='politifact':
                df_w.to_csv(remotedir  +'worker_amt_answers_pf_claims_exp'+str(experiment)+'.csv',
                          columns=df_w.columns, sep="\t", index=False)

            elif dataset == 'mia':
                df_w.to_csv(remotedir  +'worker_amt_answers_mia_claims_exp1.csv',
                          columns=df_w.columns, sep="\t", index=False)




            outF = open(remotedir + 'amt_st.txt' , 'w')
            for dem_att in df_w.columns:
                outF.write('=== ' +  dem_att +' ===\n')
                sum_feat = df_w.groupby(dem_att).sum()
                count_feat = df_w.groupby(dem_att).count()
                grouped = df_w.groupby(dem_att)

                for key in grouped.groups.keys():
                    outF.write('|| ' + str(key))
                outF.write('||\n')
                for key in grouped.groups.keys():
                    out_l = grouped.groups[key]
                    outF.write('|| ' + str(len(out_l)))
                outF.write('||\n')
            outF.close()
            # workerid, tweet_id, ra, rb, rc, at, text

            for el in res_exp1_l:
                if el[1]==1:
                    continue
                tweet_id_l.append(el[1])
                ra_l.append(el[2])
                time_l.append(el[5])
                txt_list_m.append(tweet_txt_dict[el[1]])
                ra_gt_l.append(tweet_lable_dict[el[1]])

                workerid_list_m.append(el[0])
                w_nationality_l.append(w_nationality[el[0]])
                w_residence_l.append(w_residence[el[0]])
                w_gender_l.append(w_gender[el[0]])
                w_age_l.append(w_age[el[0]])
                w_degree_l.append(w_degree[el[0]])
                w_employment_l.append(w_employment[el[0]])
                w_income_l.append(w_income[el[0]])
                w_political_view_l.append(w_political_view[el[0]])
                w_race_l.append(w_race[el[0]])
                w_marital_status_l.append(w_marital_status[el[0]])

            index_list_m = range(len(w_nationality_l))
            df = pd.DataFrame({'worker_id': Series(workerid_list_m, index=index_list_m),
                               'tweet_id': Series(tweet_id_l, index=index_list_m),
                               'ra': Series(ra_l, index=index_list_m),
                               'ra_gt': Series(ra_gt_l, index=index_list_m),
                               'time': Series(time_l, index=index_list_m),
                               'text': Series(txt_list_m, index=index_list_m),
                                'nationality': Series(w_nationality_l, index=index_list_m),
                                'residence': Series(w_residence_l, index=index_list_m),
                                'gender': Series(w_gender_l, index=index_list_m),
                                'age': Series(w_age_l, index=index_list_m),
                                'degree': Series(w_degree_l, index=index_list_m),
                                'employment': Series(w_employment_l, index=index_list_m),
                                'income': Series(w_income_l, index=index_list_m),
                                'political_view': Series(w_political_view_l, index=index_list_m),
                                'race': Series(w_race_l, index=index_list_m),
                                'marital_status': Series(w_marital_status_l, index=index_list_m),})

            worker_id_list = []



            if dataset == 'snopes':
                df.to_csv(remotedir + 'amt_answers_sp_claims_exp' + str(experiment) + '.csv',
                          columns=df.columns, sep="\t", index=False)
            elif dataset == 'snopes_nonpol':
                df.to_csv(remotedir + 'amt_answers_sp_claims_nonpol_exp' + str(experiment) + '.csv',
                          columns=df.columns, sep="\t", index=False)
            elif dataset == 'politifact':
                df.to_csv(remotedir + 'amt_answers_pf_claims_exp' + str(experiment) + '.csv',
                          columns=df.columns, sep="\t", index=False)

            elif dataset == 'mia':
                df.to_csv(remotedir + 'amt_answers_mia_claims_exp.csv',
                          columns=df.columns, sep="\t", index=False)




        else:

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            if dataset == 'snopes':
                for i in range(1,4):
                    inp1 = remotedir  +'amt_answers_sp_claims_exp'+str(i)+'.csv'
                    inp1_w = remotedir  +'worker_amt_answers_sp_claims_exp'+str(i)+'.csv'
                    df[i] = pd.read_csv(inp1, sep="\t")
                    df_w[i] = pd.read_csv(inp1_w, sep="\t")

            elif dataset == 'snopes_nonpol':
                for i in range(1,2):
                    inp1 = remotedir  +'amt_answers_sp_claims_nonpol_exp'+str(i)+'.csv'
                    inp1_w = remotedir  +'worker_amt_answers_sp_claims_nonpol_exp'+str(i)+'.csv'
                    df[i] = pd.read_csv(inp1, sep="\t")
                    df_w[i] = pd.read_csv(inp1_w, sep="\t")


            elif dataset == 'politifact':
                for i in range(1, 4):
                    inp1 = remotedir + 'amt_answers_pf_claims_exp' + str(i) + '.csv'
                    inp1_w = remotedir + 'worker_amt_answers_pf_claims_exp' + str(i) + '.csv'
                    df[i] = pd.read_csv(inp1, sep="\t")
                    df_w[i] = pd.read_csv(inp1_w, sep="\t")

            elif dataset == 'mia':
                for i in range(1, 2):
                    inp1 = remotedir + 'amt_answers_mia_claims_exp.csv'
                    inp1_w = remotedir + 'worker_amt_answers_mia_claims_exp1.csv'
                    df[i] = pd.read_csv(inp1, sep="\t")
                    df_w[i] = pd.read_csv(inp1_w, sep="\t")

            mia_rum_t_l = []
            mia_nrum_t_l = []
            if dataset=='politifact' or dataset=='snopes':
                ind_l = [1,2,3]
            else:
                ind_l = [1]
            for ind in ind_l:

                df[ind].loc[:,'rel_v'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'rel_gt_v'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'delta_time'] = df[ind]['tweet_id'] * 0.0

                df[ind].loc[:, 'err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'vote'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'gull'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'cyn'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'susc'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'acc'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'leaning'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:,'delta_time'] = df[ind]['tweet_id'] * 0.0
                tweet_rel_dict = collections.defaultdict(list)
                tweet_intst_dict = collections.defaultdict(list)
                rel_4 = 0
                for index in df[ind].index.tolist():
                    tweet_id = df[ind]['tweet_id'][index]
                    if tweet_id == 1:
                        continue

                    ra = df[ind]['ra'][index]
                    ra_gt = df[ind]['ra_gt'][index]

                    # if ra==1:
                    #     rel = -3
                    # elif ra==2:
                    #     rel=-2
                    # elif ra==3:
                    #     rel=-1
                    #
                    #
                    # elif ra==4:
                    #     rel=0
                    # elif ra==5:
                    #     rel = 1
                    # elif ra==6:
                    #     rel = 2
                    # elif ra==7:
                    #     rel = 3


                    if ra==1:
                        rel = -2
                    elif ra==2:
                        rel=-2
                    elif ra==3:
                        rel=-2
                    elif ra==4:
                        rel=0
                    elif ra==5:
                        rel = 2
                    elif ra==6:
                        rel = 2
                    elif ra==7:
                        rel = 2





                    # rel = rel / float(3)
                    rel = rel / float(2)
                    df[ind]['rel_v'][index] = rel
                    if rel < 0:
                        df[ind]['vote'][index] = rel
                    else:
                        df[ind]['vote'][index] = 0
                    tweet_rel_dict[tweet_id].append(rel)

                    if dataset == 'snopes':
                        if ra_gt == 'FALSE':
                            rel_gt = -2
                        if ra_gt == 'MOSTLY FALSE':
                            rel_gt = -2
                        if ra_gt == 'MIXTURE':
                            rel_gt = 0
                        if ra_gt == 'MOSTLY TRUE':
                            rel_gt = 2
                        if ra_gt == 'TRUE':
                            rel_gt = 2
                        df[ind]['rel_gt_v'][index] = rel_gt / float(2)


                    elif dataset == 'snopes_nonpol':
                        if ra_gt == 'FALSE':
                            rel_gt = -2
                        if ra_gt == 'MOSTLY FALSE':
                            rel_gt = -2
                        if ra_gt == 'MIXTURE':
                            rel_gt = 0
                        if ra_gt == 'MOSTLY TRUE':
                            rel_gt = 2
                        if ra_gt == 'TRUE':
                            rel_gt = 2
                        df[ind]['rel_gt_v'][index] = rel_gt / float(2)


                    elif dataset == 'politifact':
                        if ra_gt == 'pants-fire':
                            rel_gt = -2
                        if ra_gt == 'false':
                            rel_gt = -2
                        if ra_gt == 'mostly-false':
                            rel_gt = -1
                        if ra_gt == 'half-true':
                            rel_gt = 0
                        if ra_gt == 'mostly-true':
                            rel_gt = 1
                        if ra_gt == 'true':
                            rel_gt = 2
                        if rel_gt>=0:
                            df[ind]['rel_gt_v'][index] = rel_gt / float(2)
                        else:
                            df[ind]['rel_gt_v'][index] = rel_gt / float(2)




                    elif dataset == 'mia':
                        if ra_gt == 'rumor':
                            rel_gt = -1
                            mia_rum_t_l.append(tweet_id)
                        if ra_gt == 'non-rumor':
                            rel_gt = 1
                            mia_nrum_t_l.append(tweet_id)
                        # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                        #     rel_gt = 0


                        df[ind]['rel_gt_v'][index] = rel_gt / float(1)


                    l_scr = df[ind]['rel_gt_v'][index]

                    err_val = rel  - l_scr

                    gull_val = rel  - l_scr
                    if gull_val==0:
                        gull_val = 0.01
                    else:
                        gull_val = gull_val * ((np.sign(gull_val) + 1) / float(2))

                    cyn_val = l_scr  - rel
                    if cyn_val==0:
                        cyn_val = 0.01
                    else:
                        cyn_val = cyn_val * ((np.sign(cyn_val) + 1) / float(2))

                    df[ind]['err'][index] = err_val
                    df[ind]['gull'][index] = gull_val
                    df[ind]['cyn'][index] = cyn_val
                    df[ind]['susc'][index] = df[ind]['cyn'][index] + df[ind]['gull'][index]


                    if rel > 0 and rel_gt > 0:
                        df[ind]['acc'][index] = 1
                    elif rel < 0 and rel_gt < 0:
                        df[ind]['acc'][index] = 1
                    elif rel == 0 and rel_gt == 0:
                        df[ind]['acc'][index] = 1
                    elif rel > 0 and rel_gt < 0:
                        df[ind]['acc'][index] = 0
                    elif rel < 0 and rel_gt > 0:
                        df[ind]['acc'][index] = 0

                    else:
                        df[ind]['acc'][index] = -1


                    if 'liberal' in df[ind]['political_view'][index]:
                        df[ind]['leaning'][index]=1
                    elif 'conservative' in df[ind]['political_view'][index]:
                        df[ind]['leaning'][index]=-1
                    elif 'moderate' in df[ind]['political_view'][index]:
                        df[ind]['leaning'][index] = 0
                    else:
                        df[ind]['leaning'][index] = -10




                df_gr = df[ind].groupby('worker_id', sort=False)
                worker_id_l = df_gr.groups.keys()
                cc = 0
                pr_tim = 0
                cur_tim = 0



                for workerid in worker_id_l:
                    cc = 0
                    pr_tim = 0
                    cur_tim = 0
                    ind_t = df_gr.groups[workerid]
                    df_wid = df[ind].iloc[ind_t]
                    df_wid = df_wid.sort('time', ascending=True)
                    for tmp_ind in df_wid.index.tolist():
                        cur_tim = df_wid['time'][tmp_ind]
                        if cc==0:
                            pr_tim=cur_tim
                        cc+=1
                        delta = cur_tim - pr_tim
                        pr_tim = cur_tim
                        df[ind]['delta_time'][tmp_ind] = delta
                    np.max(df_wid['time']) - np.min(df_wid['time'])

            # news_time_labling_F = open(remotedir + local_dir_saving + 'news_labling_time.txt','w')
            # news_time_labling_csv = open(remotedir + local_dir_saving + dataset +'_news_labling_time.csv','w')
            # #
            # df_gr = df[i].groupby('tweet_id', sort=False)
            # #
            # news_time_labling_csv.write('tweet_id\ttweet_text\t')
            # for w_id in worker_id_l:
            #     news_time_labling_csv.write(str(w_id) + '\t')
            # #
            # news_time_labling_csv.write('AVG\t')
            # news_time_labling_csv.write('\n')
            # #
            # my_ind = -1
            # for tweetid in df_gr.groups:
            #     ts = 0
            #     my_ind += 1
            #     if tweetid==1:
            #         continue
            #     ind_l = df_gr.groups[tweetid]
            #     df_wid = df.iloc[ind_l]
            #     # df_wid = df_wid.sort('time', ascending=True)
            #     # news_time_labling_F.write( '||' + tweet_text_dic[tweetid].replace("?", "'") +'||' + str(tweetid) + '||')
            #     news_time_labling_csv.write(str(my_ind)+'\t')
            #     m_text = tweet_text_dic[tweetid].replace("?", "'")
            #     m_text = m_text.replace("\t", " ")
            #     news_time_labling_csv.write(str(tweetid) + '\t' + m_text +'\t' )
            #     news_time_labling_csv.write(str(tweetid) + '\t' )
            # #
            #     for w_id in worker_id_l:
            #         ind_t = df_wid[df_wid['worker_id'] == w_id].index[0]
            #         ts+= int(df['delta_time'][ind_t])
            # #         news_time_labling_F.write(str(df['delta_time'][ind_t])+'||')
            #         news_time_labling_csv.write(str(df['delta_time'][ind_t]) + '\t')
            # #
            # #     news_time_labling_F.write((str(ts/float(len(worker_id_l)))) + '||')
            # #     news_time_labling_F.write('\n')
            # #
            #     news_time_labling_csv.write((str(ts/float(len(worker_id_l)))) + '\t')
            #     news_time_labling_csv.write('\n')
            #
            #
            # news_time_labling_csv.close()
            # input = remotedir + local_dir_saving  + dataset+'_news_labling_time.csv'
            # df_time = pd.read_csv(input, sep="\t")










            if dataset=='snopes':

                for ind in [1,2,3]:
                    # df[ind].to_csv(remotedir  +'amt_answers_sp_claims_exp'+str(ind)+'_final.csv',
                    #           columns=df[ind].columns, sep="\t", index=False)
                    df[ind].to_csv(remotedir + 'amt_answers_sp_claims_exp' + str(ind) + '_final_weighted.csv',
                                   columns=df[ind].columns, sep="\t", index=False)

            elif dataset=='snopes_nonpol':

                for ind in [1]:
                    # df[ind].to_csv(remotedir  +'amt_answers_sp_nonpol_claims_exp'+str(ind)+'_final.csv',
                    #           columns=df[ind].columns, sep="\t", index=False)
                    df[ind].to_csv(remotedir + 'amt_answers_sp_nonpol_claims_exp' + str(ind) + '_final_weighted.csv',
                                   columns=df[ind].columns, sep="\t", index=False)

            elif dataset == 'politifact':

                for ind in [1, 2, 3]:
                    # df[ind].to_csv(remotedir + 'amt_answers_pf_claims_exp' + str(ind) + '_final.csv',
                    #                columns=df[ind].columns, sep="\t", index=False)

                    df[ind].to_csv(remotedir + 'amt_answers_pf_claims_exp' + str(ind) + '_final_weighted.csv',
                                   columns=df[ind].columns, sep="\t", index=False)

            elif dataset == 'mia':
                mia_rum_t_l = np.array(list(set(mia_rum_t_l)))
                mia_nrum_t_l = list(set(mia_nrum_t_l))
                und_l = np.array([100012, 100016, 100053, 100038, 100048])

                mia_rum_t_l = list(np.setdiff1d(mia_rum_t_l,und_l))
                random.shuffle(mia_rum_t_l)
                mia_rum_t_l = mia_rum_t_l[:30]
                random.shuffle(mia_nrum_t_l)
                mia_nrum_t_l = mia_nrum_t_l[:30]
                tmp_l = mia_rum_t_l + mia_nrum_t_l

                df[1] = df[1][df[1]['tweet_id'].isin(tmp_l)]
                for ind in [1]:
                    # df[ind].to_csv(remotedir + 'amt_answers_mia_claims_exp1_fina.csv',
                    #                columns=df[ind].columns, sep="\t", index=False)
                    df[ind].to_csv(remotedir + 'amt_answers_mia_claims_exp1_final_weighted.csv',
                                   columns=df[ind].columns, sep="\t", index=False)

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation":



        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'



        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                # m_day = int(dt_splt[2])
                # m_month = int(dt_splt[1])
                # m_year = int(dt_splt[0])
                # m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                # tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name

        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0
        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        fig_f = True
        # fig_f = False
        if dataset=='snopes':
            data_n = 'sp'
        elif dataset=='politifact':
            data_n = 'pf'
        elif dataset=='mia':
            data_n = 'mia'

        for ind in [3]:
            if balance_f == 'balanced':
                inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
            else:
                inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
            inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
            df[ind] = pd.read_csv(inp1, sep="\t")
            df_w[ind] = pd.read_csv(inp1_w, sep="\t")

            df_m = df[ind].copy()

            groupby_ftr = 'tweet_id'
            grouped = df_m.groupby(groupby_ftr, sort=False)
            grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

            # df_tmp = df_m[df_m['tweet_id'] == t_id]
            for t_id in grouped.groups.keys():

                df_tmp = df_m[df_m['tweet_id'] == t_id]
                ind_t = df_tmp.index.tolist()[0]
                weights = []
                weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                val_list = list(df_tmp['rel_v'])
                tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                tweet_avg[t_id] = np.mean(val_list)
                tweet_med[t_id] = np.median(val_list)
                tweet_var[t_id] = np.var(val_list)
                tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                tweet_avg_l.append(np.mean(val_list))
                tweet_med_l.append(np.median(val_list))
                tweet_var_l.append(np.var(val_list))
                tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
                accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))

                all_acc.append(accuracy)

                if fig_f == True:
                    mplpl.rc('xtick', labelsize='x-large')
                    mplpl.rc('ytick', labelsize='x-large')

                    try:
                        df_tmp['rel_v'].plot(kind='kde', lw=8, color='g', label='Users\' perception')
                    except:
                        print('hmm')

                    # mplpl.hist(list(df_tmp['rel_v']), weights=weights, color='g')

                    mplpl.plot([df_tmp['rel_gt_v'][ind_t], df_tmp['rel_gt_v'][ind_t]], [0, 0.5], color='r',
                               label='Ground truth value', linewidth=10)

                    mplpl.ylabel('PDF', fontsize=25)
                    mplpl.xlabel('Users\' perception', fontsize=25)
                    if t_id== 1497:
                        titl=  'GT:True, PT:True'
                    elif t_id== 1492:
                        titl=  'GT:True, PT:False'
                    elif t_id== 1101:
                        titl=  'GT:False, PT:True'
                    elif t_id== 1111:
                        titl=  'GT:False, PT:False'
                    else:
                        titl = ''
                    mplpl.title(titl, fontsize=35)
                    #     df_tmp['ra_gt'][ind_t] + '  Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : ' + str(
                    #         np.round(np.mean(val_list), 3))
                    #     + ', med : ' + str(np.round(np.median(val_list), 3)) + ', Var : ' + str(
                    #         np.round(np.var(val_list), 3)))
                    mplpl.legend(loc="upper right", fontsize='x-large')
                    mplpl.xlim([-2, 2])
                    mplpl.ylim([0, 1])
                    mplpl.subplots_adjust(bottom=0.14)
                    if balance_f == 'balanced':
                        pp = remotedir + '/fig/fig_exp1/news_based/balanced/' + str(t_id) + '_rel_dist'
                    else:
                        pp = remotedir + '/fig/fig_exp1/news_based/' + str(t_id) + '_rel_dist.pdf'
                    # pp = remotedir  + '/fig/fig_exp1/acc/'+ str(t_id)+'_rel_dist'
                    mplpl.savefig(pp, format='pdf')
                    mplpl.figure()

                weights = []
                weights.append(np.ones_like(list(df_tmp['err'])) / float(len(df_tmp)))
                val_list = list(df_tmp['susc'])
                # val_list = list(df_tmp['err'])
                tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                tweet_dev_avg[t_id] = np.mean(val_list)
                tweet_dev_med[t_id] = np.median(val_list)
                tweet_dev_var[t_id] = np.var(val_list)

                tweet_dev_avg_l.append(np.mean(val_list))
                tweet_dev_med_l.append(np.median(val_list))
                tweet_dev_var_l.append(np.var(val_list))
                # if fig_f == True:
                #
                #     try:
                #         df_tmp['susc'].plot(kind='kde', lw=4, color='c', label='Worker Judgment')
                #     except:
                #         print('hmm')
                #
                #     mplpl.hist(list(df_tmp['susc']), weights=weights, color='c')
                #
                #     # mplpl.plot([df_tmp['rel_gt_v'][ind_t], df_tmp['rel_gt_v'][ind_t]], [0, 0.5], color='r',
                #     #            label='Ground Truth', linewidth=10)
                #
                #     mplpl.ylabel('Frequency')
                #     mplpl.xlabel('Worker judgment value - Ground truth value')
                #     mplpl.title(df_tmp['ra_gt'][ind_t] + '\n Avg : ' + str(np.round(np.mean(val_list), 3))
                #                 + ', med : ' + str(np.round(np.median(val_list), 3)) + ', Var : ' + str(
                #         np.round(np.var(val_list), 3)))
                #     mplpl.legend(loc="upper right")
                #     tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                #     # mplpl.subplots_adjust(bottom=0.25)
                #     mplpl.xlim([-1, 1])
                #     mplpl.ylim([0, 3])
                #     if balance_f == 'balanced':
                #         pp = remotedir + '/fig/fig_exp1/news_based/balanced/' + str(t_id) + '_p_susc_dist'
                #
                #     else:
                #         pp = remotedir + '/fig/fig_exp1/news_based/' + str(t_id) + '_p_susc_dist'
                #     mplpl.savefig(pp, format='png')
                #     mplpl.figure()

        exit()
        AVG_list = []
        print(np.mean(all_acc))
        outF = open(remotedir + 'table_out.txt', 'w')

        news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
        for cat_l in news_cat_list:
            outF.write('== ' + cat_l + ' ==\n\n')
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            # tweet_avg = {}
            # tweet_med = {}
            # tweet_var = {}
            # tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            AVG_susc_list = []
            AVG_wl_list = []
            all_acc = []
            for ind in [1, 2, 3]:
                # for ind in [3]:

                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_sp_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_sp_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_sp_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                df_m = df_m[df_m['ra_gt'] == cat_l]
                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    # tweet_avg[t_id] = [np.mean(val_list)]
                    # tweet_med[t_id] = [np.median(val_list)]
                    # tweet_var[t_id] = [np.var(val_list)]
                    tweet_gt_var[t_id] = [df_tmp['rel_gt_v'][ind_t]]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    AVG_wl_list += list(df_tmp['rel_v'])

                    AVG_susc_list += list(df_tmp['susc'])
                    val_list = list(df_tmp['susc'])
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)
                    df_tt = df_tmp[df_tmp['acc'] > -1]

                    accuracy = np.sum(df_tt['acc']) / float(len(df_tt))

                    all_acc.append(accuracy)

                    # all_acc.append(np.sum(df_tmp['acc'])/float(len(df_tmp)))
                    #
                    # tweet_dev_avg_l.append(np.mean(val_list))
                    # tweet_dev_med_l.append(np.median(val_list))
                    # tweet_dev_var_l.append(np.var(val_list))
            print(np.mean(all_acc))
            weights = []
            weights.append(np.ones_like(AVG_wl_list) / float(len(AVG_wl_list)))

            # df_tt = pd.DataFrame(np.array(AVG_wl_list), columns=["wl_g"])
            # # df_tt['wl_g'] = df_tt['wl_g']/([np.max(df_tt['wl_g'])]*len(df_tt))
            # # df_tt['wl_g'] = df_tt['wl_g']/([2]*len(df_tt))
            # try:
            #     df_tt['wl_g'].plot(kind='kde', lw=4, color='g', label='Worker Judgment')
            # except:
            #     print('hmm')
            #
            # mplpl.hist(AVG_wl_list, weights=weights, color='g')
            # #
            # mplpl.ylabel('Frequency')
            # mplpl.xlabel('Truth value of claim')
            # mplpl.title('Avg : ' + str(np.round(np.mean(AVG_wl_list),3))
            # + ', med : '+ str(np.round(np.median(AVG_wl_list),3)) + ', Var : ' + str(np.round(np.var(AVG_wl_list),3)))
            # mplpl.legend(loc="upper right")
            # mplpl.xlim([-2,2])
            # mplpl.ylim([0,1])
            # pp = remotedir  + '/fig/fig_exp1/news_based/'+ str(cat_l)+'_rel_dist'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            # #
            # weights = []
            # weights.append(np.ones_like(AVG_susc_list) / float(len(AVG_susc_list)))
            #
            # df_tt = pd.DataFrame(np.array(AVG_susc_list), columns=["gt"])
            # try:
            #     df_tt['gt'].plot(kind='kde', lw=4, color='c', label='Ground Truth')
            # except:
            #     print('hmm')
            #
            # mplpl.hist(AVG_susc_list, weights=weights, color='c')
            # #
            # mplpl.ylabel('Frequency')
            # mplpl.xlabel('Truth value of claim')
            # mplpl.title('Avg : ' + str(np.round(np.mean(AVG_susc_list),3))
            # + ', med : '+ str(np.round(np.median(AVG_susc_list),3)) + ', Var : ' + str(np.round(np.var(AVG_susc_list),3)))
            # mplpl.legend(loc="upper right")
            # mplpl.xlim([0,1])
            # mplpl.ylim([0,3])
            # pp = remotedir  + '/fig/fig_exp1/news_based/'+ str(cat_l)+'_susc_dist'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()




            # print(np.corrcoef(tweet_avg_l,tweet_gt_var_l)[0][1])
            # print(np.corrcoef(tweet_med_l,tweet_gt_var_l)[0][1])
            # print(np.corrcoef(tweet_var_l,tweet_gt_var_l)[0][1])
            #
            # print(np.corrcoef(tweet_dev_avg_l, tweet_gt_var_l)[0][1])
            # print(np.corrcoef(tweet_dev_med_l, tweet_gt_var_l)[0][1])
            # print(np.corrcoef(tweet_dev_var_l, tweet_gt_var_l)[0][1])




            # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get,reverse=False)
            # tweet_l_sort = sorted(tweet_med, key=tweet_med.get,reverse=False)
            # tweet_l_sort = sorted(tweet_var, key=tweet_var.get,reverse=False)

            # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get,reverse=False)
            # tweet_l_sort = sorted(tweet_dev_med, key=tweet_dev_med.get,reverse=False)
            tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            count = 0
            for t_id in tweet_l_sort:
                count += 1
                outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                           str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                           str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')


    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_time_analysis":



        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'



        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                # m_day = int(dt_splt[2])
                # m_month = int(dt_splt[1])
                # m_year = int(dt_splt[0])
                # m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                # tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name

        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0
        # for ind in [1,2,3]:
        all_acc = []



        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        fig_f = True
        # fig_f = False
        if dataset=='snopes':
            data_n = 'sp'
            ind_l = [1, 2, 3]
        elif dataset=='politifact':
            data_n = 'pf'
            ind_l = [1, 2, 3]
        elif dataset=='mia':
            data_n = 'mia'
            ind_l = [1]
        t_time_all = []
        for ind in ind_l:
            if balance_f == 'balanced':
                inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
            else:
                inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_time.csv'
            inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
            df[ind] = pd.read_csv(inp1, sep="\t")
            df_w[ind] = pd.read_csv(inp1_w, sep="\t")

            df_m = df[ind].copy()

            groupby_ftr = 'tweet_id'
            grouped = df_m.groupby(groupby_ftr, sort=False)
            grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

            # df_tmp = df_m[df_m['tweet_id'] == t_id]
            for t_id in grouped.groups.keys():

                df_tmp = df_m[df_m['tweet_id'] == t_id]
                ind_t = df_tmp.index.tolist()[0]
                weights = []
                weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                val_list = list(df_tmp['rel_v'])
                tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                tweet_avg[t_id] = np.mean(val_list)
                tweet_med[t_id] = np.median(val_list)
                tweet_var[t_id] = np.var(val_list)
                tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                tweet_avg_l.append(np.mean(val_list))
                tweet_med_l.append(np.median(val_list))
                tweet_var_l.append(np.var(val_list))
                tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
                accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))

                # all_acc.append(accuracy)

                t_time_avg = np.mean(list(df_tmp['delta_time']))
                t_time_all.append(t_time_avg)
                all_acc+=list(df_tmp['delta_time'])
        print(np.mean(t_time_all))
        print(np.mean(all_acc))

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_time_fig":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes_nonpol','snopes','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']
                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            if dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpolitical'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            w_cyn_dict= collections.defaultdict()
            w_gull_dict= collections.defaultdict()
            w_apb_dict= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            cat_time_dict = collections.defaultdict(dict)
            time_dict = collections.defaultdict()
            cat_time_list = collections.defaultdict(list)
            cat_time_dict_var = collections.defaultdict(dict)
            time_dict_var = collections.defaultdict()
            cat_time_list_var = collections.defaultdict(list)


            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)

                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)



                    cat_time_dict[tweet_lable_dict[t_id]][t_id] = np.mean(df_tmp['delta_time'])
                    cat_time_list[tweet_lable_dict[t_id]].append(np.mean(df_tmp['delta_time']))
                    time_dict[t_id] = np.mean(df_tmp['delta_time'])

                    cat_time_dict_var[tweet_lable_dict[t_id]][t_id] = np.std(df_tmp['delta_time'])
                    cat_time_list_var[tweet_lable_dict[t_id]].append(np.std(df_tmp['delta_time']))
                    time_dict_var[t_id] = np.std(df_tmp['delta_time'])


                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    # w_abs_err_list = list(df_tmp['abs_err'])
                    w_sus_list = list(df_tmp['susc'])
                    # w_norm_err_list = list(df_tmp['norm_err'])
                    # w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    # w_cyn_list = list(df_tmp['cyn'])
                    # w_gull_list = list(df_tmp['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])


                    df_cyn = df_tmp[df_tmp['cyn']>0]
                    df_gull = df_tmp[df_tmp['gull']>0]

                    w_cyn_list = list(df_cyn['cyn'])
                    w_gull_list = list(df_gull['gull'])

                    w_cyn_dict[t_id] = np.mean(w_cyn_list)
                    w_gull_dict[t_id] = np.mean(w_gull_list)
                    w_apb_dict[t_id] = np.mean(w_sus_list)


                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

            # news_cat_list_f = ['pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['false', 'mostly_false', 'mixture', 'mostly_true', 'true']

            for cat in cat_time_dict:
                print(cat +' : mean = '+ str(np.mean(cat_time_list[cat]))+' ,median = '+ str(np.median(cat_time_list[cat]))
                      +' ,std= '+ str(np.std(cat_time_list[cat])))

            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []


            out_dict = w_apb_dict
            # out_dict = w_gull_dict
            # out_dict = w_cyn_dict
            # out_dict = tweet_var
            # out_dict = time_dict

            tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
            t_l = []
            t_var_l = []
            gt = []
            pt_l_dict = collections.defaultdict(list)
            for t_id in tweet_l_sort:
                t_var_l.append(time_dict_var[t_id])
                t_l.append(time_dict[t_id])
                gt.append(out_dict[t_id])
                count = 0

            print(np.corrcoef(t_var_l,gt )[0][1])
            print(np.corrcoef(t_l,gt )[0][1])
            # print(sklearn.metrics.normalized_mutual_info_score(t_l,gt))
            # print(sklearn.metrics.normalized_mutual_info_score(t_var_l,gt))
            # print(sklearn.metrics.normalized_mutual_info_score(t_l,t_var_l))
            # print(sklearn.feature_selection.mutual_info_classif(t_l,t_var_l, discrete_features='auto', n_neighbors=3, copy=True, random_state=None))
            # exit()
            if dataset=='snopes' or dataset=='snopes_nonpol':
                col_l = ['darkred', 'orange', 'gray', 'lime', 'green']
                # col = 'purple'
                col = 'k'
                news_cat_list_n = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']
            if dataset=='politifact':
                col_l = ['darkred', 'red', 'orange', 'gray', 'lime', 'green']
                news_cat_list_n = ['PANTS ON FIRE', 'FALSE', 'MOSTLY FALSE', 'HALF TRUE', 'MOSTLY TRUE', 'TRUE']
                # col = 'c'
                col = 'k'
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            if dataset=='mia':
                col_l = ['red', 'green']
                news_cat_list_n = ['RUMORS', 'NON RUMORS']
                # col = 'brown'
                col = 'k'
                col_t_f = ['red', 'green']
                news_cat_list_t_f = [['rumors'], ['non-rumors']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            count = 0
            # Y = [0]*len(thr_list)




            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []

            fig_cdf = True
            # fig_cdf = False
            if fig_cdf==True:

        #####################################################33

                # out_dict = w_apb_dict
                # out_dict = w_gull_dict
                # out_dict = w_cyn_dict
                # out_dict = tweet_var
                out_dict = time_dict

                tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(out_dict[t_id])
                count=0

                # num_bins = len(pt_l)
                # counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5,linestyle='--', label='All news stories')
                # mplpl.rcParams['figure.figsize'] = 5.8, 3.7
                mplpl.rcParams['figure.figsize'] = 4.5, 2.5
                mplpl.rc('xtick', labelsize='large')
                mplpl.rc('ytick', labelsize='large')
                mplpl.rc('legend', fontsize='small')
                for cat_m in news_cat_list:
                    count += 1
                    pt_l_dict[cat_m] = []
                    for t_id in tweet_l_sort:
                        if tweet_lable_dict[t_id]==cat_m:
                            # if out_dict[t_id]>0 or out_dict[t_id]<=0:
                                pt_l_dict[cat_m].append(out_dict[t_id])



                    # df_tt = pd.DataFrame(np.array(pt_l_dict[cat_m]), columns=[cat_m])
                    # df_tt[cat_m].plot(kind='kde', lw=6, color=col_l[count-1], label=cat_m)

                    num_bins = len(pt_l_dict[cat_m])
                    counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_l[count-1], lw=5, label=cat_m)

                # mplpl.ylabel('PDF of Perceived Truth Level', fontsize=20, fontweight = 'bold')
                # mplpl.ylabel('CDF of \n Absolute Perception Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('CDF of \n False Positive Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('CDF of \n False Negative Bias', fontsize=13, fontweight = 'bold')

                # mplpl.ylabel('PDF of \n Absolute Perception Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('PDF of \n False Positive Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('PDF of \n False Negative Bias', fontsize=13, fontweight='bold')

                # mplpl.xlabel('Total Perception Bias', fontsize=24, fontweight = 'bold')
                # mplpl.xlabel('False Positive Bias', fontsize=24, fontweight = 'bold')
                mplpl.xlabel('Time', fontsize=20, fontweight = 'bold')
                mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
                mplpl.grid()
                mplpl.title(data_name, fontsize='x-large')
                labels = ['0.0', '0.25', '0.5', '0.75', '1.0']
                y = [0.0, 0.5, 1, 1.5, 2]
                # mplpl.yticks(y, labels)
                legend_properties = {'weight': 'bold'}


                # plt.legend(prop=legend_properties)
                # mplpl.legend(loc="upper left",prop=legend_properties,fontsize='small', ncol=1)#, fontweight = 'bold')
                mplpl.legend(loc="lower right",prop=legend_properties,fontsize='small', ncol=1)#, fontweight = 'bold')
                # mplpl.xlim([0, 1])
                # mplpl.ylim([0, 2])
                # mplpl.ylim([0, 1])
                mplpl.subplots_adjust(bottom=0.24)
                mplpl.subplots_adjust(left=0.18)

                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_pt_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_APB_cdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_FPB_cdf'
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_time_cdf'

                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_APB_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_FPB_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_FNB_pdf'

                mplpl.savefig(pp + '.pdf', format='pdf')
                mplpl.savefig(pp + '.png', format='png')
                mplpl.figure()


                exit()

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_corr_weighted_answering_nonweighted":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############



        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','politifact','snopes_nonpol','snopes','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']
                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            if dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpolitical'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            df_wei = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            w_cyn_dict= collections.defaultdict()
            w_gull_dict= collections.defaultdict()
            w_apb_dict= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            cat_time_dict = collections.defaultdict(dict)
            time_dict = collections.defaultdict()
            cat_time_list = collections.defaultdict(list)
            cat_time_dict_var = collections.defaultdict(dict)
            time_dict_var = collections.defaultdict()
            cat_time_list_var = collections.defaultdict(list)
            w_apb_dict_wei = collections.defaultdict()

            tweet_avg_wei= collections.defaultdict()
            # tweet_med[t_id] = np.median(val_list)
            tweet_var_wei= collections.defaultdict()

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                    inp1_wei = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_weighted.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_wei[ind] = pd.read_csv(inp1_wei, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()
                df_m_wei = df_wei[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    df_tmp_wei = df_m_wei[df_m_wei['tweet_id'] == t_id]
                    ind_t_wei = df_tmp_wei.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)

                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)



                    cat_time_dict[tweet_lable_dict[t_id]][t_id] = np.mean(df_tmp['delta_time'])
                    cat_time_list[tweet_lable_dict[t_id]].append(np.mean(df_tmp['delta_time']))
                    time_dict[t_id] = np.mean(df_tmp['delta_time'])

                    cat_time_dict_var[tweet_lable_dict[t_id]][t_id] = np.std(df_tmp['delta_time'])
                    cat_time_list_var[tweet_lable_dict[t_id]].append(np.std(df_tmp['delta_time']))
                    time_dict_var[t_id] = np.std(df_tmp['delta_time'])


                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    # w_abs_err_list = list(df_tmp['abs_err'])
                    w_sus_list = list(df_tmp['susc'])
                    w_sus_list_wei = list(df_tmp_wei['susc'])
                    # w_norm_err_list = list(df_tmp['norm_err'])
                    # w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    # w_cyn_list = list(df_tmp['cyn'])
                    # w_gull_list = list(df_tmp['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])


                    df_cyn = df_tmp[df_tmp['cyn']>0]
                    df_gull = df_tmp[df_tmp['gull']>0]

                    w_cyn_list = list(df_cyn['cyn'])
                    w_gull_list = list(df_gull['gull'])

                    w_cyn_dict[t_id] = np.mean(w_cyn_list)
                    w_gull_dict[t_id] = np.mean(w_gull_list)
                    w_apb_dict[t_id] = np.mean(w_sus_list)
                    w_apb_dict_wei[t_id] = np.mean(w_sus_list_wei)


                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]


                    val_list_wei = list(df_tmp_wei['rel_v'])
                    # tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg_wei[t_id] = np.mean(val_list_wei)
                    # tweet_med[t_id] = np.median(val_list)
                    tweet_var_wei[t_id] = np.var(val_list_wei)





                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

            # news_cat_list_f = ['pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['false', 'mostly_false', 'mixture', 'mostly_true', 'true']

            for cat in cat_time_dict:
                print(cat +' : mean = '+ str(np.mean(cat_time_list[cat]))+' ,median = '+ str(np.median(cat_time_list[cat]))
                      +' ,std= '+ str(np.std(cat_time_list[cat])))

            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []

            w_apb_dict
            w_apb_dict_wei

            out_dict = w_apb_dict_wei
            # out_dict = w_gull_dict
            # out_dict = w_cyn_dict
            # out_dict = tweet_var
            # out_dict = time_dict
            w_apb_wei_list = []
            w_apb_list = []

            tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
            t_l = []
            t_var_l = []
            gt = []
            var_list = []
            var_wei_list = []
            pt_l_dict = collections.defaultdict(list)
            for t_id in tweet_l_sort:
                # t_var_l.append(time_dict_var[t_id])
                w_apb_wei_list.append(w_apb_dict_wei[t_id])
                w_apb_list.append(w_apb_dict[t_id])
                var_list.append(tweet_var[t_id])
                var_wei_list.append(tweet_var_wei[t_id])
                count = 0

            # print(np.corrcoef(t_var_l,gt )[0][1])
            print(np.corrcoef(w_apb_wei_list,w_apb_list )[0][1])
            print(np.corrcoef(w_apb_wei_list,var_wei_list )[0][1])
            print(np.corrcoef(var_list,w_apb_list )[0][1])


            print(scipy.stats.spearmanr(w_apb_wei_list,w_apb_list ))
            print(scipy.stats.spearmanr(w_apb_wei_list,var_wei_list ))
            print(scipy.stats.spearmanr(var_list,w_apb_list ))

            # scipy.stats.spearmanr
            # print(sklearn.metrics.normalized_mutual_info_score(t_l,gt))
            # print(sklearn.metrics.normalized_mutual_info_score(t_var_l,gt))
            # print(sklearn.metrics.normalized_mutual_info_score(t_l,t_var_l))
            # print(sklearn.feature_selection.mutual_info_classif(t_l,t_var_l, discrete_features='auto', n_neighbors=3, copy=True, random_state=None))
            exit()
            if dataset=='snopes' or dataset=='snopes_nonpol':
                col_l = ['darkred', 'orange', 'gray', 'lime', 'green']
                # col = 'purple'
                col = 'k'
                news_cat_list_n = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']
            if dataset=='politifact':
                col_l = ['darkred', 'red', 'orange', 'gray', 'lime', 'green']
                news_cat_list_n = ['PANTS ON FIRE', 'FALSE', 'MOSTLY FALSE', 'HALF TRUE', 'MOSTLY TRUE', 'TRUE']
                # col = 'c'
                col = 'k'
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            if dataset=='mia':
                col_l = ['red', 'green']
                news_cat_list_n = ['RUMORS', 'NON RUMORS']
                # col = 'brown'
                col = 'k'
                col_t_f = ['red', 'green']
                news_cat_list_t_f = [['rumors'], ['non-rumors']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            count = 0
            # Y = [0]*len(thr_list)




            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []

            fig_cdf = True
            # fig_cdf = False
            if fig_cdf==True:

        #####################################################33

                # out_dict = w_apb_dict
                # out_dict = w_gull_dict
                # out_dict = w_cyn_dict
                # out_dict = tweet_var
                out_dict = time_dict

                tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(out_dict[t_id])
                count=0

                # num_bins = len(pt_l)
                # counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5,linestyle='--', label='All news stories')
                # mplpl.rcParams['figure.figsize'] = 5.8, 3.7
                mplpl.rcParams['figure.figsize'] = 4.5, 2.5
                mplpl.rc('xtick', labelsize='large')
                mplpl.rc('ytick', labelsize='large')
                mplpl.rc('legend', fontsize='small')
                for cat_m in news_cat_list:
                    count += 1
                    pt_l_dict[cat_m] = []
                    for t_id in tweet_l_sort:
                        if tweet_lable_dict[t_id]==cat_m:
                            # if out_dict[t_id]>0 or out_dict[t_id]<=0:
                                pt_l_dict[cat_m].append(out_dict[t_id])



                    # df_tt = pd.DataFrame(np.array(pt_l_dict[cat_m]), columns=[cat_m])
                    # df_tt[cat_m].plot(kind='kde', lw=6, color=col_l[count-1], label=cat_m)

                    num_bins = len(pt_l_dict[cat_m])
                    counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_l[count-1], lw=5, label=cat_m)

                # mplpl.ylabel('PDF of Perceived Truth Level', fontsize=20, fontweight = 'bold')
                # mplpl.ylabel('CDF of \n Absolute Perception Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('CDF of \n False Positive Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('CDF of \n False Negative Bias', fontsize=13, fontweight = 'bold')

                # mplpl.ylabel('PDF of \n Absolute Perception Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('PDF of \n False Positive Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('PDF of \n False Negative Bias', fontsize=13, fontweight='bold')

                # mplpl.xlabel('Total Perception Bias', fontsize=24, fontweight = 'bold')
                # mplpl.xlabel('False Positive Bias', fontsize=24, fontweight = 'bold')
                mplpl.xlabel('Time', fontsize=20, fontweight = 'bold')
                mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
                mplpl.grid()
                mplpl.title(data_name, fontsize='x-large')
                labels = ['0.0', '0.25', '0.5', '0.75', '1.0']
                y = [0.0, 0.5, 1, 1.5, 2]
                # mplpl.yticks(y, labels)
                legend_properties = {'weight': 'bold'}


                # plt.legend(prop=legend_properties)
                # mplpl.legend(loc="upper left",prop=legend_properties,fontsize='small', ncol=1)#, fontweight = 'bold')
                mplpl.legend(loc="lower right",prop=legend_properties,fontsize='small', ncol=1)#, fontweight = 'bold')
                # mplpl.xlim([0, 1])
                # mplpl.ylim([0, 2])
                # mplpl.ylim([0, 1])
                mplpl.subplots_adjust(bottom=0.24)
                mplpl.subplots_adjust(left=0.18)

                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_pt_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_APB_cdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_FPB_cdf'
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_time_cdf'

                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_APB_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_FPB_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_FNB_pdf'

                mplpl.savefig(pp + '.pdf', format='pdf')
                mplpl.savefig(pp + '.png', format='png')
                mplpl.figure()


                exit()

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_corr_gradient_descent":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['mia','politifact','snopes_nonpol','snopes','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']
                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            if dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpolitical'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            df_wei = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            w_cyn_dict= collections.defaultdict()
            w_gull_dict= collections.defaultdict()
            w_apb_dict= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            cat_time_dict = collections.defaultdict(dict)
            time_dict = collections.defaultdict()
            cat_time_list = collections.defaultdict(list)
            cat_time_dict_var = collections.defaultdict(dict)
            time_dict_var = collections.defaultdict()
            cat_time_list_var = collections.defaultdict(list)
            w_apb_dict_wei = collections.defaultdict()

            tweet_avg_wei= collections.defaultdict()
            # tweet_med[t_id] = np.median(val_list)
            tweet_var_wei= collections.defaultdict()

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'
                    inp1_wei = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_weighted.csv'
                inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_wei[ind] = pd.read_csv(inp1_wei, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

            max_corr = 0
            c_f = 0;v_l_f=0; p_f =0 ; c_s=0; p_t=0; v_l_t=0; c_t=0
            for conf_false_v in frange(-1, 0, 0.2):
                for v_l_false_v in frange(conf_false_v, 0, 0.2):
                    for pos_false_v in frange(v_l_false_v, 0, 0.2):
                        for cant_s_v in [0]:
                            for p_true_v in frange(0, 1.02, 0.2):
                                for v_l_true_v in frange(p_true_v,1.02, 0.2):
                                    for conf_true_v in frange(v_l_true_v, 1.02, 0.2):
                                        print(conf_false_v, v_l_false_v, pos_false_v, cant_s_v, p_true_v, v_l_true_v, conf_true_v)

                                        tweet_abs_dev_avg_l = []; tweet_avg_l=[]
                                        for ind in ind_l:
                                            df_m = df[ind].copy()
                                            df_m_wei = df_wei[ind].copy()

                                            groupby_ftr = 'tweet_id'
                                            grouped = df_m.groupby(groupby_ftr, sort=False)
                                            grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                                            for t_id in grouped.groups.keys():
                                                if t_id==1:
                                                    continue
                                                df_tmp = df_m[df_m['tweet_id'] == t_id]
                                                ind_t = df_tmp.index.tolist()[0]
                                                df_tmp_wei = df_m_wei[df_m_wei['tweet_id'] == t_id]
                                                ind_t_wei = df_tmp_wei.index.tolist()[0]
                                                weights = []
                                                for index in df_tmp.index.tolist():
                                                    # t_id
                                                    ra = df_tmp['ra'][index]
                                                    if ra == 1:
                                                        rel = conf_false_v
                                                    elif ra == 2:
                                                        rel = v_l_false_v
                                                    elif ra == 3:
                                                        rel = pos_false_v
                                                    elif ra == 4:
                                                        rel = cant_s_v
                                                    elif ra == 5:
                                                        rel = p_true_v
                                                    elif ra == 6:
                                                        rel = v_l_true_v
                                                    elif ra == 7:
                                                        rel = conf_true_v

                                                    # df[ind]['rel_v'][index] = rel
                                                    gt = df_tmp['rel_gt_v'][index]
                                                    pt = rel
                                                    df_tmp['susc'][index] = np.abs(pt-gt)

                                                tweet_avg_l.append(np.var(df_tmp['rel_v']))
                                                tweet_abs_dev_avg_l.append(np.mean(df_tmp['susc']))


                                        cur_corr = scipy.stats.spearmanr(tweet_avg_l,tweet_abs_dev_avg_l )[0]
                                        print(cur_corr)
                                        if cur_corr>max_corr:
                                            max_corr = cur_corr
                                            c_f = conf_false_v
                                            v_l_f = v_l_false_v
                                            p_f = pos_false_v
                                            c_s = cant_s_v
                                            p_t = p_true_v
                                            v_l_t = v_l_true_v
                                            c_t = conf_true_v
            print(max_corr)
            print(c_f)
            print(v_l_f)
            print(p_f)
            print(c_s)
            print(p_t)
            print(v_l_t)
            print(c_t)







            # print(np.corrcoef(w_apb_wei_list,w_apb_list )[0][1])
            # print(np.corrcoef(w_apb_wei_list,var_wei_list )[0][1])
            # print(np.corrcoef(var_list,w_apb_list )[0][1])

            # print(scipy.stats.spearmanr(w_apb_wei_list,w_apb_list ))
            # print(scipy.stats.spearmanr(w_apb_wei_list,var_wei_list ))


            exit()

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_true-false_accuracy":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        # dataset = 'snopes'
        # dataset = 'mia'
        dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','snopes_nonpol','snopes','mia','mia','politifact']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            if dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpol'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()
            tweet_var_diff_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            tweet_chi_group = {}
            tweet_chi_group_1 = {}
            tweet_chi_group_2 = {}
            tweet_skew = {}
            news_cat_list_tf = [4,2,3,1]
            t_f_dict_len = collections.defaultdict(int)
            t_f_dict = {}
            if dataset=='snopes' or dataset=='snopes_nonpol':
                news_cat_list_t_f = ['FALSE', 'MOSTLY FALSE','MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_v = [-1, -.5, 0, 0.5, 1]

            if dataset=='politifact':
                news_cat_list_t_f = ['pants-fire', 'false', 'mostly-false','half-true','mostly-true', 'true']
                news_cat_list_v = [-1,-1, -0.5, 0, 0.5, 1]

            if dataset=='mia':
                news_cat_list_t_f = ['rumor', 'non-rumor']
                news_cat_list_v = [-1, 1]

            w_fnb_dict= collections.defaultdict()
            w_fpb_dict= collections.defaultdict()
            w_apb_dict = collections.defaultdict()
            gt_acc = collections.defaultdict()
            for cat in news_cat_list_v:
                gt_acc[cat] = [0]*(len(news_cat_list_t_f))
            weight_list = [-1, -0.66, -0.33, 0, 0.33, 0.66, 1]
            # weight_list = [-2.5, -0.84, 0.25, 1, -1.127, 1.1, 1.05]
            # weight_list = [-2.36, -0.73, 0.53, 0.87, -0.87, 0.93, 1.53]
            pt_list = []
            gt_list = []
            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()



                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                tmp_list = df_m[df_m['ra']==1].index.tolist()
                df_m['rel_v'][tmp_list]=[weight_list[0]]*len(tmp_list)
                tmp_list = df_m[df_m['ra']==2].index.tolist()
                df_m['rel_v'][tmp_list]=[weight_list[1]]*len(tmp_list)
                tmp_list = df_m[df_m['ra'] == 3].index.tolist()
                df_m['rel_v'][tmp_list] = [weight_list[2]] * len(tmp_list)
                tmp_list = df_m[df_m['ra']==4].index.tolist()
                df_m['rel_v'][tmp_list]=[weight_list[3]]*len(tmp_list)
                tmp_list = df_m[df_m['ra'] == 5].index.tolist()
                df_m['rel_v'][tmp_list] = [weight_list[4]] * len(tmp_list)
                tmp_list = df_m[df_m['ra']==6].index.tolist()
                df_m['rel_v'][tmp_list]=[weight_list[5]]*len(tmp_list)
                tmp_list = df_m[df_m['ra'] == 7].index.tolist()
                df_m['rel_v'][tmp_list] = [weight_list[6]] * len(tmp_list)

                for t_id in grouped.groups.keys():

                    df_tmp = df_m[df_m['tweet_id']==t_id]
                    pt = np.mean(df_tmp['rel_v'])
                    gt = list(df_tmp['rel_gt_v'])[0]
                    min_dist = 10
                    for i_cat in range(len(news_cat_list_t_f)):
                        curr_dist = np.abs(pt - news_cat_list_v[i_cat])
                        if curr_dist < min_dist:
                            min_dist = curr_dist
                            cat = i_cat

                    gt_acc[gt][cat]+=1
                    gt_list.append(gt)
                    pt_list.append(news_cat_list_v[cat])
            ##################################################
            print(np.corrcoef(pt_list, gt_list))
            print(scipy.stats.spearmanr(pt_list, gt_list))
            width = 0.05
            pr = -10
            title_l = news_cat_list
            outp = {}
            outp_var = {}
            # news_cat_list = ['pants-fire', 'false', 'mostly_false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            if dataset == 'snopes' or dataset == 'snopes_nonpol':
                # col_l = ['b', 'g', 'c', 'y', 'r']
                col_l = ['red', 'orange', 'gray', 'lime', 'green']

                news_cat_list_n = ['FALSE', 'MOSTLY\nFALSE', 'MIXTURE', 'MOSTLY\nTRUE', 'TRUE']
            if dataset == 'politifact':
                # col_l = ['grey','b', 'g', 'c', 'y', 'r']
                col_l = ['darkred', 'red', 'orange', 'gray', 'lime', 'green']

                news_cat_list_n = ['PANTS ON\nFIRE', 'FALSE', 'MOSTLY\nFALSE', 'HALF\nTRUE', 'MOSTLY\nTRUE', 'TRUE']

            if dataset == 'mia':
                # col_l = ['b', 'r']
                col_l = ['red', 'green']
                news_cat_list_n = ['RUMORS', 'NON RUMORS']
            count = 0
            Y = [0] * len(news_cat_list_n)
            # Y1 = [0] * len(thr_list)
            mplpl.rcParams['figure.figsize'] = 6.8, 5
            mplpl.rc('xtick', labelsize='large')
            mplpl.rc('ytick', labelsize='large')
            mplpl.rc('legend', fontsize='small')
            cat_num_st = 0
            for cat_v_ind in range(len(news_cat_list_v)):
                cat_v = news_cat_list_v[cat_v_ind]
                for i in range(len(gt_acc[cat_v])):
                    cat_num_st += gt_acc[cat_v][i]
                break
            acc_dict = {}
            for cat_v_ind in range(len(news_cat_list_v)):
                cat_v = news_cat_list_v[cat_v_ind]
                acc_dict[cat_v] = gt_acc[cat_v][cat_v_ind] / float(cat_num_st)

                acc = np.mean(acc_dict.values())
            for cat_v in news_cat_list_v:
                count += 1
                count += 1
                outp[cat_v] = []
                outp[cat_v] = gt_acc[cat_v]
                # for i in range(len(gt_acc[cat_v])):
                #     outp[i].append(gt_acc[cat_v][i])
                if dataset=='snopes' or dataset=='snopes_nonpol':
                    mplpl.bar([0.09, 0.18, 0.28, 0.38,0.48], outp[cat_v], width, bottom=np.array(Y), color=col_l[count - 1],
                              label=news_cat_list_n[count - 1])
                elif dataset=='politifact':
                    mplpl.bar([0.09, 0.18, 0.28, 0.38,0.48], outp[cat_v], width, bottom=np.array(Y), color=col_l[count - 1],
                              label=news_cat_list_n[count - 1])
                Y = np.array(Y) + np.array(outp[cat_v])



            mplpl.xlim([0.08, 0.58])
            # mplpl.ylim([0, 1.38])
            mplpl.ylabel('Composition of labeled news stories', fontsize=14, fontweight='bold')
            # mplpl.xlabel('Top k news stories reported by negative PTL', fontsize=13.8,fontweight = 'bold')
            mplpl.xlabel('Users\' perception', fontsize=14,
                         fontweight='bold')
            # mplpl.xlabel('Top k news stories ranked by NAPB', fontsize=18)

            mplpl.legend(loc="upper right", ncol=2, fontsize='small')

            mplpl.subplots_adjust(bottom=0.2)

            mplpl.subplots_adjust(left=0.18)
            mplpl.grid()
            mplpl.title(data_name, fontsize='x-large')
            labels = news_cat_list_n
            x = [0.1, 0.2, 0.3, 0.4, 0.5]
            mplpl.xticks(x, labels)
            # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_vote_composition_gt'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_vote_composition_gt'
            pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_napb_composition_gt'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_gt_accuracy_ordinalreg_weight'
            mplpl.savefig(pp + '.pdf', format='pdf')
            mplpl.savefig(pp + '.png', format='png')

            exit()
    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_creating_features_prediction_old":

        one_d_array = [1, 2, 3]
        list_tmp=[]
        list_tmp.append([one_d_array])
        list_tmp.append([one_d_array])

        pd.DataFrame([
            [one_d_array],
            [one_d_array]])


        # array([[0., 1., 1., 0., 0., 1., 0., 0., 0.]])

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            # remotedir = ''
            inp_all = glob.glob(remotedir + 'all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','politifact','snopes_nonpol','snopes','mia','mia','politifact']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                # remotedir = ''
                inp_all = glob.glob(remotedir + 'all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
                num_claims = 150

            if dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpol'
                num_claims = 100
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
                num_claims = 180

            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'
                num_claims = 60

            columns_l = []
            columns_l.append('label')
            columns_l.append('tweet_id')
            # for i in range(0,100):
            for i in range(1, 8):
                    columns_l.append(str('pt_count_' + str(i)))
            # for i in range(0, 100):
            #         columns_l.append(str('worker_pt_' + str(i)))

            # for i in range(0, 100):
            #         columns_l.append(str('worker_leaning_' + str(i)))

            df_feat = pd.DataFrame(index=range(num_claims), columns=columns_l)
            df_feat = df_feat.fillna(0)
            df_feat.loc[:, 'tpb'] = df_feat['tweet_id'] * 0.0
            df_feat.loc[:, 'text'] = df_feat['tweet_id'] * 0.0
            enc = preprocessing.OneHotEncoder(n_values=[3, 7])
            # lb = preprocessing.LabelBinarizer()
            # lb.fit(['dem', 'rep', 'neut', '1','2','3','4','5','6','7'])
            enc.fit([1, 1])
            test = np.array(enc.transform([[0, 0]]).toarray())
            # test = np.array(lb.transform(['dem', '1']))
            test_list = []
            # for j in range(0,100):
            #     test_list = []
            #     for i in range(len(df_feat)):
            #         test_list.append([list(test[0])])
            #     df_tmp_f = pd.DataFrame(test_list, columns=[str('worker_leaning_' + str(j))])
            #     df_feat = pd.concat([df_feat, df_tmp_f], axis=1, join='inner')
            #

                # df_feat.loc[:, str('worker_leaning_' + str(i))] = df_feat['tweet_id'] * [0]
                # columns_l.append(str('worker_leaning_' + str(i)))

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                # inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp1_final.csv'
                df_in = pd.read_csv(inp1, sep="\t")


                demographic_feat = 'leaning'
                for lean_f in set(df_in[demographic_feat]):
                    for judge in  set(df_in['ra']):
                        print (str(lean_f) + ' : ' + str(judge))
                        if demographic_feat + '_'+ str(lean_f) + ',' +'pt_' + str(judge) not in df_feat.columns:
                            df_feat.loc[:, demographic_feat + '_'+ str(lean_f) + '_' +'pt_' + str(judge)] = df_feat['tweet_id'] * 0.0

                # converting_demographic_num
                demographic_feat_list = ['nationality','residence','gender', 'age','degree', 'employment','income','political_view', 'race', 'marital_status']
                for demographic_feat in demographic_feat_list:
                    for lean_f in set(df_in[demographic_feat]):
                        for judge in  set(df_in['ra']):
                            # print (str(lean_f) + ' : ' + str(judge))
                            if demographic_feat + '_' + str(lean_f) + ',' + 'pt_' + str(judge) not in df_feat.columns:
                                df_feat.loc[:, demographic_feat + '_'+ str(lean_f) + '_' +'pt_' + str(judge)] = df_feat['tweet_id'] * 0.0


            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()
            tweet_var_diff_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            tweet_chi_group = {}
            tweet_chi_group_1 = {}
            tweet_chi_group_2 = {}
            tweet_skew = {}
            rel_gt_dict = {}
            news_cat_list_tf = [4,2,3,1]
            t_f_dict_len = collections.defaultdict(int)
            t_f_dict = {}
            if dataset=='snopes' or dataset=='snopes_nonpol':
                news_cat_list_t_f = ['FALSE', 'MOSTLY FALSE','MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_v = [-1, -.5, 0, 0.5, 1]
                rel_gt_dict['FALSE'] = -2
                rel_gt_dict['MOSTLY FALSE'] = -1
                rel_gt_dict['MIXTURE'] = 0
                rel_gt_dict['MOSTLY TRUE'] = 1
                rel_gt_dict['TRUE'] = 2
            if dataset=='politifact':
                news_cat_list_t_f = ['pants-fire', 'false', 'mostly-false','half-true','mostly-true', 'true']
                news_cat_list_v = [-1,-1, -0.5, 0, 0.5, 1]
                rel_gt_l = [-2,-2, -1, 0, 1, 2]
                rel_gt_dict['pants-fire'] = -3
                rel_gt_dict['false'] = -2
                rel_gt_dict['mostly-false'] = -1
                rel_gt_dict['half-true'] = 0
                rel_gt_dict['mostly-true'] = 1
                rel_gt_dict['true'] = 2


            if dataset=='mia':
                news_cat_list_t_f = ['rumor', 'non-rumor']
                news_cat_list_v = [-1, 1]
                rel_gt_l = [-1,1]
                rel_gt_dict['rumor'] = -1
                rel_gt_dict['non-rumor'] = 1

            w_fnb_dict= collections.defaultdict()
            w_fpb_dict= collections.defaultdict()
            w_apb_dict = collections.defaultdict()
            gt_acc = collections.defaultdict()





            for cat in news_cat_list_v:
                gt_acc[cat] = [0]*(len(news_cat_list_t_f))
            t_id_ind = 0

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                # df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                ccc=0
                for t_id in grouped.groups.keys():
                    if t_id==1:
                        continue

                    ccc+=1
                    print(ccc)
                    df_tmp = df_m[df_m['tweet_id']==t_id]
                    df_tmp_ind = df_tmp[df_tmp['tweet_id']==t_id].index.tolist()[0]

                    groupby_ftr = 'ra'
                    grouped_ra = df_tmp.groupby(groupby_ftr, sort=False)
                    grouped_count = df_tmp.groupby(groupby_ftr, sort=False).count()
                    # ra_list = grouped_groups.keys()
                    # print(grouped_count.index.tolist())
                    for i in range(1,8):
                        try:
                            df_feat['pt_count_' + str(i)][t_id_ind] = grouped_count['age'][i]
                        except:
                            df_feat['pt_count_' + str(i)][t_id_ind] = 0
                            continue
                    df_tmp_sort = df_tmp.sort(['worker_id'])

                    demographic_feat = 'leaning'
                    for lean_f in set(df_tmp[demographic_feat]):
                        df_lean_f = df_tmp[df_tmp[demographic_feat]==lean_f]
                        if len(df_lean_f)==0:
                            continue
                        groupby_ftr = 'ra'
                        grouped_ra = df_lean_f.groupby(groupby_ftr, sort=False)
                        grouped_count = df_lean_f.groupby(groupby_ftr, sort=False).count()
                        for i in range(1,8):
                            try:
                                df_feat[demographic_feat + '_'+ str(lean_f) + '_' +'pt_' + str(i)][t_id_ind] = grouped_count['age'][i]/float(len(df_lean_f))
                            except:
                                df_feat[demographic_feat + '_'+ str(lean_f) + '_' +'pt_' + str(i)][t_id_ind] = 0
                                continue

                    for demographic_feat in demographic_feat_list:
                        print(demographic_feat)
                        for lean_f in set(df_tmp[demographic_feat]):
                            df_lean_f = df_tmp[df_tmp[demographic_feat] == lean_f]
                            if len(df_lean_f) == 0:
                                continue
                            groupby_ftr = 'ra'
                            grouped_ra = df_lean_f.groupby(groupby_ftr, sort=False)
                            grouped_count = df_lean_f.groupby(groupby_ftr, sort=False).count()
                            for i in range(1, 8):
                                try:
                                    df_feat[demographic_feat + '_' + str(lean_f) + '_' + 'pt_' + str(i)][
                                        t_id_ind] = grouped_count['age'][i] / float(len(df_lean_f))
                                except:
                                    df_feat[demographic_feat + '_' + str(lean_f) + '_' + 'pt_' + str(i)][t_id_ind] = 0
                                    continue


                                            # for judge in set(df_tmp['ra']):
                        #     # print (str(lean_f) + ' : ' + str(judge))
                        #     df_feat.loc[:, str(lean_f) + '_' + str(judge)] = df_feat['tweet_id'] * 0.0



                    df_feat['tweet_id'][t_id_ind] = t_id
                    df_feat['tpb'][t_id_ind] = df_tmp['susc'][df_tmp_ind]
                    df_feat['text'][t_id_ind] = df_tmp['text'][df_tmp_ind]
                    df_feat['label'][t_id_ind] = rel_gt_dict[df_tmp['ra_gt'][df_tmp_ind]]
                    w_c = 0
                    # for index in df_tmp_sort.index.tolist():
                    #     ra = df_tmp_sort['ra'][index]
                    #     lean_tmp = df_tmp_sort['leaning'][index]
                    #     # if lean_tmp==-1:
                    #     #     lean='rep'
                    #     # elif lean_tmp==0:
                    #     #     lean ='netu'
                    #     # elif lean_tmp==1:
                    #     #     lean='dem'
                    #     if lean_tmp==-1:
                    #         lean=1
                    #     elif lean_tmp==0:
                    #         lean =2
                    #     elif lean_tmp==1:
                    #         lean=3
                    #
                    #     if ra==1:
                    #         rel = -3
                    #     elif ra==2:
                    #         rel=-2
                    #     elif ra==3:
                    #         rel=-1
                    #
                    #
                    #     elif ra==4:
                    #         rel=0
                    #     elif ra==5:
                    #         rel = 1
                    #     elif ra==6:
                    #         rel = 2
                    #     elif ra==7:
                    #         rel = 3
                    #     pt = rel
                    #     try:
                    #         df_feat[str('worker_pt_' + str(w_c))][t_id_ind] = pt
                    #         # df_feat[str('worker_leaning_' + str(w_c))][t_id_ind] = lean * ra
                    #         # df_feat[str('worker_leaning_' + str(w_c))][t_id_ind] = lb.transform([lean ,ra])
                    #         # df_feat[str('worker_leaning_' + str(w_c))][t_id_ind] = enc.transform([[lean-1 ,ra-1]]).toarray()[0].tolist()
                    #     except:
                    #         continue
                    #     w_c+=1
                    t_id_ind+=1
            # for i in range(len(df_feat)):


            df_feat.to_csv(remotedir + 'fake_news_features_'+data_n+'.csv',columns=df_feat.columns, sep="\t", index=False)
            exit()

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_creating_features_prediction":

        one_d_array = [1, 2, 3]
        list_tmp=[]
        list_tmp.append([one_d_array])
        list_tmp.append([one_d_array])

        pd.DataFrame([
            [one_d_array],
            [one_d_array]])


        # array([[0., 1., 1., 0., 0., 1., 0., 0., 0.]])

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            # remotedir = ''
            inp_all = glob.glob(remotedir + 'all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','politifact','snopes_nonpol','snopes','mia','mia','politifact']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                # remotedir = ''
                inp_all = glob.glob(remotedir + 'all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
                num_claims = 50

            if dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpol'
                num_claims = 100
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
                num_claims = 60

            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'
                num_claims = 60

            columns_l = []
            columns_l.append('label')
            columns_l.append('tweet_id')
            # for i in range(0,100):
            for i in range(1, 8):
                    columns_l.append(str('pt_count_' + str(i)))
            # for i in range(0, 100):
            #         columns_l.append(str('worker_pt_' + str(i)))

            # for i in range(0, 100):
            #         columns_l.append(str('worker_leaning_' + str(i)))
            df_feat = collections.defaultdict()
            for ind in ind_l:
                df_feat[ind] = pd.DataFrame(index=range(num_claims), columns=columns_l)
                df_feat[ind] = df_feat[ind].fillna(0)
                df_feat[ind].loc[:, 'tpb'] = df_feat[ind]['tweet_id'] * 0.0
                df_feat[ind].loc[:, 'text'] = df_feat[ind]['tweet_id'] * 0.0
                enc = preprocessing.OneHotEncoder(n_values=[3, 7])
                enc.fit([1, 1])
                test = np.array(enc.transform([[0, 0]]).toarray())
                # test = np.array(lb.transform(['dem', '1']))
                test_list = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                # inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp1_final.csv'
                df_in = pd.read_csv(inp1, sep="\t")


                demographic_feat = 'leaning'
                for lean_f in set(df_in[demographic_feat]):
                    for judge in  set(df_in['ra']):
                        print (str(lean_f) + ' : ' + str(judge))
                        new_feat = demographic_feat + '_'+ str(lean_f) + '_' +'pt_' + str(judge)
                        for ind in ind_l:
                            if new_feat not in df_feat[ind].columns:
                                df_feat[ind].loc[:, new_feat] = df_feat[ind]['tweet_id'] * 0.0

                # converting_demographic_num
                demographic_feat_list = ['nationality','residence','gender', 'age','degree', 'employment','income','political_view', 'race', 'marital_status']
                for demographic_feat in demographic_feat_list:
                    for lean_f in set(df_in[demographic_feat]):
                        for judge in  set(df_in['ra']):
                            # print (str(lean_f) + ' : ' + str(judge))
                            new_feat = demographic_feat + '_' + str(lean_f) + '_' + 'pt_' + str(judge)
                            for ind in ind_l:
                                if  new_feat not in df_feat[ind].columns:
                                    df_feat[ind].loc[:, new_feat] = df_feat[ind]['tweet_id'] * 0.0


            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()
            tweet_var_diff_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            tweet_chi_group = {}
            tweet_chi_group_1 = {}
            tweet_chi_group_2 = {}
            tweet_skew = {}
            rel_gt_dict = {}
            news_cat_list_tf = [4,2,3,1]
            t_f_dict_len = collections.defaultdict(int)
            t_f_dict = {}
            if dataset=='snopes' or dataset=='snopes_nonpol':
                news_cat_list_t_f = ['FALSE', 'MOSTLY FALSE','MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_v = [-1, -.5, 0, 0.5, 1]
                rel_gt_dict['FALSE'] = -2
                rel_gt_dict['MOSTLY FALSE'] = -1
                rel_gt_dict['MIXTURE'] = 0
                rel_gt_dict['MOSTLY TRUE'] = 1
                rel_gt_dict['TRUE'] = 2
            if dataset=='politifact':
                news_cat_list_t_f = ['pants-fire', 'false', 'mostly-false','half-true','mostly-true', 'true']
                news_cat_list_v = [-1,-1, -0.5, 0, 0.5, 1]
                rel_gt_l = [-2,-2, -1, 0, 1, 2]
                rel_gt_dict['pants-fire'] = -3
                rel_gt_dict['false'] = -2
                rel_gt_dict['mostly-false'] = -1
                rel_gt_dict['half-true'] = 0
                rel_gt_dict['mostly-true'] = 1
                rel_gt_dict['true'] = 2


            if dataset=='mia':
                news_cat_list_t_f = ['rumor', 'non-rumor']
                news_cat_list_v = [-1, 1]
                rel_gt_l = [-1,1]
                rel_gt_dict['rumor'] = -1
                rel_gt_dict['non-rumor'] = 1

            w_fnb_dict= collections.defaultdict()
            w_fpb_dict= collections.defaultdict()
            w_apb_dict = collections.defaultdict()
            gt_acc = collections.defaultdict()





            for cat in news_cat_list_v:
                gt_acc[cat] = [0]*(len(news_cat_list_t_f))

            for ind in ind_l:
                t_id_ind = 0

                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                # df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                ccc=0
                for t_id in grouped.groups.keys():
                    if t_id==1:
                        continue

                    ccc+=1
                    print(ccc)
                    df_tmp = df_m[df_m['tweet_id']==t_id]
                    df_tmp_ind = df_tmp[df_tmp['tweet_id']==t_id].index.tolist()[0]

                    groupby_ftr = 'ra'
                    grouped_ra = df_tmp.groupby(groupby_ftr, sort=False)
                    grouped_count = df_tmp.groupby(groupby_ftr, sort=False).count()
                    # ra_list = grouped_groups.keys()
                    # print(grouped_count.index.tolist())
                    for i in range(1,8):
                        try:
                            df_feat[ind]['pt_count_' + str(i)][t_id_ind] = grouped_count['age'][i]
                        except:
                            df_feat[ind]['pt_count_' + str(i)][t_id_ind] = 0
                            continue
                    df_tmp_sort = df_tmp.sort(['worker_id'])

                    demographic_feat = 'leaning'
                    for lean_f in set(df_tmp[demographic_feat]):
                        df_lean_f = df_tmp[df_tmp[demographic_feat]==lean_f]
                        if len(df_lean_f)==0:
                            continue
                        groupby_ftr = 'ra'
                        grouped_ra = df_lean_f.groupby(groupby_ftr, sort=False)
                        grouped_count = df_lean_f.groupby(groupby_ftr, sort=False).count()
                        for i in range(1,8):
                            try:
                                df_feat[ind][demographic_feat + '_'+ str(lean_f) + '_' +'pt_' + str(i)][t_id_ind] = grouped_count['age'][i]/float(len(df_lean_f))
                            except:
                                df_feat[ind][demographic_feat + '_'+ str(lean_f) + '_' +'pt_' + str(i)][t_id_ind] = 0
                                continue

                    for demographic_feat in demographic_feat_list:
                        print(demographic_feat)
                        for lean_f in set(df_tmp[demographic_feat]):
                            df_lean_f = df_tmp[df_tmp[demographic_feat] == lean_f]
                            if len(df_lean_f) == 0:
                                continue
                            groupby_ftr = 'ra'
                            grouped_ra = df_lean_f.groupby(groupby_ftr, sort=False)
                            grouped_count = df_lean_f.groupby(groupby_ftr, sort=False).count()
                            for i in range(1, 8):
                                try:
                                    df_feat[ind][demographic_feat + '_' + str(lean_f) + '_' + 'pt_' + str(i)][
                                        t_id_ind] = grouped_count['age'][i] / float(len(df_lean_f))
                                except:
                                    df_feat[ind][demographic_feat + '_' + str(lean_f) + '_' + 'pt_' + str(i)][t_id_ind] = 0
                                    continue


                                            # for judge in set(df_tmp['ra']):
                        #     # print (str(lean_f) + ' : ' + str(judge))
                        #     df_feat.loc[:, str(lean_f) + '_' + str(judge)] = df_feat['tweet_id'] * 0.0



                    df_feat[ind]['tweet_id'][t_id_ind] = t_id
                    df_feat[ind]['tpb'][t_id_ind] = df_tmp['susc'][df_tmp_ind]
                    df_feat[ind]['text'][t_id_ind] = df_tmp['text'][df_tmp_ind]
                    df_feat[ind]['label'][t_id_ind] = rel_gt_dict[df_tmp['ra_gt'][df_tmp_ind]]
                    w_c = 0
                    # for index in df_tmp_sort.index.tolist():
                    #     ra = df_tmp_sort['ra'][index]
                    #     lean_tmp = df_tmp_sort['leaning'][index]
                    #     # if lean_tmp==-1:
                    #     #     lean='rep'
                    #     # elif lean_tmp==0:
                    #     #     lean ='netu'
                    #     # elif lean_tmp==1:
                    #     #     lean='dem'
                    #     if lean_tmp==-1:
                    #         lean=1
                    #     elif lean_tmp==0:
                    #         lean =2
                    #     elif lean_tmp==1:
                    #         lean=3
                    #
                    #     if ra==1:
                    #         rel = -3
                    #     elif ra==2:
                    #         rel=-2
                    #     elif ra==3:
                    #         rel=-1
                    #
                    #
                    #     elif ra==4:
                    #         rel=0
                    #     elif ra==5:
                    #         rel = 1
                    #     elif ra==6:
                    #         rel = 2
                    #     elif ra==7:
                    #         rel = 3
                    #     pt = rel
                    #     try:
                    #         df_feat[str('worker_pt_' + str(w_c))][t_id_ind] = pt
                    #         # df_feat[str('worker_leaning_' + str(w_c))][t_id_ind] = lean * ra
                    #         # df_feat[str('worker_leaning_' + str(w_c))][t_id_ind] = lb.transform([lean ,ra])
                    #         # df_feat[str('worker_leaning_' + str(w_c))][t_id_ind] = enc.transform([[lean-1 ,ra-1]]).toarray()[0].tolist()
                    #     except:
                    #         continue
                    #     w_c+=1
                    t_id_ind+=1
            # for i in range(len(df_feat)):


                df_feat[ind].to_csv(remotedir + 'fake_news_features_'+data_n+'_'+str(ind)+'.csv',columns=df_feat[ind].columns, sep="\t", index=False)
            exit()


    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_corr":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','mia','politifact','snopes','mia','politifact']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()
            tweet_var_diff_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            tweet_chi_group = {}
            tweet_chi_group_1 = {}
            tweet_chi_group_2 = {}
            tweet_skew = {}
            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    val_list = list(df_tmp['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)


                    n_dem, bins, patches = Plab.hist(dem_val_list,
                                                         bins=frange(0, 1, 0.05), normed=1)
                    n_rep, bins, patches = Plab.hist(rep_val_list,
                                                         bins=frange(0, 1, 0.05), normed=1)
                    n_neut, bins, patches = Plab.hist(neut_val_list,
                                                      bins=frange(0, 1, 0.05), normed=1)

                    dem_rep_chi = chi_sqr(n_dem, n_rep)
                    dem_neut_chi = chi_sqr(n_dem, n_neut)
                    neut_rep_chi = chi_sqr(n_neut, n_rep)



                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_diff_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) + np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)

                    if ~(dem_rep_chi > 0 or dem_rep_chi <0 or dem_rep_chi ==0):
                        tweet_chi_group[t_id] = 0
                    else:
                        tweet_chi_group[t_id] = dem_rep_chi

                    if ~(dem_neut_chi > 0 or dem_neut_chi <0 or dem_neut_chi ==0):
                        tweet_chi_group_1[t_id] = 0
                    else:
                        tweet_chi_group_1[t_id] = dem_neut_chi

                    if ~(dem_neut_chi > 0 or dem_neut_chi <0 or dem_neut_chi ==0):
                        tweet_chi_group_2[t_id] = 0
                    else:
                        tweet_chi_group_2[t_id] = dem_rep_chi


                    # tweet_chi_group[t_id] = np.var([tweet_chi_group[t_id], tweet_chi_group_1[t_id], tweet_chi_group_2[t_id]])

                    # tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew[t_id] = scipy.stats.skew(dem_val_list) + scipy.stats.skew(rep_val_list)
                    # tweet_skew_l.append(tweet_skew[t_id])

                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    # vot_list_tmp = list(df_tmp['vote'])
                    vot_list_tmp = []

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    # tweet_skew[t_id] = scipy.stats.skew(val_list)
                    # tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)




            ##################################################
            # news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE','TRUE']
            # news_cat_list_f = [ 'false','mostly_false', 'mixture', 'mostly_true', 'true']

            tweet_vote_sort = sorted(tweet_vote_avg, key=tweet_vote_avg.get, reverse=False)
            thr = 10
            thr_list = []
            categ_dict = collections.defaultdict(int)
            for i in range(int(len(tweet_vote_sort)/float(thr))):
                k = (i+1)*thr
                thr_list.append(k)
                perc_rnd_l = []
                abs_perc_rnd_l = []
                disputability_l = []
                above_avg = 0
                less_avg = 0
                above_avg_rnd = 0
                less_avg_rnd = 0
                above_avg = 0
                less_avg = 0
                categ_dict[k] = collections.defaultdict(float)
                for j in range(k):
                    for cat_n in news_cat_list:
                        if tweet_lable_dict[tweet_vote_sort[j]] == cat_n:
                            categ_dict[k][cat_n]+=1/float(k)

            width = 0.02
            pr = -10
            title_l = news_cat_list
            outp = {}
            # news_cat_list = ['pants-fire', 'false', 'mostly_false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            if dataset=='snopes':
                col_l = ['b', 'g', 'c', 'y', 'r']
                news_cat_list_n = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
            if dataset=='politifact':
                col_l = ['grey','b', 'g', 'c', 'y', 'r']
                news_cat_list_n = ['PANTS ON FIRE', 'FALSE', 'MOSTLY FALSE', 'HALF TRUE', 'MOSTLY TRUE', 'TRUE']

            if dataset=='mia':
                col_l = ['b', 'r']
                news_cat_list_n = ['RUMORS', 'NON RUMORS']
            count = 0
            Y = [0]*len(thr_list)
            for cat_m in news_cat_list:
                count+=1
                outp[cat_m] = []
                for i in thr_list:
                    outp[cat_m].append(categ_dict[i][cat_m])
                mplpl.bar([xx/float(len(tweet_vote_sort)) for xx in thr_list], outp[cat_m], width, bottom= np.array(Y), color=col_l[count-1], label=news_cat_list_n[count-1])
                Y = np.array(Y) + np.array(outp[cat_m])



            # mplpl.xlim([0, 160])
            mplpl.ylim([0, 1.3])
            mplpl.ylabel('Fraction of labeled news stories', fontsize=18)
            mplpl.xlabel('K top fraction of news ranked based on NPTL', fontsize=18)

            mplpl.legend(loc="upper right", ncol=2)

            mplpl.grid()
            mplpl.title(data_name)
            pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_vote_biased'
            mplpl.savefig(pp, format='png')

            mplpl.figure()


            # exit()
            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l = []
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            thr = 5
            prec_rnd_acc = 0
            abs_prec_rnd_acc = 0
            norm_abs_perc_bias_l = []
            abs_perc_bias_l = []
            norm_abs_perc_bias_ll = []
            abs_perc_bias_ll = []
            k_l = []
            # for i in range(int(len(tweet_disp_sort)/float(thr))):
            #     k = (i+1)*thr
            #     perc_rnd_l = []
            #     abs_perc_rnd_l = []
            #     disputability_l = []
            #     above_avg = 0
            #     less_avg = 0
            #     above_avg_rnd = 0
            #     less_avg_rnd = 0
            #     above_avg = 0
            #     less_avg = 0
            #     for j in range(k):
            #
            #         perc_rnd_l.append(tweet_abs_dev_avg[tweet_disp_sort[j]])
            #         abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[tweet_disp_sort[j]])
            #
            #         if tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg_rnd.values()):
            #             above_avg_rnd+=1
            #         elif tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg_rnd.values()):
            #             less_avg_rnd+=1
            #
            #
            #         if tweet_abs_dev_avg[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg.values()):
            #             above_avg+=1
            #         elif tweet_abs_dev_avg[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg.values()):
            #             less_avg+=1
            #
            #     norm_abs_perc_bias_ll.append(np.mean(abs_perc_rnd_l))
            #     abs_perc_bias_ll.append(np.mean(perc_rnd_l))
            #
            #     # perc_rnd_acc = len(set(disputability_l).intersection(perc_rnd_l))/ float(len(perc_rnd_l))
            #     # abs_perc_rnd_acc = len(set(disputability_l).intersection(abs_perc_rnd_l))/ float(len(abs_perc_rnd_l))
            #     # print('---------- k = ' +str(k/float(len(tweet_disp_sort)))+ '-------------')
            #     # print(str(above_avg_rnd/float(above_avg_rnd+less_avg_rnd)))
            #     # print(str(above_avg/float(above_avg+less_avg)))
            #     # print(perc_rnd_acc)
            #     # print(abs_perc_rnd_acc)
            #     # print('------------------------------')
            #
            #     norm_abs_perc_bias_l.append(above_avg_rnd/float(above_avg_rnd+less_avg_rnd))
            #     abs_perc_bias_l.append(above_avg/float(above_avg+less_avg))
            #     k_l.append(np.round(k/float(len(tweet_disp_sort)), 3))
            #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_l ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_l, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their NAPB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_l ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_l, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their APB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_abs_perception'
            # # mplpl.savefig(pp, format='png')
            # #
            # # mplpl.figure()
            # #
            # #
            # #
            # #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_ll ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_ll, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg NAPB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_ll ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_ll, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg APB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-abs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.show()
            #
            #
            #
            #     ##################################################
            #
            # print(np.mean(tweet_abs_dev_avg_rnd.values()))
            # exit()


            # tweet_avg_group = collections.defaultdict()
            # tweet_med_group = collections.defaultdict()
            # tweet_var_group = collections.defaultdict()
            #
            # tweet_kldiv_group= collections.defaultdict()

            tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l=[]
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            popularity_l= []
            vote_l = []
            gr_disp_l = []
            for t_id in tweet_l_sort:
                gt_l.append(tweet_gt_var[t_id])
                pt_l.append(tweet_avg[t_id])
                disputability_l.append(tweet_var[t_id])
                perc_l.append(tweet_dev_avg[t_id])
                abs_perc_l.append(tweet_abs_dev_avg[t_id])
                # popularity_l.append(tweet_popularity[t_id])
                vote_l.append(tweet_vote_avg[t_id])
                # vote_l.append(tweet_vote_var[t_id])
                perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
                abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
                tweet_skew_ll.append(tweet_skew[t_id])
                gr_disp_l.append(tweet_skew[t_id])
            value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l, vote_l,gr_disp_l]#,popularity_l,tweet_skew_ll]
            value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
                          'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'vote', 'group_disputability']#,'popularity' 'skewness']

            outF.write('|| ')
            for v_name in value_name:
                outF.write('||' + v_name)
            outF.write('||\n')

            for f_list in range(9):
                outF.write('|| ' + value_name[f_list] + '||')
                for s_list in range(9):
                    m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
                    outF.write(str(m_corr) + '||')
                outF.write('\n')


            exit()
            tweet_group_gt = collections.defaultdict(list)
            gt_set = sorted(set(gt_l))
            for gt_e in gt_set:
                for t_id in tweet_l_sort:
                    if tweet_gt_var[t_id]==gt_e:
                        tweet_group_gt[gt_e].append(t_id)

            pt_mean = []
            disp_mean = []
            perc_mean = []
            abs_perc_mean = []
            for gt_e in gt_set:
                t_id_l = tweet_group_gt[gt_e]
                pt_mean.append(np.mean([tweet_avg[x] for x in t_id_l]))
                disp_mean.append(np.mean([tweet_var[x] for x in t_id_l]))
                perc_mean.append(np.mean([tweet_dev_avg[x] for x in t_id_l]))
                abs_perc_mean.append(np.mean([tweet_abs_dev_avg[x] for x in t_id_l]))

            # outF = open(remotedir + 'table_out.txt', 'w')

            # outF.write('== ' + data_name + ' ==\n')
            font = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 16,
                    }

            font_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 16,
                    }


            font_t = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 12,
                    }

            font_t_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 12,
                    }



            # fig_f = True
            fig_f = False
            if fig_f==True:

                mplpl.scatter(gt_l, pt_l,  s=40,color='r',marker='o', label='All users')
                mplpl.scatter(gt_set, pt_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]

                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_avg[x]>0:
                            num_pos+=1
                        elif tweet_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e , 0.85, str(num_pos) , fontdict=font)
                    mplpl.text(gt_e , 0.75, str(num_neg) , fontdict=font_1)

                mplpl.text(-0.3, -0.75, ' # news has positive perception truth value', fontdict=font_t)
                mplpl.text(-0.3, -0.9,   '# news has negative perception truth value', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_pt| alt text| width = 500px}}')


                mplpl.scatter(gt_l,disputability_l ,  s=40,color='g',marker='o', label='All users')
                mplpl.scatter(gt_set, disp_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, disp_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                # for gt_e in gt_set:
                #     t_id_l = tweet_group_gt[gt_e]

                #     num_pos=0
                #     num_neg = 0
                #
                #     for x in t_id_l:
                #         if tweet_var[x]>0:
                #             num_pos+=1
                #         elif tweet_var[x]<0:
                #             num_neg+=1
                #     # mplpl.text(gt_e-0.2, 0.85, str(num_pos) + r'$ > 0$', fontdict=font)
                #     # mplpl.text(gt_e-0.2, 0.75, str(num_neg) + r'$ < 0$', fontdict=font_1)

                #
                # mplpl.text(0, 0.9, ' # news perceived true(postitive)', fontdict=font_t)
                # mplpl.text(0, 0.75,   '# news perceived false(negative)', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_disputability'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_disputability| alt text| width = 500px}}')


                gt_l_dict[data_name] = gt_l
                perc_l_dict[data_name] = perc_l
                gt_set_dict[data_name] = gt_set
                perc_mean_dict[data_name] = perc_mean

                mplpl.scatter(gt_l, perc_l,  s=40,color='b',marker='o', label='All users')
                mplpl.scatter(gt_set, perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, perc_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]
                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_dev_avg[x]>0:
                            num_pos+=1
                        elif tweet_dev_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e, 1.65, str(num_pos), fontdict=font)
                    mplpl.text(gt_e, 1.3, str(num_neg), fontdict=font_1)

                mplpl.text(-1, -1, '# news that has positive perception bias value', fontdict=font_t)
                mplpl.text(-1, -1.5, '# news that has nigative perception bias value', fontdict=font_t_1)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_perception| alt text| width = 500px}}')


                mplpl.scatter(gt_l,abs_perc_l ,  s=40,color='c',marker='o', label='All users')
                mplpl.scatter(gt_set, abs_perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, abs_perc_mean, color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)


                abs_perc_l_dict[data_name] = abs_perc_l
                abs_perc_mean_dict[data_name] = abs_perc_mean


                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_abs_perception| alt text| width = 500px}} ||\n')

                # outF.write('|| Table ||\n\n')

            # exit
        ########################

                tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)
                pt_l = []
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_avg[t_id])

                mplpl.scatter(range(len(pt_l)), pt_l,  s=40,color='r',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PTL', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_pt| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=False)
                disputability_l = []
                for t_id in tweet_l_sort:
                    disputability_l.append(tweet_var[t_id])


                mplpl.scatter(range(len(pt_l)), disputability_l ,  s=40,color='g',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ranked news stories according Disputability', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_disput'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_disput| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=False)
                perc_l = []
                for t_id in tweet_l_sort:
                    perc_l.append(tweet_dev_avg[t_id])

                mplpl.scatter(range(len(pt_l)), perc_l,  s=40,color='b',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias (PB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_percep| alt text| width = 500px}}')



                tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=False)
                perc_abs_l = []
                for t_id in tweet_l_sort:
                    perc_abs_l.append(tweet_abs_dev_avg[t_id])

                mplpl.scatter(range(len(perc_abs_l)), perc_abs_l,  s=40,color='c',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according APB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_abs_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_abs-percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_abs-percep| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



        #####################################################33


                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='r', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')


                num_bins = len(disputability_l)
                counts, bin_edges = np.histogram(disputability_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Disputability', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_cdf| alt text| width = 500px}}')


                num_bins = len(perc_l)
                counts, bin_edges = np.histogram(perc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='b', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception bias (PB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_cdf| alt text| width = 500px}}')

                num_bins = len(perc_abs_l)
                counts, bin_edges = np.histogram(perc_abs_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_cdf| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



                # mplpl.show()

                col_l = ['r', 'b', 'g']

                i = 0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=400, color=col_l[i], marker='*')
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i = 0
                mark_l = ['*', 'o', '^']
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i=0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o')
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=300, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                i=0
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()











                mplpl.show()

            else:

                AVG_list = []
                print(np.mean(all_acc))
                outF = open(remotedir + 'output.txt', 'w')

                tweet_all_var = {}
                tweet_all_dev_avg = {}
                tweet_all_avg = {}
                tweet_all_gt_var = {}
                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []
                tweet_all_avg_l = []
                tweet_all_med_l = []
                tweet_all_var_l = []
                tweet_all_gt_var_l = []
                diff_group_disp_l = []
                dem_disp_l = []
                rep_disp_l = []

                tweet_all_dev_avg = {}
                tweet_all_dev_med = {}
                tweet_all_dev_var = {}

                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []

                tweet_all_abs_dev_avg = {}
                tweet_all_abs_dev_med = {}
                tweet_all_abs_dev_var = {}

                tweet_all_abs_dev_avg_l = []
                tweet_all_abs_dev_med_l = []
                tweet_all_abs_dev_var_l = []
                tweet_all_dev_avg_rnd = {}
                tweet_all_abs_dev_avg_rnd = {}

                diff_group_disp_dict = {}
                if dataset == 'snopes':
                    data_n = 'sp'
                    news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                    ind_l = [1, 2, 3]
                elif dataset == 'politifact':
                    data_n = 'pf'
                    news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                    ind_l = [1, 2, 3]
                elif dataset == 'mia':
                    data_n = 'mia'
                    news_cat_list = ['rumor', 'non-rumor']
                    ind_l = [1]

                for cat_l in news_cat_list:
                    outF.write('== ' + str(cat_l) + ' ==\n\n')
                    print('== ' + str(cat_l) + ' ==')
                    tweet_dev_avg = {}
                    tweet_dev_med = {}
                    tweet_dev_var = {}
                    tweet_abs_dev_avg = {}
                    tweet_abs_dev_med = {}
                    tweet_abs_dev_var = {}

                    tweet_avg = {}
                    tweet_med = {}
                    tweet_var = {}
                    tweet_gt_var = {}

                    tweet_dev_avg_rnd = {}
                    tweet_abs_dev_avg_rnd = {}


                    tweet_dev_avg_l = []
                    tweet_dev_med_l = []
                    tweet_dev_var_l = []
                    tweet_abs_dev_avg_l = []
                    tweet_abs_dev_med_l = []
                    tweet_abs_dev_var_l = []

                    tweet_avg_l = []
                    tweet_med_l = []
                    tweet_var_l = []
                    tweet_gt_var_l = []
                    AVG_susc_list = []
                    AVG_wl_list = []
                    all_acc = []
                    AVG_dev_list = []
                    # for lean in [-1, 0, 1]:

                        # AVG_susc_list = []
                        # AVG_wl_list = []
                        # all_acc = []
                        # df_m = df_m[df_m['leaning'] == lean]
                        # if lean == 0:
                        #     col = 'g'
                        #     lean_cat = 'neutral'
                        # elif lean == 1:
                        #     col = 'b'
                        #     lean_cat = 'democrat'
                        # elif lean == -1:
                        #     col = 'r'
                        #     lean_cat = 'republican'
                        # print(lean_cat)
                    for ind in ind_l:

                        if balance_f == 'balanced':
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
                        else:
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'

                        inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
                        df[ind] = pd.read_csv(inp1, sep="\t")
                        df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                        df_m = df[ind].copy()
                        df_mm = df_m.copy()

                        df_m = df_m[df_m['ra_gt'] == cat_l]
                        # df_mm = df_m[df_m['ra_gt']==cat_l]
                        # df_m = df_m[df_m['leaning'] == lean]

                        groupby_ftr = 'tweet_id'
                        grouped = df_m.groupby(groupby_ftr, sort=False)
                        grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                        for t_id in grouped.groups.keys():
                            df_tmp = df_m[df_m['tweet_id'] == t_id]

                            df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
                            df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
                            df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []
                            df_tmp = df_m[df_m['tweet_id'] == t_id]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []

                            weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                            val_list = list(df_tmp['rel_v'])
                            tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_avg[t_id] = np.mean(val_list)
                            tweet_med[t_id] = np.median(val_list)
                            tweet_var[t_id] = np.var(val_list)
                            tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_avg_l.append(np.mean(val_list))
                            tweet_med_l.append(np.median(val_list))
                            tweet_var_l.append(np.var(val_list))
                            tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])




                            tweet_all_avg[t_id] = np.mean(val_list)
                            tweet_all_var[t_id] = np.var(val_list)
                            tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_all_avg_l.append(np.mean(val_list))
                            tweet_all_med_l.append(np.median(val_list))
                            tweet_all_var_l.append(np.var(val_list))
                            tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])



                            val_list = list(df_tmp['err'])
                            abs_var_err = [np.abs(x) for x in val_list]
                            tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_dev_avg[t_id] = np.mean(val_list)
                            tweet_dev_med[t_id] = np.median(val_list)
                            tweet_dev_var[t_id] = np.var(val_list)

                            tweet_dev_avg_l.append(np.mean(val_list))
                            tweet_dev_med_l.append(np.median(val_list))
                            tweet_dev_var_l.append(np.var(val_list))

                            tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_abs_dev_var_l.append(np.var(abs_var_err))


                            tweet_all_dev_avg[t_id] = np.mean(val_list)
                            tweet_all_dev_med[t_id] = np.median(val_list)
                            tweet_all_dev_var[t_id] = np.var(val_list)

                            tweet_all_dev_avg_l.append(np.mean(val_list))
                            tweet_all_dev_med_l.append(np.median(val_list))
                            tweet_all_dev_var_l.append(np.var(val_list))

                            tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_all_abs_dev_var_l.append(np.var(abs_var_err))



                            sum_rnd_abs_perc = 0
                            sum_rnd_perc = 0
                            for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                                sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                                sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                            random_perc = np.abs(sum_rnd_perc / float(7))
                            random_abs_perc = sum_rnd_abs_perc / float(7)

                            tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                            tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                    gt_l = []
                    pt_l = []
                    disputability_l = []
                    perc_l = []
                    abs_perc_l = []
                    # for t_id in tweet_l_sort:
                    #     gt_l.append(tweet_gt_var[t_id])
                    #     pt_l.append(tweet_avg[t_id])
                    #     disputability_l.append(tweet_var[t_id])
                    #     perc_l.append(tweet_dev_avg[t_id])
                    #     abs_perc_l.append(tweet_abs_dev_avg[t_id])



                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
                    tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)

                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)


                    if dataset == 'snopes':
                        data_addr = 'snopes'
                    elif dataset == 'politifact':
                        data_addr = 'politifact/fig'
                    elif dataset == 'mia':
                        data_addr = 'mia/fig'

                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')

                    for t_id in tweet_l_sort:
                        count+=1
                        if balance_f=='balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
                                       + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
                                       # str(np.round(diff_group_disp_dict[t_id], 3)) +
                                       '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
                                        + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
                                        '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')




                if dataset == 'snopes':
                    data_addr = 'snopes'
                elif dataset == 'politifact':
                    data_addr = 'politifact/fig'
                elif dataset == 'mia':
                    data_addr = 'mia/fig'

                # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
                tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)

                # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)

                tweet_napb_dict_high_disp = {}
                tweet_napb_dict_low_disp = {}
                for t_id in tweet_l_sort[:20]:
                    # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                for t_id in tweet_l_sort[-20:]:
                    # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                kk = 0

                for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
                    if kk==0:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
                    else:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)

                    kk+=1
                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    for t_id in tweet_l_sort:
                        count += 1
                        # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
                        if balance_f == 'balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
                                       str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +
                            #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
                            #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                                       str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                        # +
                        # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                        # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_labeld_news_ktop_nptl":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        # dataset = 'snopes'
        # dataset = 'mia'
        dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['mia','mia','politifact','snopes','mia','politifact']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()
            tweet_var_diff_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            tweet_chi_group = {}
            tweet_chi_group_1 = {}
            tweet_chi_group_2 = {}
            tweet_skew = {}
            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    val_list = list(df_tmp['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)


                    # n_dem, bins, patches = Plab.hist(dem_val_list,
                    #                                      bins=frange(0, 1, 0.05), normed=1)
                    # n_rep, bins, patches = Plab.hist(rep_val_list,
                    #                                      bins=frange(0, 1, 0.05), normed=1)
                    # n_neut, bins, patches = Plab.hist(neut_val_list,
                    #                                   bins=frange(0, 1, 0.05), normed=1)
                    #
                    # dem_rep_chi = chi_sqr(n_dem, n_rep)
                    # dem_neut_chi = chi_sqr(n_dem, n_neut)
                    # neut_rep_chi = chi_sqr(n_neut, n_rep)



                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_diff_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) + np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)

                    # if ~(dem_rep_chi > 0 or dem_rep_chi <0 or dem_rep_chi ==0):
                    #     tweet_chi_group[t_id] = 0
                    # else:
                    #     tweet_chi_group[t_id] = dem_rep_chi
                    #
                    # if ~(dem_neut_chi > 0 or dem_neut_chi <0 or dem_neut_chi ==0):
                    #     tweet_chi_group_1[t_id] = 0
                    # else:
                    #     tweet_chi_group_1[t_id] = dem_neut_chi
                    #
                    # if ~(dem_neut_chi > 0 or dem_neut_chi <0 or dem_neut_chi ==0):
                    #     tweet_chi_group_2[t_id] = 0
                    # else:
                    #     tweet_chi_group_2[t_id] = dem_rep_chi


                    # tweet_chi_group[t_id] = np.var([tweet_chi_group[t_id], tweet_chi_group_1[t_id], tweet_chi_group_2[t_id]])

                    # tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew[t_id] = scipy.stats.skew(dem_val_list) + scipy.stats.skew(rep_val_list)
                    # tweet_skew_l.append(tweet_skew[t_id])

                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])
                    # vot_list_tmp = []

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    # tweet_skew[t_id] = scipy.stats.skew(val_list)
                    # tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)




            ##################################################
            # news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE','TRUE']
            # news_cat_list_f = [ 'false','mostly_false', 'mixture', 'mostly_true', 'true']
            len_cat_dict = {}
            if dataset=='snopes' or dataset=='politifact':
                for cat in news_cat_list:
                    len_cat_dict[cat]=30
            elif dataset=='mia':
                for cat in news_cat_list:
                    if cat=='rumor':
                        len_cat_dict[cat]=30
                    else:
                        len_cat_dict[cat] = 30
            # tweet_vote_sort = sorted(tweet_vote_avg, key=tweet_vote_avg.get, reverse=False)

            # tweet_vote_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            tweet_vote_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            thr = 20
            thr_list = []
            len_t = len(tweet_vote_sort)
            categ_dict = collections.defaultdict(int)
            k_list = [int(0.1*len_t), int(0.2*len_t), int(0.3*len_t), int(0.4*len_t), int(0.5*len_t), int(1*len_t) ]
            count = 0
            for k in k_list:
                # k = (i+1)*thr
                # k = k_list[count]
                # count+=1
                thr_list.append(k)
                perc_rnd_l = []
                abs_perc_rnd_l = []
                disputability_l = []
                above_avg = 0
                less_avg = 0
                above_avg_rnd = 0
                less_avg_rnd = 0
                above_avg = 0
                less_avg = 0
                categ_dict[k] = collections.defaultdict(float)
                for j in range(k):
                    for cat_n in news_cat_list:
                        if tweet_lable_dict[tweet_vote_sort[j]] == cat_n:
                            categ_dict[k][cat_n]+=(1/float(len_cat_dict[cat_n]))

            # if dataset=='mia':
            for j in categ_dict:
                sum = np.sum(categ_dict[j].values())
                for cat_n in categ_dict[j]:
                    categ_dict[j][cat_n] =  categ_dict[j][cat_n] / sum

            width = 0.03
            pr = -10
            title_l = news_cat_list
            outp = {}
            # news_cat_list = ['pants-fire', 'false', 'mostly_false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            if dataset=='snopes':
                # col_l = ['b', 'g', 'c', 'y', 'r']
                col_l = ['red', 'magenta', 'gray', 'lime', 'green']

                news_cat_list_n = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
            if dataset=='politifact':
                # col_l = ['grey','b', 'g', 'c', 'y', 'r']
                col_l = ['darkred', 'red', 'magenta', 'gray', 'lime', 'green']

                news_cat_list_n = ['PANTS ON FIRE', 'FALSE', 'MOSTLY FALSE', 'HALF TRUE', 'MOSTLY TRUE', 'TRUE']

            if dataset=='mia':
                # col_l = ['b', 'r']
                col_l = ['red', 'green']
                news_cat_list_n = ['RUMORS', 'NON RUMORS']
            count = 0
            Y = [0]*len(thr_list)
            mplpl.rcParams['figure.figsize'] = 5.2, 4
            mplpl.rc('xtick', labelsize='large')
            mplpl.rc('ytick', labelsize='large')
            mplpl.rc('legend', fontsize='small')

            for cat_m in news_cat_list:
                count+=1
                outp[cat_m] = []
                for i in thr_list:
                    outp[cat_m].append(categ_dict[i][cat_m])
                # mplpl.bar([xx/float(len(tweet_vote_sort)) for xx in thr_list], outp[cat_m], width, bottom= np.array(Y), color=col_l[count-1], label=news_cat_list_n[count-1])
                mplpl.bar([0.1, 0.2, 0.3, 0.4,0.5,0.6], outp[cat_m], width, bottom= np.array(Y), color=col_l[count-1], label=news_cat_list_n[count-1])
                Y = np.array(Y) + np.array(outp[cat_m])



            mplpl.xlim([0.08, 0.64])
            mplpl.ylim([0, 1.38])
            mplpl.ylabel('Composition of \n labeled news stories', fontsize=14,fontweight = 'bold')
            # mplpl.xlabel('Top k news stories reported by negative PTL', fontsize=13.8,fontweight = 'bold')
            mplpl.xlabel('Top k news stories ranked by disputability', fontsize=14,fontweight = 'bold')
            # mplpl.xlabel('Top k news stories ranked by NAPB', fontsize=18)

            mplpl.legend(loc="upper right", ncol=2, fontsize='small')

            mplpl.subplots_adjust(bottom=0.16)

            mplpl.subplots_adjust(left=0.18)
            mplpl.grid()
            mplpl.title(data_name)
            labels = ['0.1', '0.2', '0.3', '0.4', '0.5','1.0']
            x = [ 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
            mplpl.xticks(x, labels)
            # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_vote_composition_gt'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_vote_composition_gt'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_napb_composition_gt'
            pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_disp_composition_gt'
            mplpl.savefig(pp + '.pdf', format='pdf')
            mplpl.savefig(pp + '.png', format='png')

            mplpl.figure()

            # mplpl.rcParams['figure.figsize'] = 5.4, 3.2
            # mplpl.rc('xtick', labelsize='xx-large')
            # mplpl.rc('ytick', labelsize='xx-large')
            # mplpl.rc('legend', fontsize='xx-large')
            # for cat_m in news_cat_list:
            #     count += 1
            #     pt_l_dict[cat_m] = []
            #     for t_id in tweet_l_sort:
            #         if tweet_lable_dict[t_id] == cat_m:
            #             pt_l_dict[cat_m].append(tweet_dev_avg[t_id])
            #
            #     num_bins = len(pt_l_dict[cat_m])
            #     counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
            #     cdf = np.cumsum(counts)
            #     scale = 1.0 / cdf[-1]
            #     ncdf = scale * cdf
            #     mplpl.plot(bin_edges[1:], ncdf, c=col_l[count - 1], lw=5, label=cat_m)
            #
            # mplpl.ylabel('CDF', fontsize=28)
            # mplpl.xlabel('Mean pereption bias', fontsize=28)
            # mplpl.grid()
            # mplpl.title(data_name, fontsize='xx-large')
            # mplpl.legend(loc="lower right", fontsize='x-large')
            # mplpl.xlim([-1.5, 1.5])
            # mplpl.ylim([0, 1])
            # mplpl.subplots_adjust(bottom=0.14)
            #
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_mpb_cdf.pdf'
            # mplpl.savefig(pp, format='pdf')
            # mplpl.figure()

            exit()
            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l = []
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            thr = 5
            prec_rnd_acc = 0
            abs_prec_rnd_acc = 0
            norm_abs_perc_bias_l = []
            abs_perc_bias_l = []
            norm_abs_perc_bias_ll = []
            abs_perc_bias_ll = []
            k_l = []
            # for i in range(int(len(tweet_disp_sort)/float(thr))):
            #     k = (i+1)*thr
            #     perc_rnd_l = []
            #     abs_perc_rnd_l = []
            #     disputability_l = []
            #     above_avg = 0
            #     less_avg = 0
            #     above_avg_rnd = 0
            #     less_avg_rnd = 0
            #     above_avg = 0
            #     less_avg = 0
            #     for j in range(k):
            #
            #         perc_rnd_l.append(tweet_abs_dev_avg[tweet_disp_sort[j]])
            #         abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[tweet_disp_sort[j]])
            #
            #         if tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg_rnd.values()):
            #             above_avg_rnd+=1
            #         elif tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg_rnd.values()):
            #             less_avg_rnd+=1
            #
            #
            #         if tweet_abs_dev_avg[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg.values()):
            #             above_avg+=1
            #         elif tweet_abs_dev_avg[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg.values()):
            #             less_avg+=1
            #
            #     norm_abs_perc_bias_ll.append(np.mean(abs_perc_rnd_l))
            #     abs_perc_bias_ll.append(np.mean(perc_rnd_l))
            #
            #     # perc_rnd_acc = len(set(disputability_l).intersection(perc_rnd_l))/ float(len(perc_rnd_l))
            #     # abs_perc_rnd_acc = len(set(disputability_l).intersection(abs_perc_rnd_l))/ float(len(abs_perc_rnd_l))
            #     # print('---------- k = ' +str(k/float(len(tweet_disp_sort)))+ '-------------')
            #     # print(str(above_avg_rnd/float(above_avg_rnd+less_avg_rnd)))
            #     # print(str(above_avg/float(above_avg+less_avg)))
            #     # print(perc_rnd_acc)
            #     # print(abs_perc_rnd_acc)
            #     # print('------------------------------')
            #
            #     norm_abs_perc_bias_l.append(above_avg_rnd/float(above_avg_rnd+less_avg_rnd))
            #     abs_perc_bias_l.append(above_avg/float(above_avg+less_avg))
            #     k_l.append(np.round(k/float(len(tweet_disp_sort)), 3))
            #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_l ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_l, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their NAPB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_l ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_l, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their APB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_abs_perception'
            # # mplpl.savefig(pp, format='png')
            # #
            # # mplpl.figure()
            # #
            # #
            # #
            # #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_ll ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_ll, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg NAPB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_ll ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_ll, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg APB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-abs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.show()
            #
            #
            #
            #     ##################################################
            #
            # print(np.mean(tweet_abs_dev_avg_rnd.values()))
            # exit()


            # tweet_avg_group = collections.defaultdict()
            # tweet_med_group = collections.defaultdict()
            # tweet_var_group = collections.defaultdict()
            #
            # tweet_kldiv_group= collections.defaultdict()

            tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l=[]
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            popularity_l= []
            vote_l = []
            gr_disp_l = []
            for t_id in tweet_l_sort:
                gt_l.append(tweet_gt_var[t_id])
                pt_l.append(tweet_avg[t_id])
                disputability_l.append(tweet_var[t_id])
                perc_l.append(tweet_dev_avg[t_id])
                abs_perc_l.append(tweet_abs_dev_avg[t_id])
                # popularity_l.append(tweet_popularity[t_id])
                vote_l.append(tweet_vote_avg[t_id])
                # vote_l.append(tweet_vote_var[t_id])
                perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
                abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
                tweet_skew_ll.append(tweet_skew[t_id])
                gr_disp_l.append(tweet_skew[t_id])
            value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l, vote_l,gr_disp_l]#,popularity_l,tweet_skew_ll]
            value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
                          'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'vote', 'group_disputability']#,'popularity' 'skewness']

            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(9):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(9):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')


            exit()
            tweet_group_gt = collections.defaultdict(list)
            gt_set = sorted(set(gt_l))
            for gt_e in gt_set:
                for t_id in tweet_l_sort:
                    if tweet_gt_var[t_id]==gt_e:
                        tweet_group_gt[gt_e].append(t_id)

            pt_mean = []
            disp_mean = []
            perc_mean = []
            abs_perc_mean = []
            for gt_e in gt_set:
                t_id_l = tweet_group_gt[gt_e]
                pt_mean.append(np.mean([tweet_avg[x] for x in t_id_l]))
                disp_mean.append(np.mean([tweet_var[x] for x in t_id_l]))
                perc_mean.append(np.mean([tweet_dev_avg[x] for x in t_id_l]))
                abs_perc_mean.append(np.mean([tweet_abs_dev_avg[x] for x in t_id_l]))

            # outF = open(remotedir + 'table_out.txt', 'w')

            # outF.write('== ' + data_name + ' ==\n')
            font = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 16,
                    }

            font_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 16,
                    }


            font_t = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 12,
                    }

            font_t_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 12,
                    }



            # fig_f = True
            fig_f = False
            if fig_f==True:

                mplpl.scatter(gt_l, pt_l,  s=40,color='r',marker='o', label='All users')
                mplpl.scatter(gt_set, pt_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]

                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_avg[x]>0:
                            num_pos+=1
                        elif tweet_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e , 0.85, str(num_pos) , fontdict=font)
                    mplpl.text(gt_e , 0.75, str(num_neg) , fontdict=font_1)

                mplpl.text(-0.3, -0.75, ' # news has positive perception truth value', fontdict=font_t)
                mplpl.text(-0.3, -0.9,   '# news has negative perception truth value', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_pt| alt text| width = 500px}}')


                mplpl.scatter(gt_l,disputability_l ,  s=40,color='g',marker='o', label='All users')
                mplpl.scatter(gt_set, disp_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, disp_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                # for gt_e in gt_set:
                #     t_id_l = tweet_group_gt[gt_e]

                #     num_pos=0
                #     num_neg = 0
                #
                #     for x in t_id_l:
                #         if tweet_var[x]>0:
                #             num_pos+=1
                #         elif tweet_var[x]<0:
                #             num_neg+=1
                #     # mplpl.text(gt_e-0.2, 0.85, str(num_pos) + r'$ > 0$', fontdict=font)
                #     # mplpl.text(gt_e-0.2, 0.75, str(num_neg) + r'$ < 0$', fontdict=font_1)

                #
                # mplpl.text(0, 0.9, ' # news perceived true(postitive)', fontdict=font_t)
                # mplpl.text(0, 0.75,   '# news perceived false(negative)', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_disputability'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_disputability| alt text| width = 500px}}')


                gt_l_dict[data_name] = gt_l
                perc_l_dict[data_name] = perc_l
                gt_set_dict[data_name] = gt_set
                perc_mean_dict[data_name] = perc_mean

                mplpl.scatter(gt_l, perc_l,  s=40,color='b',marker='o', label='All users')
                mplpl.scatter(gt_set, perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, perc_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]
                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_dev_avg[x]>0:
                            num_pos+=1
                        elif tweet_dev_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e, 1.65, str(num_pos), fontdict=font)
                    mplpl.text(gt_e, 1.3, str(num_neg), fontdict=font_1)

                mplpl.text(-1, -1, '# news that has positive perception bias value', fontdict=font_t)
                mplpl.text(-1, -1.5, '# news that has nigative perception bias value', fontdict=font_t_1)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_perception| alt text| width = 500px}}')


                mplpl.scatter(gt_l,abs_perc_l ,  s=40,color='c',marker='o', label='All users')
                mplpl.scatter(gt_set, abs_perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, abs_perc_mean, color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)


                abs_perc_l_dict[data_name] = abs_perc_l
                abs_perc_mean_dict[data_name] = abs_perc_mean


                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_abs_perception| alt text| width = 500px}} ||\n')

                # outF.write('|| Table ||\n\n')

            # exit
        ########################

                tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)
                pt_l = []
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_avg[t_id])

                mplpl.scatter(range(len(pt_l)), pt_l,  s=40,color='r',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PTL', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_pt| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=False)
                disputability_l = []
                for t_id in tweet_l_sort:
                    disputability_l.append(tweet_var[t_id])


                mplpl.scatter(range(len(pt_l)), disputability_l ,  s=40,color='g',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ranked news stories according Disputability', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_disput'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_disput| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=False)
                perc_l = []
                for t_id in tweet_l_sort:
                    perc_l.append(tweet_dev_avg[t_id])

                mplpl.scatter(range(len(pt_l)), perc_l,  s=40,color='b',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias (PB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_percep| alt text| width = 500px}}')



                tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=False)
                perc_abs_l = []
                for t_id in tweet_l_sort:
                    perc_abs_l.append(tweet_abs_dev_avg[t_id])

                mplpl.scatter(range(len(perc_abs_l)), perc_abs_l,  s=40,color='c',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according APB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_abs_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_abs-percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_abs-percep| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



        #####################################################33


                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='r', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')


                num_bins = len(disputability_l)
                counts, bin_edges = np.histogram(disputability_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Disputability', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_cdf| alt text| width = 500px}}')


                num_bins = len(perc_l)
                counts, bin_edges = np.histogram(perc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='b', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception bias (PB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_cdf| alt text| width = 500px}}')

                num_bins = len(perc_abs_l)
                counts, bin_edges = np.histogram(perc_abs_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_cdf| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



                # mplpl.show()

                col_l = ['r', 'b', 'g']

                i = 0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=400, color=col_l[i], marker='*')
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i = 0
                mark_l = ['*', 'o', '^']
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i=0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o')
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=300, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                i=0
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()











                mplpl.show()

            else:

                AVG_list = []
                print(np.mean(all_acc))
                outF = open(remotedir + 'output.txt', 'w')

                tweet_all_var = {}
                tweet_all_dev_avg = {}
                tweet_all_avg = {}
                tweet_all_gt_var = {}
                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []
                tweet_all_avg_l = []
                tweet_all_med_l = []
                tweet_all_var_l = []
                tweet_all_gt_var_l = []
                diff_group_disp_l = []
                dem_disp_l = []
                rep_disp_l = []

                tweet_all_dev_avg = {}
                tweet_all_dev_med = {}
                tweet_all_dev_var = {}

                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []

                tweet_all_abs_dev_avg = {}
                tweet_all_abs_dev_med = {}
                tweet_all_abs_dev_var = {}

                tweet_all_abs_dev_avg_l = []
                tweet_all_abs_dev_med_l = []
                tweet_all_abs_dev_var_l = []
                tweet_all_dev_avg_rnd = {}
                tweet_all_abs_dev_avg_rnd = {}

                diff_group_disp_dict = {}
                if dataset == 'snopes':
                    data_n = 'sp'
                    news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                    ind_l = [1, 2, 3]
                elif dataset == 'politifact':
                    data_n = 'pf'
                    news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                    ind_l = [1, 2, 3]
                elif dataset == 'mia':
                    data_n = 'mia'
                    news_cat_list = ['rumor', 'non-rumor']
                    ind_l = [1]

                for cat_l in news_cat_list:
                    outF.write('== ' + str(cat_l) + ' ==\n\n')
                    print('== ' + str(cat_l) + ' ==')
                    tweet_dev_avg = {}
                    tweet_dev_med = {}
                    tweet_dev_var = {}
                    tweet_abs_dev_avg = {}
                    tweet_abs_dev_med = {}
                    tweet_abs_dev_var = {}

                    tweet_avg = {}
                    tweet_med = {}
                    tweet_var = {}
                    tweet_gt_var = {}

                    tweet_dev_avg_rnd = {}
                    tweet_abs_dev_avg_rnd = {}


                    tweet_dev_avg_l = []
                    tweet_dev_med_l = []
                    tweet_dev_var_l = []
                    tweet_abs_dev_avg_l = []
                    tweet_abs_dev_med_l = []
                    tweet_abs_dev_var_l = []

                    tweet_avg_l = []
                    tweet_med_l = []
                    tweet_var_l = []
                    tweet_gt_var_l = []
                    AVG_susc_list = []
                    AVG_wl_list = []
                    all_acc = []
                    AVG_dev_list = []
                    # for lean in [-1, 0, 1]:

                        # AVG_susc_list = []
                        # AVG_wl_list = []
                        # all_acc = []
                        # df_m = df_m[df_m['leaning'] == lean]
                        # if lean == 0:
                        #     col = 'g'
                        #     lean_cat = 'neutral'
                        # elif lean == 1:
                        #     col = 'b'
                        #     lean_cat = 'democrat'
                        # elif lean == -1:
                        #     col = 'r'
                        #     lean_cat = 'republican'
                        # print(lean_cat)
                    for ind in ind_l:

                        if balance_f == 'balanced':
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
                        else:
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'

                        inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
                        df[ind] = pd.read_csv(inp1, sep="\t")
                        df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                        df_m = df[ind].copy()
                        df_mm = df_m.copy()

                        df_m = df_m[df_m['ra_gt'] == cat_l]
                        # df_mm = df_m[df_m['ra_gt']==cat_l]
                        # df_m = df_m[df_m['leaning'] == lean]

                        groupby_ftr = 'tweet_id'
                        grouped = df_m.groupby(groupby_ftr, sort=False)
                        grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                        for t_id in grouped.groups.keys():
                            df_tmp = df_m[df_m['tweet_id'] == t_id]

                            df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
                            df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
                            df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []
                            df_tmp = df_m[df_m['tweet_id'] == t_id]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []

                            weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                            val_list = list(df_tmp['rel_v'])
                            tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_avg[t_id] = np.mean(val_list)
                            tweet_med[t_id] = np.median(val_list)
                            tweet_var[t_id] = np.var(val_list)
                            tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_avg_l.append(np.mean(val_list))
                            tweet_med_l.append(np.median(val_list))
                            tweet_var_l.append(np.var(val_list))
                            tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])




                            tweet_all_avg[t_id] = np.mean(val_list)
                            tweet_all_var[t_id] = np.var(val_list)
                            tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_all_avg_l.append(np.mean(val_list))
                            tweet_all_med_l.append(np.median(val_list))
                            tweet_all_var_l.append(np.var(val_list))
                            tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])



                            val_list = list(df_tmp['err'])
                            abs_var_err = [np.abs(x) for x in val_list]
                            tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_dev_avg[t_id] = np.mean(val_list)
                            tweet_dev_med[t_id] = np.median(val_list)
                            tweet_dev_var[t_id] = np.var(val_list)

                            tweet_dev_avg_l.append(np.mean(val_list))
                            tweet_dev_med_l.append(np.median(val_list))
                            tweet_dev_var_l.append(np.var(val_list))

                            tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_abs_dev_var_l.append(np.var(abs_var_err))


                            tweet_all_dev_avg[t_id] = np.mean(val_list)
                            tweet_all_dev_med[t_id] = np.median(val_list)
                            tweet_all_dev_var[t_id] = np.var(val_list)

                            tweet_all_dev_avg_l.append(np.mean(val_list))
                            tweet_all_dev_med_l.append(np.median(val_list))
                            tweet_all_dev_var_l.append(np.var(val_list))

                            tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_all_abs_dev_var_l.append(np.var(abs_var_err))



                            sum_rnd_abs_perc = 0
                            sum_rnd_perc = 0
                            for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                                sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                                sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                            random_perc = np.abs(sum_rnd_perc / float(7))
                            random_abs_perc = sum_rnd_abs_perc / float(7)

                            tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                            tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                    gt_l = []
                    pt_l = []
                    disputability_l = []
                    perc_l = []
                    abs_perc_l = []
                    # for t_id in tweet_l_sort:
                    #     gt_l.append(tweet_gt_var[t_id])
                    #     pt_l.append(tweet_avg[t_id])
                    #     disputability_l.append(tweet_var[t_id])
                    #     perc_l.append(tweet_dev_avg[t_id])
                    #     abs_perc_l.append(tweet_abs_dev_avg[t_id])



                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
                    tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)

                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)


                    if dataset == 'snopes':
                        data_addr = 'snopes'
                    elif dataset == 'politifact':
                        data_addr = 'politifact/fig'
                    elif dataset == 'mia':
                        data_addr = 'mia/fig'

                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')

                    for t_id in tweet_l_sort:
                        count+=1
                        if balance_f=='balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
                                       + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
                                       # str(np.round(diff_group_disp_dict[t_id], 3)) +
                                       '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
                                        + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
                                        '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')




                if dataset == 'snopes':
                    data_addr = 'snopes'
                elif dataset == 'politifact':
                    data_addr = 'politifact/fig'
                elif dataset == 'mia':
                    data_addr = 'mia/fig'

                # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
                tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)

                # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)

                tweet_napb_dict_high_disp = {}
                tweet_napb_dict_low_disp = {}
                for t_id in tweet_l_sort[:20]:
                    # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                for t_id in tweet_l_sort[-20:]:
                    # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                kk = 0

                for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
                    if kk==0:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
                    else:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)

                    kk+=1
                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    for t_id in tweet_l_sort:
                        count += 1
                        # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
                        if balance_f == 'balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
                                       str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +
                            #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
                            #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                                       str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                        # +
                        # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                        # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_labeld_news_ktop_nptl_together":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        # dataset = 'snopes'
        # dataset = 'mia'
        dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes_nonpol','politifact','mia','politifact','snopes','mia','politifact']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            if dataset=='snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpol'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()
            tweet_var_diff_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            tweet_chi_group = {}
            tweet_chi_group_1 = {}
            tweet_chi_group_2 = {}
            tweet_skew = {}
            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    val_list = list(df_tmp['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)


                    # n_dem, bins, patches = Plab.hist(dem_val_list,
                    #                                      bins=frange(0, 1, 0.05), normed=1)
                    # n_rep, bins, patches = Plab.hist(rep_val_list,
                    #                                      bins=frange(0, 1, 0.05), normed=1)
                    # n_neut, bins, patches = Plab.hist(neut_val_list,
                    #                                   bins=frange(0, 1, 0.05), normed=1)
                    #
                    # dem_rep_chi = chi_sqr(n_dem, n_rep)
                    # dem_neut_chi = chi_sqr(n_dem, n_neut)
                    # neut_rep_chi = chi_sqr(n_neut, n_rep)



                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_diff_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) + np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)

                    # if ~(dem_rep_chi > 0 or dem_rep_chi <0 or dem_rep_chi ==0):
                    #     tweet_chi_group[t_id] = 0
                    # else:
                    #     tweet_chi_group[t_id] = dem_rep_chi
                    #
                    # if ~(dem_neut_chi > 0 or dem_neut_chi <0 or dem_neut_chi ==0):
                    #     tweet_chi_group_1[t_id] = 0
                    # else:
                    #     tweet_chi_group_1[t_id] = dem_neut_chi
                    #
                    # if ~(dem_neut_chi > 0 or dem_neut_chi <0 or dem_neut_chi ==0):
                    #     tweet_chi_group_2[t_id] = 0
                    # else:
                    #     tweet_chi_group_2[t_id] = dem_rep_chi


                    # tweet_chi_group[t_id] = np.var([tweet_chi_group[t_id], tweet_chi_group_1[t_id], tweet_chi_group_2[t_id]])

                    # tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew[t_id] = scipy.stats.skew(dem_val_list) + scipy.stats.skew(rep_val_list)
                    # tweet_skew_l.append(tweet_skew[t_id])

                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])
                    # vot_list_tmp = []

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    # tweet_skew[t_id] = scipy.stats.skew(val_list)
                    # tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)




            ##################################################
            # news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE','TRUE']
            # news_cat_list_f = [ 'false','mostly_false', 'mixture', 'mostly_true', 'true']
            len_cat_dict = {}
            if dataset=='snopes' or dataset=='politifact':
                for cat in news_cat_list:
                    len_cat_dict[cat]=30
            elif dataset == 'snopes_nonpol':
                for cat in news_cat_list:
                    len_cat_dict[cat] = 20
            elif dataset=='mia':
                for cat in news_cat_list:
                    if cat=='rumor':
                        len_cat_dict[cat]=30
                    else:
                        len_cat_dict[cat] = 30
            tweet_vote_sort = sorted(tweet_vote_avg, key=tweet_vote_avg.get, reverse=False)

            # tweet_vote_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            tweet_var_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            thr = 20
            thr_list = []
            len_t = len(tweet_vote_sort)
            categ_dict = collections.defaultdict(int)
            k_list = [int(0.1*len_t), int(0.2*len_t), int(0.3*len_t), int(1*len_t) ]
            count = 0
            for k in k_list:
                # k = (i+1)*thr
                # k = k_list[count]
                # count+=1
                thr_list.append(k)
                perc_rnd_l = []
                abs_perc_rnd_l = []
                disputability_l = []
                above_avg = 0
                less_avg = 0
                above_avg_rnd = 0
                less_avg_rnd = 0
                above_avg = 0
                less_avg = 0
                categ_dict[k] = collections.defaultdict(float)
                for j in range(k):
                    for cat_n in news_cat_list:
                        if tweet_lable_dict[tweet_vote_sort[j]] == cat_n:
                            categ_dict[k][cat_n]+=(1/float(len_cat_dict[cat_n]))

            # if dataset=='mia':
            for j in categ_dict:
                sum = np.sum(categ_dict[j].values())
                for cat_n in categ_dict[j]:
                    categ_dict[j][cat_n] =  categ_dict[j][cat_n] / sum




            thr = 20
            thr_list = []
            len_var = len(tweet_var_sort)
            categ_dict_var = collections.defaultdict(int)
            k_list = [int(0.1*len_var), int(0.2*len_var), int(0.3*len_t), int(1*len_var) ]
            count = 0
            for k in k_list:
                # k = (i+1)*thr
                # k = k_list[count]
                # count+=1
                thr_list.append(k)
                perc_rnd_l = []
                abs_perc_rnd_l = []
                disputability_l = []
                above_avg = 0
                less_avg = 0
                above_avg_rnd = 0
                less_avg_rnd = 0
                above_avg = 0
                less_avg = 0
                categ_dict_var[k] = collections.defaultdict(float)
                for j in range(k):
                    for cat_n in news_cat_list:
                        if tweet_lable_dict[tweet_var_sort[j]] == cat_n:
                            categ_dict_var[k][cat_n]+=(1/float(len_cat_dict[cat_n]))

            # if dataset=='mia':
            for j in categ_dict_var:
                sum = np.sum(categ_dict_var[j].values())
                for cat_n in categ_dict_var[j]:
                    categ_dict_var[j][cat_n] =  categ_dict_var[j][cat_n] / sum









            width = 0.03
            pr = -10
            title_l = news_cat_list
            outp = {}
            outp_var = {}
            # news_cat_list = ['pants-fire', 'false', 'mostly_false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            if dataset=='snopes' or dataset=='snopes_nonpol':
                # col_l = ['b', 'g', 'c', 'y', 'r']
                col_l = ['red', 'orange', 'gray', 'lime', 'green']

                news_cat_list_n = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
            if dataset=='politifact':
                # col_l = ['grey','b', 'g', 'c', 'y', 'r']
                col_l = ['darkred', 'red', 'orange', 'gray', 'lime', 'green']

                news_cat_list_n = ['PANTS ON FIRE', 'FALSE', 'MOSTLY FALSE', 'HALF TRUE', 'MOSTLY TRUE', 'TRUE']

            if dataset=='mia':
                # col_l = ['b', 'r']
                col_l = ['red', 'green']
                news_cat_list_n = ['RUMORS', 'NON RUMORS']
            count = 0
            Y = [0]*len(thr_list)
            Y1 = [0]*len(thr_list)
            mplpl.rcParams['figure.figsize'] = 6.8, 5
            mplpl.rc('xtick', labelsize='large')
            mplpl.rc('ytick', labelsize='large')
            mplpl.rc('legend', fontsize='small')

            for cat_m in news_cat_list:
                count+=1
                outp[cat_m] = []
                outp_var[cat_m] = []
                for i in thr_list:
                    outp[cat_m].append(categ_dict[i][cat_m])
                    outp_var[cat_m].append(categ_dict_var[i][cat_m])
                # mplpl.bar([xx/float(len(tweet_vote_sort)) for xx in thr_list], outp[cat_m], width, bottom= np.array(Y), color=col_l[count-1], label=news_cat_list_n[count-1])
                mplpl.bar([0.1, 0.2, 0.3, 0.4], outp[cat_m], width, bottom= np.array(Y), color=col_l[count-1], label=news_cat_list_n[count-1])
                Y = np.array(Y) + np.array(outp[cat_m])

                mplpl.bar([0.1+0.03, 0.2+0.03, 0.3+0.03,0.4+0.03], outp_var[cat_m], width, bottom=np.array(Y1), color=col_l[count - 1])
                Y1 = np.array(Y1) + np.array(outp_var[cat_m])

            mplpl.xlim([0.08, 0.5])
            mplpl.ylim([0, 1.38])
            mplpl.ylabel('Composition of labeled news stories', fontsize=14,fontweight = 'bold')
            # mplpl.xlabel('Top k news stories reported by negative PTL', fontsize=13.8,fontweight = 'bold')
            mplpl.xlabel('K fraction stories ranked by \n negative PTL(1st bar) and Disputability(2nd bar)', fontsize=14,fontweight = 'bold')
            # mplpl.xlabel('Top k news stories ranked by NAPB', fontsize=18)

            mplpl.legend(loc="upper center", ncol=3, fontsize='small')

            mplpl.subplots_adjust(bottom=0.2)

            mplpl.subplots_adjust(left=0.18)
            mplpl.grid()
            mplpl.title(data_name, fontsize='x-large')
            labels = ['0.1', '0.2','0.3','1.0']
            x = [ 0.12, 0.22, 0.33, 0.44]
            mplpl.xticks(x, labels)
            # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_vote_composition_gt'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_vote_composition_gt'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_napb_composition_gt'
            pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_vote_disp_composition_gt'
            mplpl.savefig(pp + '.pdf', format='pdf')
            mplpl.savefig(pp + '.png', format='png')

            mplpl.figure()

            # mplpl.rcParams['figure.figsize'] = 5.4, 3.2
            # mplpl.rc('xtick', labelsize='xx-large')
            # mplpl.rc('ytick', labelsize='xx-large')
            # mplpl.rc('legend', fontsize='xx-large')
            # for cat_m in news_cat_list:
            #     count += 1
            #     pt_l_dict[cat_m] = []
            #     for t_id in tweet_l_sort:
            #         if tweet_lable_dict[t_id] == cat_m:
            #             pt_l_dict[cat_m].append(tweet_dev_avg[t_id])
            #
            #     num_bins = len(pt_l_dict[cat_m])
            #     counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
            #     cdf = np.cumsum(counts)
            #     scale = 1.0 / cdf[-1]
            #     ncdf = scale * cdf
            #     mplpl.plot(bin_edges[1:], ncdf, c=col_l[count - 1], lw=5, label=cat_m)
            #
            # mplpl.ylabel('CDF', fontsize=28)
            # mplpl.xlabel('Mean pereption bias', fontsize=28)
            # mplpl.grid()
            # mplpl.title(data_name, fontsize='xx-large')
            # mplpl.legend(loc="lower right", fontsize='x-large')
            # mplpl.xlim([-1.5, 1.5])
            # mplpl.ylim([0, 1])
            # mplpl.subplots_adjust(bottom=0.14)
            #
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_mpb_cdf.pdf'
            # mplpl.savefig(pp, format='pdf')
            # mplpl.figure()

            exit()
            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l = []
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            thr = 5
            prec_rnd_acc = 0
            abs_prec_rnd_acc = 0
            norm_abs_perc_bias_l = []
            abs_perc_bias_l = []
            norm_abs_perc_bias_ll = []
            abs_perc_bias_ll = []
            k_l = []
            # for i in range(int(len(tweet_disp_sort)/float(thr))):
            #     k = (i+1)*thr
            #     perc_rnd_l = []
            #     abs_perc_rnd_l = []
            #     disputability_l = []
            #     above_avg = 0
            #     less_avg = 0
            #     above_avg_rnd = 0
            #     less_avg_rnd = 0
            #     above_avg = 0
            #     less_avg = 0
            #     for j in range(k):
            #
            #         perc_rnd_l.append(tweet_abs_dev_avg[tweet_disp_sort[j]])
            #         abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[tweet_disp_sort[j]])
            #
            #         if tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg_rnd.values()):
            #             above_avg_rnd+=1
            #         elif tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg_rnd.values()):
            #             less_avg_rnd+=1
            #
            #
            #         if tweet_abs_dev_avg[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg.values()):
            #             above_avg+=1
            #         elif tweet_abs_dev_avg[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg.values()):
            #             less_avg+=1
            #
            #     norm_abs_perc_bias_ll.append(np.mean(abs_perc_rnd_l))
            #     abs_perc_bias_ll.append(np.mean(perc_rnd_l))
            #
            #     # perc_rnd_acc = len(set(disputability_l).intersection(perc_rnd_l))/ float(len(perc_rnd_l))
            #     # abs_perc_rnd_acc = len(set(disputability_l).intersection(abs_perc_rnd_l))/ float(len(abs_perc_rnd_l))
            #     # print('---------- k = ' +str(k/float(len(tweet_disp_sort)))+ '-------------')
            #     # print(str(above_avg_rnd/float(above_avg_rnd+less_avg_rnd)))
            #     # print(str(above_avg/float(above_avg+less_avg)))
            #     # print(perc_rnd_acc)
            #     # print(abs_perc_rnd_acc)
            #     # print('------------------------------')
            #
            #     norm_abs_perc_bias_l.append(above_avg_rnd/float(above_avg_rnd+less_avg_rnd))
            #     abs_perc_bias_l.append(above_avg/float(above_avg+less_avg))
            #     k_l.append(np.round(k/float(len(tweet_disp_sort)), 3))
            #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_l ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_l, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their NAPB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_l ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_l, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their APB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_abs_perception'
            # # mplpl.savefig(pp, format='png')
            # #
            # # mplpl.figure()
            # #
            # #
            # #
            # #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_ll ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_ll, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg NAPB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_ll ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_ll, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg APB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-abs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.show()
            #
            #
            #
            #     ##################################################
            #
            # print(np.mean(tweet_abs_dev_avg_rnd.values()))
            # exit()


            # tweet_avg_group = collections.defaultdict()
            # tweet_med_group = collections.defaultdict()
            # tweet_var_group = collections.defaultdict()
            #
            # tweet_kldiv_group= collections.defaultdict()

            tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l=[]
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            popularity_l= []
            vote_l = []
            gr_disp_l = []
            for t_id in tweet_l_sort:
                gt_l.append(tweet_gt_var[t_id])
                pt_l.append(tweet_avg[t_id])
                disputability_l.append(tweet_var[t_id])
                perc_l.append(tweet_dev_avg[t_id])
                abs_perc_l.append(tweet_abs_dev_avg[t_id])
                # popularity_l.append(tweet_popularity[t_id])
                vote_l.append(tweet_vote_avg[t_id])
                # vote_l.append(tweet_vote_var[t_id])
                perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
                abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
                tweet_skew_ll.append(tweet_skew[t_id])
                gr_disp_l.append(tweet_skew[t_id])
            value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l, vote_l,gr_disp_l]#,popularity_l,tweet_skew_ll]
            value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
                          'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'vote', 'group_disputability']#,'popularity' 'skewness']

            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(9):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(9):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')


            exit()
            tweet_group_gt = collections.defaultdict(list)
            gt_set = sorted(set(gt_l))
            for gt_e in gt_set:
                for t_id in tweet_l_sort:
                    if tweet_gt_var[t_id]==gt_e:
                        tweet_group_gt[gt_e].append(t_id)

            pt_mean = []
            disp_mean = []
            perc_mean = []
            abs_perc_mean = []
            for gt_e in gt_set:
                t_id_l = tweet_group_gt[gt_e]
                pt_mean.append(np.mean([tweet_avg[x] for x in t_id_l]))
                disp_mean.append(np.mean([tweet_var[x] for x in t_id_l]))
                perc_mean.append(np.mean([tweet_dev_avg[x] for x in t_id_l]))
                abs_perc_mean.append(np.mean([tweet_abs_dev_avg[x] for x in t_id_l]))

            # outF = open(remotedir + 'table_out.txt', 'w')

            # outF.write('== ' + data_name + ' ==\n')
            font = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 16,
                    }

            font_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 16,
                    }


            font_t = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 12,
                    }

            font_t_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 12,
                    }



            # fig_f = True
            fig_f = False
            if fig_f==True:

                mplpl.scatter(gt_l, pt_l,  s=40,color='r',marker='o', label='All users')
                mplpl.scatter(gt_set, pt_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]

                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_avg[x]>0:
                            num_pos+=1
                        elif tweet_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e , 0.85, str(num_pos) , fontdict=font)
                    mplpl.text(gt_e , 0.75, str(num_neg) , fontdict=font_1)

                mplpl.text(-0.3, -0.75, ' # news has positive perception truth value', fontdict=font_t)
                mplpl.text(-0.3, -0.9,   '# news has negative perception truth value', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_pt| alt text| width = 500px}}')


                mplpl.scatter(gt_l,disputability_l ,  s=40,color='g',marker='o', label='All users')
                mplpl.scatter(gt_set, disp_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, disp_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                # for gt_e in gt_set:
                #     t_id_l = tweet_group_gt[gt_e]

                #     num_pos=0
                #     num_neg = 0
                #
                #     for x in t_id_l:
                #         if tweet_var[x]>0:
                #             num_pos+=1
                #         elif tweet_var[x]<0:
                #             num_neg+=1
                #     # mplpl.text(gt_e-0.2, 0.85, str(num_pos) + r'$ > 0$', fontdict=font)
                #     # mplpl.text(gt_e-0.2, 0.75, str(num_neg) + r'$ < 0$', fontdict=font_1)

                #
                # mplpl.text(0, 0.9, ' # news perceived true(postitive)', fontdict=font_t)
                # mplpl.text(0, 0.75,   '# news perceived false(negative)', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_disputability'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_disputability| alt text| width = 500px}}')


                gt_l_dict[data_name] = gt_l
                perc_l_dict[data_name] = perc_l
                gt_set_dict[data_name] = gt_set
                perc_mean_dict[data_name] = perc_mean

                mplpl.scatter(gt_l, perc_l,  s=40,color='b',marker='o', label='All users')
                mplpl.scatter(gt_set, perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, perc_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]
                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_dev_avg[x]>0:
                            num_pos+=1
                        elif tweet_dev_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e, 1.65, str(num_pos), fontdict=font)
                    mplpl.text(gt_e, 1.3, str(num_neg), fontdict=font_1)

                mplpl.text(-1, -1, '# news that has positive perception bias value', fontdict=font_t)
                mplpl.text(-1, -1.5, '# news that has nigative perception bias value', fontdict=font_t_1)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_perception| alt text| width = 500px}}')


                mplpl.scatter(gt_l,abs_perc_l ,  s=40,color='c',marker='o', label='All users')
                mplpl.scatter(gt_set, abs_perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, abs_perc_mean, color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)


                abs_perc_l_dict[data_name] = abs_perc_l
                abs_perc_mean_dict[data_name] = abs_perc_mean


                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_abs_perception| alt text| width = 500px}} ||\n')

                # outF.write('|| Table ||\n\n')

            # exit
        ########################

                tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)
                pt_l = []
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_avg[t_id])

                mplpl.scatter(range(len(pt_l)), pt_l,  s=40,color='r',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PTL', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_pt| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=False)
                disputability_l = []
                for t_id in tweet_l_sort:
                    disputability_l.append(tweet_var[t_id])


                mplpl.scatter(range(len(pt_l)), disputability_l ,  s=40,color='g',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ranked news stories according Disputability', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_disput'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_disput| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=False)
                perc_l = []
                for t_id in tweet_l_sort:
                    perc_l.append(tweet_dev_avg[t_id])

                mplpl.scatter(range(len(pt_l)), perc_l,  s=40,color='b',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias (PB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_percep| alt text| width = 500px}}')



                tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=False)
                perc_abs_l = []
                for t_id in tweet_l_sort:
                    perc_abs_l.append(tweet_abs_dev_avg[t_id])

                mplpl.scatter(range(len(perc_abs_l)), perc_abs_l,  s=40,color='c',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according APB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_abs_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_abs-percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_abs-percep| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



        #####################################################33


                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='r', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')


                num_bins = len(disputability_l)
                counts, bin_edges = np.histogram(disputability_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Disputability', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_cdf| alt text| width = 500px}}')


                num_bins = len(perc_l)
                counts, bin_edges = np.histogram(perc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='b', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception bias (PB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_cdf| alt text| width = 500px}}')

                num_bins = len(perc_abs_l)
                counts, bin_edges = np.histogram(perc_abs_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_cdf| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



                # mplpl.show()

                col_l = ['r', 'b', 'g']

                i = 0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=400, color=col_l[i], marker='*')
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i = 0
                mark_l = ['*', 'o', '^']
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i=0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o')
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=300, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                i=0
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()











                mplpl.show()

            else:

                AVG_list = []
                print(np.mean(all_acc))
                outF = open(remotedir + 'output.txt', 'w')

                tweet_all_var = {}
                tweet_all_dev_avg = {}
                tweet_all_avg = {}
                tweet_all_gt_var = {}
                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []
                tweet_all_avg_l = []
                tweet_all_med_l = []
                tweet_all_var_l = []
                tweet_all_gt_var_l = []
                diff_group_disp_l = []
                dem_disp_l = []
                rep_disp_l = []

                tweet_all_dev_avg = {}
                tweet_all_dev_med = {}
                tweet_all_dev_var = {}

                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []

                tweet_all_abs_dev_avg = {}
                tweet_all_abs_dev_med = {}
                tweet_all_abs_dev_var = {}

                tweet_all_abs_dev_avg_l = []
                tweet_all_abs_dev_med_l = []
                tweet_all_abs_dev_var_l = []
                tweet_all_dev_avg_rnd = {}
                tweet_all_abs_dev_avg_rnd = {}

                diff_group_disp_dict = {}
                if dataset == 'snopes':
                    data_n = 'sp'
                    news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                    ind_l = [1, 2, 3]
                elif dataset == 'politifact':
                    data_n = 'pf'
                    news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                    ind_l = [1, 2, 3]
                elif dataset == 'mia':
                    data_n = 'mia'
                    news_cat_list = ['rumor', 'non-rumor']
                    ind_l = [1]

                for cat_l in news_cat_list:
                    outF.write('== ' + str(cat_l) + ' ==\n\n')
                    print('== ' + str(cat_l) + ' ==')
                    tweet_dev_avg = {}
                    tweet_dev_med = {}
                    tweet_dev_var = {}
                    tweet_abs_dev_avg = {}
                    tweet_abs_dev_med = {}
                    tweet_abs_dev_var = {}

                    tweet_avg = {}
                    tweet_med = {}
                    tweet_var = {}
                    tweet_gt_var = {}

                    tweet_dev_avg_rnd = {}
                    tweet_abs_dev_avg_rnd = {}


                    tweet_dev_avg_l = []
                    tweet_dev_med_l = []
                    tweet_dev_var_l = []
                    tweet_abs_dev_avg_l = []
                    tweet_abs_dev_med_l = []
                    tweet_abs_dev_var_l = []

                    tweet_avg_l = []
                    tweet_med_l = []
                    tweet_var_l = []
                    tweet_gt_var_l = []
                    AVG_susc_list = []
                    AVG_wl_list = []
                    all_acc = []
                    AVG_dev_list = []
                    # for lean in [-1, 0, 1]:

                        # AVG_susc_list = []
                        # AVG_wl_list = []
                        # all_acc = []
                        # df_m = df_m[df_m['leaning'] == lean]
                        # if lean == 0:
                        #     col = 'g'
                        #     lean_cat = 'neutral'
                        # elif lean == 1:
                        #     col = 'b'
                        #     lean_cat = 'democrat'
                        # elif lean == -1:
                        #     col = 'r'
                        #     lean_cat = 'republican'
                        # print(lean_cat)
                    for ind in ind_l:

                        if balance_f == 'balanced':
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
                        else:
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'

                        inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
                        df[ind] = pd.read_csv(inp1, sep="\t")
                        df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                        df_m = df[ind].copy()
                        df_mm = df_m.copy()

                        df_m = df_m[df_m['ra_gt'] == cat_l]
                        # df_mm = df_m[df_m['ra_gt']==cat_l]
                        # df_m = df_m[df_m['leaning'] == lean]

                        groupby_ftr = 'tweet_id'
                        grouped = df_m.groupby(groupby_ftr, sort=False)
                        grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                        for t_id in grouped.groups.keys():
                            df_tmp = df_m[df_m['tweet_id'] == t_id]

                            df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
                            df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
                            df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []
                            df_tmp = df_m[df_m['tweet_id'] == t_id]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []

                            weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                            val_list = list(df_tmp['rel_v'])
                            tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_avg[t_id] = np.mean(val_list)
                            tweet_med[t_id] = np.median(val_list)
                            tweet_var[t_id] = np.var(val_list)
                            tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_avg_l.append(np.mean(val_list))
                            tweet_med_l.append(np.median(val_list))
                            tweet_var_l.append(np.var(val_list))
                            tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])




                            tweet_all_avg[t_id] = np.mean(val_list)
                            tweet_all_var[t_id] = np.var(val_list)
                            tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_all_avg_l.append(np.mean(val_list))
                            tweet_all_med_l.append(np.median(val_list))
                            tweet_all_var_l.append(np.var(val_list))
                            tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])



                            val_list = list(df_tmp['err'])
                            abs_var_err = [np.abs(x) for x in val_list]
                            tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_dev_avg[t_id] = np.mean(val_list)
                            tweet_dev_med[t_id] = np.median(val_list)
                            tweet_dev_var[t_id] = np.var(val_list)

                            tweet_dev_avg_l.append(np.mean(val_list))
                            tweet_dev_med_l.append(np.median(val_list))
                            tweet_dev_var_l.append(np.var(val_list))

                            tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_abs_dev_var_l.append(np.var(abs_var_err))


                            tweet_all_dev_avg[t_id] = np.mean(val_list)
                            tweet_all_dev_med[t_id] = np.median(val_list)
                            tweet_all_dev_var[t_id] = np.var(val_list)

                            tweet_all_dev_avg_l.append(np.mean(val_list))
                            tweet_all_dev_med_l.append(np.median(val_list))
                            tweet_all_dev_var_l.append(np.var(val_list))

                            tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_all_abs_dev_var_l.append(np.var(abs_var_err))



                            sum_rnd_abs_perc = 0
                            sum_rnd_perc = 0
                            for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                                sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                                sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                            random_perc = np.abs(sum_rnd_perc / float(7))
                            random_abs_perc = sum_rnd_abs_perc / float(7)

                            tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                            tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                    gt_l = []
                    pt_l = []
                    disputability_l = []
                    perc_l = []
                    abs_perc_l = []
                    # for t_id in tweet_l_sort:
                    #     gt_l.append(tweet_gt_var[t_id])
                    #     pt_l.append(tweet_avg[t_id])
                    #     disputability_l.append(tweet_var[t_id])
                    #     perc_l.append(tweet_dev_avg[t_id])
                    #     abs_perc_l.append(tweet_abs_dev_avg[t_id])



                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
                    tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)

                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)


                    if dataset == 'snopes':
                        data_addr = 'snopes'
                    elif dataset == 'politifact':
                        data_addr = 'politifact/fig'
                    elif dataset == 'mia':
                        data_addr = 'mia/fig'

                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')

                    for t_id in tweet_l_sort:
                        count+=1
                        if balance_f=='balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
                                       + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
                                       # str(np.round(diff_group_disp_dict[t_id], 3)) +
                                       '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
                                        + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
                                        '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')




                if dataset == 'snopes':
                    data_addr = 'snopes'
                elif dataset == 'politifact':
                    data_addr = 'politifact/fig'
                elif dataset == 'mia':
                    data_addr = 'mia/fig'

                # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
                tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)

                # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)

                tweet_napb_dict_high_disp = {}
                tweet_napb_dict_low_disp = {}
                for t_id in tweet_l_sort[:20]:
                    # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                for t_id in tweet_l_sort[-20:]:
                    # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                kk = 0

                for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
                    if kk==0:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
                    else:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)

                    kk+=1
                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    for t_id in tweet_l_sort:
                        count += 1
                        # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
                        if balance_f == 'balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
                                       str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +
                            #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
                            #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                                       str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                        # +
                        # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                        # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_true-false(gt-pt)_news_ktop_nptl":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        # dataset = 'snopes'
        # dataset = 'mia'
        dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','mia','politifact','snopes','mia','politifact']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()
            tweet_var_diff_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            tweet_chi_group = {}
            tweet_chi_group_1 = {}
            tweet_chi_group_2 = {}
            tweet_skew = {}
            news_cat_list_tf = [4,2,3,1]
            t_f_dict_len = collections.defaultdict(int)
            t_f_dict = {}
            if dataset=='snopes':
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
            if dataset=='politifact':
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]

            if dataset=='mia':
                news_cat_list_t_f = [['rumor'], ['non-rumor']]

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]


                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])
                    # vot_list_tmp = []

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))
                    # ['FALSE-FALSE', 'FALSE-TRUE', 'TRUE-FALSE', 'TRUE-TRUE']

                    if tweet_lable_dict[t_id] in news_cat_list_t_f[0] and np.mean(df_tmp['rel_v']) < 0:
                        t_f_dict[t_id] = 4
                        t_f_dict_len[4]+=1
                    elif tweet_lable_dict[t_id] in news_cat_list_t_f[0] and np.mean(df_tmp['rel_v']) > 0:
                        t_f_dict[t_id] = 2
                        t_f_dict_len[2]+=1
                    elif tweet_lable_dict[t_id] in news_cat_list_t_f[1] and np.mean(df_tmp['rel_v']) > 0:
                        t_f_dict[t_id] = 1
                        t_f_dict_len[1]+=1
                    elif tweet_lable_dict[t_id] in news_cat_list_t_f[1] and np.mean(df_tmp['rel_v']) < 0:
                        t_f_dict[t_id] = 3
                        t_f_dict_len[3]+=1
                    else:
                        t_f_dict[t_id] = -10
                        t_f_dict_len[-10] += 1

                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    val_list = list(df_tmp['rel_v'])




                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_diff_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) + np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)



                    # tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew[t_id] = scipy.stats.skew(dem_val_list) + scipy.stats.skew(rep_val_list)
                    # tweet_skew_l.append(tweet_skew[t_id])

                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])




                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    # tweet_skew[t_id] = scipy.stats.skew(val_list)
                    # tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)




            ##################################################
            # news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE','TRUE']
            # news_cat_list_f = [ 'false','mostly_false', 'mixture', 'mostly_true', 'true']
            len_cat_dict = {}
            # if dataset=='snopes' or dataset=='politifact':
            for cat in news_cat_list_tf:
                len_cat_dict[cat]=t_f_dict_len[cat]
            # elif dataset=='mia':
            #     for cat in news_cat_list_tf:
            #         if cat=='rumor':
            #             len_cat_dict[cat]=t_f_dict_len[cat]
            #         else:
            #             len_cat_dict[cat] = t_f_dict_len[cat]
            # tweet_vote_sort = sorted(tweet_vote_avg, key=tweet_vote_avg.get, reverse=False)

            # tweet_vote_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            tweet_vote_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            thr = 10
            thr_list = []
            categ_dict = collections.defaultdict(int)
            len_t = len(tweet_vote_sort)
            k_list = [int(0.1*len_t), int(0.2*len_t), int(0.3*len_t), int(0.4*len_t), int(0.5*len_t), int(1*len_t) ]
            count = 0
            for k in k_list:
                thr_list.append(k)
                perc_rnd_l = []
                abs_perc_rnd_l = []
                disputability_l = []
                above_avg = 0
                less_avg = 0
                above_avg_rnd = 0
                less_avg_rnd = 0
                above_avg = 0
                less_avg = 0
                categ_dict[k] = collections.defaultdict(float)
                for j in range(k):
                    for cat_n in news_cat_list_tf:
                        if cat_n==-10:
                            continue
                        if t_f_dict[tweet_vote_sort[j]] == cat_n:
                            categ_dict[k][cat_n]+=1/float(len_cat_dict[cat_n])

            # if dataset=='mia':
            for j in categ_dict:
                sum = np.sum(categ_dict[j].values())
                # for cat_n in categ_dict[j]:
                for cat_n in [4, 2, 3, 1]:
                    categ_dict[j][cat_n] =  categ_dict[j][cat_n] / sum

            width = 0.03
            pr = -10
            title_l = news_cat_list_tf
            outp = {}
            # news_cat_list = ['pants-fire', 'false', 'mostly_false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            if dataset=='snopes':
                # col_l = ['b', 'g', 'c', 'y', 'r']
                col_l = ['red', 'magenta', 'lime', 'green']

                news_cat_list_n = ['GT:FALSE, PT:FALSE', 'GT:FALSE, PT:TRUE', 'GT:TRUE, PT:FALSE', 'GT:TRUE, PT:TRUE']
            if dataset=='politifact':
                # col_l = ['grey','b', 'g', 'c', 'y', 'r']
                col_l = ['red', 'magenta', 'lime', 'green']

                news_cat_list_n = ['FALSE-FALSE', 'FALSE-TRUE', 'TRUE-FALSE', 'TRUE-TRUE']

            if dataset=='mia':
                # col_l = ['b', 'r']
                col_l = ['red', 'magenta', 'lime', 'green']
                news_cat_list_n = ['FALSE-FALSE', 'FALSE-TRUE', 'TRUE-FALSE', 'TRUE-TRUE']
            count = 0
            Y = [0]*len(thr_list)
            mplpl.rcParams['figure.figsize'] = 5.4, 4
            mplpl.rc('xtick', labelsize='large')
            mplpl.rc('ytick', labelsize='large')
            mplpl.rc('legend', fontsize='small')
            for cat_m in news_cat_list_tf:
                count+=1
                outp[cat_m] = []
                for i in thr_list:
                    outp[cat_m].append(categ_dict[i][cat_m])
                mplpl.bar([0.1, 0.2, 0.3, 0.4,0.5,0.6], outp[cat_m], width, bottom= np.array(Y), color=col_l[count-1], label=news_cat_list_n[count-1])
                Y = np.array(Y) + np.array(outp[cat_m])






            mplpl.xlim([0.08, 0.64])
            mplpl.ylim([0, 1.38])



            mplpl.subplots_adjust(bottom=0.16)

            mplpl.subplots_adjust(left=0.18)
            mplpl.title(data_name)
            labels = ['0.1', '0.2', '0.3', '0.4', '0.5','1.0']
            x = [ 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
            mplpl.xticks(x, labels)

            mplpl.ylabel('Composition of news stories \n in different quadrants', fontsize=14,fontweight = 'bold')
            # mplpl.xlabel('Top k news stories reported by negative PTL', fontsize=14,fontweight = 'bold')
            mplpl.xlabel('Top k news stories ranked by disputability', fontsize=18)
            # mplpl.xlabel('Top k news stories ranked by NAPB', fontsize=18)

            mplpl.legend(loc="upper center", ncol=2, fontsize='small')



            mplpl.grid()
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_vote_compos_true-false'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_napb_compos_true-false'
            pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_disp_compos_true-false'
            mplpl.savefig(pp + '.pdf', format='pdf')
            mplpl.savefig(pp+ '.png', format='png')

            mplpl.figure()


            exit()





























            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l = []
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            thr = 5
            prec_rnd_acc = 0
            abs_prec_rnd_acc = 0
            norm_abs_perc_bias_l = []
            abs_perc_bias_l = []
            norm_abs_perc_bias_ll = []
            abs_perc_bias_ll = []
            k_l = []
            # for i in range(int(len(tweet_disp_sort)/float(thr))):
            #     k = (i+1)*thr
            #     perc_rnd_l = []
            #     abs_perc_rnd_l = []
            #     disputability_l = []
            #     above_avg = 0
            #     less_avg = 0
            #     above_avg_rnd = 0
            #     less_avg_rnd = 0
            #     above_avg = 0
            #     less_avg = 0
            #     for j in range(k):
            #
            #         perc_rnd_l.append(tweet_abs_dev_avg[tweet_disp_sort[j]])
            #         abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[tweet_disp_sort[j]])
            #
            #         if tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg_rnd.values()):
            #             above_avg_rnd+=1
            #         elif tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg_rnd.values()):
            #             less_avg_rnd+=1
            #
            #
            #         if tweet_abs_dev_avg[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg.values()):
            #             above_avg+=1
            #         elif tweet_abs_dev_avg[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg.values()):
            #             less_avg+=1
            #
            #     norm_abs_perc_bias_ll.append(np.mean(abs_perc_rnd_l))
            #     abs_perc_bias_ll.append(np.mean(perc_rnd_l))
            #
            #     # perc_rnd_acc = len(set(disputability_l).intersection(perc_rnd_l))/ float(len(perc_rnd_l))
            #     # abs_perc_rnd_acc = len(set(disputability_l).intersection(abs_perc_rnd_l))/ float(len(abs_perc_rnd_l))
            #     # print('---------- k = ' +str(k/float(len(tweet_disp_sort)))+ '-------------')
            #     # print(str(above_avg_rnd/float(above_avg_rnd+less_avg_rnd)))
            #     # print(str(above_avg/float(above_avg+less_avg)))
            #     # print(perc_rnd_acc)
            #     # print(abs_perc_rnd_acc)
            #     # print('------------------------------')
            #
            #     norm_abs_perc_bias_l.append(above_avg_rnd/float(above_avg_rnd+less_avg_rnd))
            #     abs_perc_bias_l.append(above_avg/float(above_avg+less_avg))
            #     k_l.append(np.round(k/float(len(tweet_disp_sort)), 3))
            #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_l ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_l, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their NAPB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_l ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_l, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their APB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_abs_perception'
            # # mplpl.savefig(pp, format='png')
            # #
            # # mplpl.figure()
            # #
            # #
            # #
            # #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_ll ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_ll, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg NAPB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_ll ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_ll, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg APB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-abs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.show()
            #
            #
            #
            #     ##################################################
            #
            # print(np.mean(tweet_abs_dev_avg_rnd.values()))
            # exit()


            # tweet_avg_group = collections.defaultdict()
            # tweet_med_group = collections.defaultdict()
            # tweet_var_group = collections.defaultdict()
            #
            # tweet_kldiv_group= collections.defaultdict()

            tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l=[]
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            popularity_l= []
            vote_l = []
            gr_disp_l = []
            for t_id in tweet_l_sort:
                gt_l.append(tweet_gt_var[t_id])
                pt_l.append(tweet_avg[t_id])
                disputability_l.append(tweet_var[t_id])
                perc_l.append(tweet_dev_avg[t_id])
                abs_perc_l.append(tweet_abs_dev_avg[t_id])
                # popularity_l.append(tweet_popularity[t_id])
                vote_l.append(tweet_vote_avg[t_id])
                # vote_l.append(tweet_vote_var[t_id])
                perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
                abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
                tweet_skew_ll.append(tweet_skew[t_id])
                gr_disp_l.append(tweet_skew[t_id])
            value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l, vote_l,gr_disp_l]#,popularity_l,tweet_skew_ll]
            value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
                          'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'vote', 'group_disputability']#,'popularity' 'skewness']

            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(9):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(9):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')


            exit()
            tweet_group_gt = collections.defaultdict(list)
            gt_set = sorted(set(gt_l))
            for gt_e in gt_set:
                for t_id in tweet_l_sort:
                    if tweet_gt_var[t_id]==gt_e:
                        tweet_group_gt[gt_e].append(t_id)

            pt_mean = []
            disp_mean = []
            perc_mean = []
            abs_perc_mean = []
            for gt_e in gt_set:
                t_id_l = tweet_group_gt[gt_e]
                pt_mean.append(np.mean([tweet_avg[x] for x in t_id_l]))
                disp_mean.append(np.mean([tweet_var[x] for x in t_id_l]))
                perc_mean.append(np.mean([tweet_dev_avg[x] for x in t_id_l]))
                abs_perc_mean.append(np.mean([tweet_abs_dev_avg[x] for x in t_id_l]))

            # outF = open(remotedir + 'table_out.txt', 'w')

            # outF.write('== ' + data_name + ' ==\n')
            font = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 16,
                    }

            font_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 16,
                    }


            font_t = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 12,
                    }

            font_t_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 12,
                    }



            # fig_f = True
            fig_f = False
            if fig_f==True:

                mplpl.scatter(gt_l, pt_l,  s=40,color='r',marker='o', label='All users')
                mplpl.scatter(gt_set, pt_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]

                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_avg[x]>0:
                            num_pos+=1
                        elif tweet_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e , 0.85, str(num_pos) , fontdict=font)
                    mplpl.text(gt_e , 0.75, str(num_neg) , fontdict=font_1)

                mplpl.text(-0.3, -0.75, ' # news has positive perception truth value', fontdict=font_t)
                mplpl.text(-0.3, -0.9,   '# news has negative perception truth value', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_pt| alt text| width = 500px}}')


                mplpl.scatter(gt_l,disputability_l ,  s=40,color='g',marker='o', label='All users')
                mplpl.scatter(gt_set, disp_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, disp_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                # for gt_e in gt_set:
                #     t_id_l = tweet_group_gt[gt_e]

                #     num_pos=0
                #     num_neg = 0
                #
                #     for x in t_id_l:
                #         if tweet_var[x]>0:
                #             num_pos+=1
                #         elif tweet_var[x]<0:
                #             num_neg+=1
                #     # mplpl.text(gt_e-0.2, 0.85, str(num_pos) + r'$ > 0$', fontdict=font)
                #     # mplpl.text(gt_e-0.2, 0.75, str(num_neg) + r'$ < 0$', fontdict=font_1)

                #
                # mplpl.text(0, 0.9, ' # news perceived true(postitive)', fontdict=font_t)
                # mplpl.text(0, 0.75,   '# news perceived false(negative)', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_disputability'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_disputability| alt text| width = 500px}}')


                gt_l_dict[data_name] = gt_l
                perc_l_dict[data_name] = perc_l
                gt_set_dict[data_name] = gt_set
                perc_mean_dict[data_name] = perc_mean

                mplpl.scatter(gt_l, perc_l,  s=40,color='b',marker='o', label='All users')
                mplpl.scatter(gt_set, perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, perc_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]
                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_dev_avg[x]>0:
                            num_pos+=1
                        elif tweet_dev_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e, 1.65, str(num_pos), fontdict=font)
                    mplpl.text(gt_e, 1.3, str(num_neg), fontdict=font_1)

                mplpl.text(-1, -1, '# news that has positive perception bias value', fontdict=font_t)
                mplpl.text(-1, -1.5, '# news that has nigative perception bias value', fontdict=font_t_1)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_perception| alt text| width = 500px}}')


                mplpl.scatter(gt_l,abs_perc_l ,  s=40,color='c',marker='o', label='All users')
                mplpl.scatter(gt_set, abs_perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, abs_perc_mean, color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)


                abs_perc_l_dict[data_name] = abs_perc_l
                abs_perc_mean_dict[data_name] = abs_perc_mean


                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_abs_perception| alt text| width = 500px}} ||\n')

                # outF.write('|| Table ||\n\n')

            # exit
        ########################

                tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)
                pt_l = []
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_avg[t_id])

                mplpl.scatter(range(len(pt_l)), pt_l,  s=40,color='r',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PTL', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_pt| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=False)
                disputability_l = []
                for t_id in tweet_l_sort:
                    disputability_l.append(tweet_var[t_id])


                mplpl.scatter(range(len(pt_l)), disputability_l ,  s=40,color='g',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ranked news stories according Disputability', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_disput'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_disput| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=False)
                perc_l = []
                for t_id in tweet_l_sort:
                    perc_l.append(tweet_dev_avg[t_id])

                mplpl.scatter(range(len(pt_l)), perc_l,  s=40,color='b',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias (PB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_percep| alt text| width = 500px}}')



                tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=False)
                perc_abs_l = []
                for t_id in tweet_l_sort:
                    perc_abs_l.append(tweet_abs_dev_avg[t_id])

                mplpl.scatter(range(len(perc_abs_l)), perc_abs_l,  s=40,color='c',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according APB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_abs_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_abs-percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_abs-percep| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



        #####################################################33


                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='r', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')


                num_bins = len(disputability_l)
                counts, bin_edges = np.histogram(disputability_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Disputability', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_cdf| alt text| width = 500px}}')


                num_bins = len(perc_l)
                counts, bin_edges = np.histogram(perc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='b', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception bias (PB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_cdf| alt text| width = 500px}}')

                num_bins = len(perc_abs_l)
                counts, bin_edges = np.histogram(perc_abs_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_cdf| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



                # mplpl.show()

                col_l = ['r', 'b', 'g']

                i = 0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=400, color=col_l[i], marker='*')
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i = 0
                mark_l = ['*', 'o', '^']
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i=0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o')
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=300, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                i=0
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()











                mplpl.show()

            else:

                AVG_list = []
                print(np.mean(all_acc))
                outF = open(remotedir + 'output.txt', 'w')

                tweet_all_var = {}
                tweet_all_dev_avg = {}
                tweet_all_avg = {}
                tweet_all_gt_var = {}
                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []
                tweet_all_avg_l = []
                tweet_all_med_l = []
                tweet_all_var_l = []
                tweet_all_gt_var_l = []
                diff_group_disp_l = []
                dem_disp_l = []
                rep_disp_l = []

                tweet_all_dev_avg = {}
                tweet_all_dev_med = {}
                tweet_all_dev_var = {}

                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []

                tweet_all_abs_dev_avg = {}
                tweet_all_abs_dev_med = {}
                tweet_all_abs_dev_var = {}

                tweet_all_abs_dev_avg_l = []
                tweet_all_abs_dev_med_l = []
                tweet_all_abs_dev_var_l = []
                tweet_all_dev_avg_rnd = {}
                tweet_all_abs_dev_avg_rnd = {}

                diff_group_disp_dict = {}
                if dataset == 'snopes':
                    data_n = 'sp'
                    news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                    ind_l = [1, 2, 3]
                elif dataset == 'politifact':
                    data_n = 'pf'
                    news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                    ind_l = [1, 2, 3]
                elif dataset == 'mia':
                    data_n = 'mia'
                    news_cat_list = ['rumor', 'non-rumor']
                    ind_l = [1]

                for cat_l in news_cat_list:
                    outF.write('== ' + str(cat_l) + ' ==\n\n')
                    print('== ' + str(cat_l) + ' ==')
                    tweet_dev_avg = {}
                    tweet_dev_med = {}
                    tweet_dev_var = {}
                    tweet_abs_dev_avg = {}
                    tweet_abs_dev_med = {}
                    tweet_abs_dev_var = {}

                    tweet_avg = {}
                    tweet_med = {}
                    tweet_var = {}
                    tweet_gt_var = {}

                    tweet_dev_avg_rnd = {}
                    tweet_abs_dev_avg_rnd = {}


                    tweet_dev_avg_l = []
                    tweet_dev_med_l = []
                    tweet_dev_var_l = []
                    tweet_abs_dev_avg_l = []
                    tweet_abs_dev_med_l = []
                    tweet_abs_dev_var_l = []

                    tweet_avg_l = []
                    tweet_med_l = []
                    tweet_var_l = []
                    tweet_gt_var_l = []
                    AVG_susc_list = []
                    AVG_wl_list = []
                    all_acc = []
                    AVG_dev_list = []
                    # for lean in [-1, 0, 1]:

                        # AVG_susc_list = []
                        # AVG_wl_list = []
                        # all_acc = []
                        # df_m = df_m[df_m['leaning'] == lean]
                        # if lean == 0:
                        #     col = 'g'
                        #     lean_cat = 'neutral'
                        # elif lean == 1:
                        #     col = 'b'
                        #     lean_cat = 'democrat'
                        # elif lean == -1:
                        #     col = 'r'
                        #     lean_cat = 'republican'
                        # print(lean_cat)
                    for ind in ind_l:

                        if balance_f == 'balanced':
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
                        else:
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'

                        inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
                        df[ind] = pd.read_csv(inp1, sep="\t")
                        df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                        df_m = df[ind].copy()
                        df_mm = df_m.copy()

                        df_m = df_m[df_m['ra_gt'] == cat_l]
                        # df_mm = df_m[df_m['ra_gt']==cat_l]
                        # df_m = df_m[df_m['leaning'] == lean]

                        groupby_ftr = 'tweet_id'
                        grouped = df_m.groupby(groupby_ftr, sort=False)
                        grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                        for t_id in grouped.groups.keys():
                            df_tmp = df_m[df_m['tweet_id'] == t_id]

                            df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
                            df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
                            df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []
                            df_tmp = df_m[df_m['tweet_id'] == t_id]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []

                            weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                            val_list = list(df_tmp['rel_v'])
                            tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_avg[t_id] = np.mean(val_list)
                            tweet_med[t_id] = np.median(val_list)
                            tweet_var[t_id] = np.var(val_list)
                            tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_avg_l.append(np.mean(val_list))
                            tweet_med_l.append(np.median(val_list))
                            tweet_var_l.append(np.var(val_list))
                            tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])




                            tweet_all_avg[t_id] = np.mean(val_list)
                            tweet_all_var[t_id] = np.var(val_list)
                            tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_all_avg_l.append(np.mean(val_list))
                            tweet_all_med_l.append(np.median(val_list))
                            tweet_all_var_l.append(np.var(val_list))
                            tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])



                            val_list = list(df_tmp['err'])
                            abs_var_err = [np.abs(x) for x in val_list]
                            tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_dev_avg[t_id] = np.mean(val_list)
                            tweet_dev_med[t_id] = np.median(val_list)
                            tweet_dev_var[t_id] = np.var(val_list)

                            tweet_dev_avg_l.append(np.mean(val_list))
                            tweet_dev_med_l.append(np.median(val_list))
                            tweet_dev_var_l.append(np.var(val_list))

                            tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_abs_dev_var_l.append(np.var(abs_var_err))


                            tweet_all_dev_avg[t_id] = np.mean(val_list)
                            tweet_all_dev_med[t_id] = np.median(val_list)
                            tweet_all_dev_var[t_id] = np.var(val_list)

                            tweet_all_dev_avg_l.append(np.mean(val_list))
                            tweet_all_dev_med_l.append(np.median(val_list))
                            tweet_all_dev_var_l.append(np.var(val_list))

                            tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_all_abs_dev_var_l.append(np.var(abs_var_err))



                            sum_rnd_abs_perc = 0
                            sum_rnd_perc = 0
                            for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                                sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                                sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                            random_perc = np.abs(sum_rnd_perc / float(7))
                            random_abs_perc = sum_rnd_abs_perc / float(7)

                            tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                            tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                    gt_l = []
                    pt_l = []
                    disputability_l = []
                    perc_l = []
                    abs_perc_l = []
                    # for t_id in tweet_l_sort:
                    #     gt_l.append(tweet_gt_var[t_id])
                    #     pt_l.append(tweet_avg[t_id])
                    #     disputability_l.append(tweet_var[t_id])
                    #     perc_l.append(tweet_dev_avg[t_id])
                    #     abs_perc_l.append(tweet_abs_dev_avg[t_id])



                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
                    tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)

                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)


                    if dataset == 'snopes':
                        data_addr = 'snopes'
                    elif dataset == 'politifact':
                        data_addr = 'politifact/fig'
                    elif dataset == 'mia':
                        data_addr = 'mia/fig'

                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')

                    for t_id in tweet_l_sort:
                        count+=1
                        if balance_f=='balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
                                       + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
                                       # str(np.round(diff_group_disp_dict[t_id], 3)) +
                                       '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
                                        + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
                                        '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')




                if dataset == 'snopes':
                    data_addr = 'snopes'
                elif dataset == 'politifact':
                    data_addr = 'politifact/fig'
                elif dataset == 'mia':
                    data_addr = 'mia/fig'

                # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
                tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)

                # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)

                tweet_napb_dict_high_disp = {}
                tweet_napb_dict_low_disp = {}
                for t_id in tweet_l_sort[:20]:
                    # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                for t_id in tweet_l_sort[-20:]:
                    # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                kk = 0

                for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
                    if kk==0:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
                    else:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)

                    kk+=1
                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    for t_id in tweet_l_sort:
                        count += 1
                        # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
                        if balance_f == 'balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
                                       str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +
                            #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
                            #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                                       str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                        # +
                        # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                        # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_true-false(gt-pt)_news_ktop_nptl_scatter_fig1":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        # dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'
        dataset = 'snopes_nonpol'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes_nonpol':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            tweet_topic_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims_1.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                topic = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_topic_dict[tweet_id] = topic

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes_nonpol','snopes_nonpol','mia','politifact','snopes','mia','politifact']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                tweet_topic_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims_1.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    topic = line_splt[5]
                    tweet_topic_dict[tweet_id] = topic
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            if dataset=='snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpol'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()
            tweet_var_diff_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            tweet_chi_group = {}
            tweet_chi_group_1 = {}
            tweet_chi_group_2 = {}
            tweet_skew = {}
            news_cat_list_tf = [4,2,3,1]
            t_f_dict_len = collections.defaultdict(int)
            t_f_dict = {}
            if dataset=='snopes' or dataset=='snopes_nonpol':
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
            if dataset=='politifact':
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]

            if dataset=='mia':
                news_cat_list_t_f = [['rumor'], ['non-rumor']]

            w_fnb_dict= collections.defaultdict()
            w_fpb_dict= collections.defaultdict()
            w_apb_dict = collections.defaultdict()
            tweet_avg_dem = collections.defaultdict()
            tweet_avg_rep = collections.defaultdict()
            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                    # inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_weighted.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    if t_id == 1367:
                        continue
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    if t_id== 1116:
                        print(df_m['text'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])
                    # vot_list_tmp = []



                    w_sus_list = list(df_tmp['susc'])
                    # w_norm_err_list = list(df_tmp['norm_err'])
                    # w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    # w_cyn_list = list(df_tmp['cyn'])
                    # w_gull_list = list(df_tmp['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])


                    df_cyn = df_tmp[df_tmp['cyn']>0]
                    df_gull = df_tmp[df_tmp['gull']>0]

                    w_cyn_list_t = list(df_cyn['cyn'])
                    w_gull_list_t = list(df_gull['gull'])

                    w_cyn_list = []
                    w_gull_list = []

                    for tt in w_cyn_list_t:
                        if tt>0 or tt<=0:
                            w_cyn_list.append(tt)


                    for tt in w_gull_list_t:
                        if tt>0 or tt<=0:
                            w_gull_list.append(tt)
                    w_fnb_dict[t_id] = np.mean(w_cyn_list)
                    w_fpb_dict[t_id] = np.mean(w_gull_list)
                    w_apb_dict[t_id] = np.mean(w_sus_list)



                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))
                    # ['FALSE-FALSE', 'FALSE-TRUE', 'TRUE-FALSE', 'TRUE-TRUE']

                    if tweet_lable_dict[t_id] in news_cat_list_t_f[0] and np.mean(df_tmp['rel_v']) < 0:
                        t_f_dict[t_id] = 4
                        t_f_dict_len[4]+=1
                    elif tweet_lable_dict[t_id] in news_cat_list_t_f[0] and np.mean(df_tmp['rel_v']) > 0:
                        t_f_dict[t_id] = 2
                        t_f_dict_len[2]+=1
                    elif tweet_lable_dict[t_id] in news_cat_list_t_f[1] and np.mean(df_tmp['rel_v']) > 0:
                        t_f_dict[t_id] = 1
                        t_f_dict_len[1]+=1
                    elif tweet_lable_dict[t_id] in news_cat_list_t_f[1] and np.mean(df_tmp['rel_v']) < 0:
                        t_f_dict[t_id] = 3
                        t_f_dict_len[3]+=1
                    else:
                        t_f_dict[t_id] = -10
                        t_f_dict_len[-10] += 1

                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    val_list = list(df_tmp['rel_v'])



                    tweet_avg_dem[t_id] = np.mean(dem_val_list)
                    tweet_avg_rep[t_id] = np.mean(rep_val_list)

                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_diff_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) + np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)



                    # tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew[t_id] = scipy.stats.skew(dem_val_list) + scipy.stats.skew(rep_val_list)
                    # tweet_skew_l.append(tweet_skew[t_id])

                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])




                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    # tweet_skew[t_id] = scipy.stats.skew(val_list)
                    # tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)




            ##################################################
            gt_fpb = []
            pt_fpb = []
            gt_fnb = []
            pt_fnb = []
            gt_pt = []
            pt_pt = []
            len_cat_dict = {}
            # if dataset=='snopes' or dataset=='politifact':
            for cat in news_cat_list_tf:
                len_cat_dict[cat]=t_f_dict_len[cat]
            # elif dataset=='mia':
            #     for cat in news_cat_list_tf:
            #         if cat=='rumor':
            #             len_cat_dict[cat]=t_f_dict_len[cat]
            #         else:
            #             len_cat_dict[cat] = t_f_dict_len[cat]
            tweet_vote_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)
            tweet_pick =tweet_vote_sort[:2]
            for t_id in tweet_pick:
                print(t_id)
                gt_pt.append(tweet_gt_var[t_id])
                pt_pt.append(tweet_avg[t_id])
            # tweet_vote_sort = sorted(tweet_vote_avg, key=tweet_vote_avg.get, reverse=False)

            tweet_apb_sort = sorted(w_apb_dict, key=w_apb_dict.get, reverse=True)
            tweet_fpb_sort = sorted(w_fpb_dict, key=w_fpb_dict.get, reverse=True)
            tweet_fnb_sort = sorted(w_fnb_dict, key=w_fnb_dict.get, reverse=True)
            tweet_ideological_mpb_sort = sorted(tweet_avg_group, key=tweet_avg_group.get, reverse=True)
            # tweet_vote_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_vote_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            top_pic_des_FPB = tweet_fpb_sort[:1]
            for t_id in top_pic_des_FPB:
                print(t_id)
                gt_fpb.append(tweet_gt_var[t_id])
                pt_fpb.append(tweet_avg[t_id])

            top_pic_des_FNB = tweet_fnb_sort[:1]
            for t_id in top_pic_des_FNB:
                print(t_id)
                gt_fnb.append(tweet_gt_var[t_id])
                pt_fnb.append(tweet_avg[t_id])

            cc=0

            tpb_all = collections.defaultdict(int)
            for t_id in tweet_apb_sort:
                cc+=1
                tpb_all[tweet_topic_dict[t_id]]+=1


            top_tpb = collections.defaultdict(float)
            for t_id in tweet_apb_sort[:25]:
                cc+=1
                top_tpb[tweet_topic_dict[t_id]]+=1
            # + '_' + str(mid_tpb[key])
            print('----top ----')
            for key in top_tpb:
                print(str(int(100*(top_tpb[key]/float(tpb_all[key])))) + '\t' + key+ '_'+str(int(100*(top_tpb[key]/float(tpb_all[key]))))+'%')


            bot_tpb = collections.defaultdict(float)
            for t_id in tweet_apb_sort[75:]:
                cc+=1
                bot_tpb[tweet_topic_dict[t_id]]+=1

            print('----bottomn ----')
            for key in bot_tpb:
                print(str(int(100*(bot_tpb[key]/float(tpb_all[key]))))+ '\t' + key+ '_'+str(int(100*(bot_tpb[key]/float(tpb_all[key]))))+'%')


            mid_tpb = collections.defaultdict(float)
            for t_id in tweet_apb_sort[25:75]:
                cc+=1
                mid_tpb[tweet_topic_dict[t_id]]+=1


            print('----mid ----')
            for key in mid_tpb:
                print(str(int(100*(mid_tpb[key]/float(tpb_all[key]))))+ '\t' + key + '_'+str(int(100*(mid_tpb[key]/float(tpb_all[key]))))+'%')



            # exit()
            cc=0
            for t_id in tweet_ideological_mpb_sort:
                cc+=1
                print('|| ' + str(cc) + ' || ' +str(t_id) + ' || ' + tweet_txt_dict[t_id] + ' || ' + tweet_lable_dict[t_id]
                      + ' || ' + tweet_topic_dict[t_id] + ' || '+  str(tweet_avg_group[t_id]) +' || ' + str(tweet_avg_dem[t_id])+
                      ' || ' + str(tweet_avg_rep[t_id]) + ' || '+ str(w_apb_dict[t_id]) + ' || ' + str(tweet_avg[t_id]) + '||')
            exit()

            thr = 10
            thr_list = []
            categ_dict = collections.defaultdict(int)
            categ_dict_n = collections.defaultdict(int)
            len_t = len(tweet_vote_sort)
            k_list = [int(0.1*len_t), int(0.2*len_t), int(0.3*len_t), int(0.4*len_t), int(0.5*len_t), int(1*len_t) ]
            count = 0
            for k in k_list:
                thr_list.append(k)
                perc_rnd_l = []
                abs_perc_rnd_l = []
                disputability_l = []
                above_avg = 0
                less_avg = 0
                above_avg_rnd = 0
                less_avg_rnd = 0
                above_avg = 0
                less_avg = 0
                categ_dict[k] = collections.defaultdict(float)
                categ_dict_n[k] = collections.defaultdict(list)
                for j in range(k):
                    for cat_n in news_cat_list_tf:
                        if cat_n==-10:
                            continue
                        if t_f_dict[tweet_vote_sort[j]] == cat_n:
                            categ_dict[k][cat_n]+=1/float(len_cat_dict[cat_n])

                            categ_dict_n[k][cat_n].append(tweet_vote_sort[j])
            # if dataset=='mia':
            total_data = collections.defaultdict(int)
            for j in categ_dict:
                sum = np.sum(categ_dict[j].values())
                # for cat_n in categ_dict[j]:
                for cat_n in [4, 2, 3, 1]:
                    categ_dict[j][cat_n] =  categ_dict[j][cat_n] / sum

                    total_data[j] += len(categ_dict_n[k][cat_n])


            width = 0.03
            pr = -10
            title_l = news_cat_list_tf
            outp = {}
            # news_cat_list = ['pants-fire', 'false', 'mostly_false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            if dataset=='snopes':
                # col_l = ['b', 'g', 'c', 'y', 'r']
                col_l = ['red', 'magenta', 'lime', 'green']
                marker_l=['s', '*', 'o', 'd']
                news_cat_list_n = ['GT:FALSE, PT:FALSE', 'GT:FALSE, PT:TRUE', 'GT:TRUE, PT:FALSE', 'GT:TRUE, PT:TRUE']
            if dataset=='politifact':
                # col_l = ['grey','b', 'g', 'c', 'y', 'r']
                col_l = ['red', 'magenta', 'lime', 'green']

                news_cat_list_n = ['FALSE-FALSE', 'FALSE-TRUE', 'TRUE-FALSE', 'TRUE-TRUE']

            if dataset=='mia':
                # col_l = ['b', 'r']
                col_l = ['red', 'magenta', 'lime', 'green']
                news_cat_list_n = ['FALSE-FALSE', 'FALSE-TRUE', 'TRUE-FALSE', 'TRUE-TRUE']
            count = 0
            Y = [0]*len(thr_list)
            mplpl.rcParams['figure.figsize'] = 4.8, 4
            mplpl.rc('xtick', labelsize='large')
            mplpl.rc('ytick', labelsize='large')
            mplpl.rc('legend', fontsize='small')
            # for cat_m in news_cat_list_tf:
            #     count+=1
            #     outp[cat_m] = []
            #     for i in thr_list:
            #         outp[cat_m].append(categ_dict[i][cat_m])
            #     mplpl.bar([0.1, 0.2, 0.3, 0.4,0.5,0.6], outp[cat_m], width, bottom= np.array(Y), color=col_l[count-1], label=news_cat_list_n[count-1])
            #     Y = np.array(Y) + np.array(outp[cat_m])
            gt_l_t = []
            pt_l_t = []
            for i in [int(1*len_t)]:
                tweet_id_list = tweet_vote_sort
                for t_id in tweet_id_list:
                    gt_l_t.append(tweet_gt_var[t_id])
                    pt_l_t.append(tweet_avg[t_id])

            #
            # for cat_m in news_cat_list_tf:
            #     count+=1
            #     outp[cat_m] = []
            #     gt_l_t = []
            #     pt_l_t = []
            #     for i in [int(1*len_t)]:
            #         tweet_id_list = categ_dict_n[i][cat_m]
            #         for t_id in tweet_id_list:
            #             gt_l_t.append(tweet_gt_var[t_id])
            #             pt_l_t.append(tweet_avg[t_id])
            #
            #     m_label = str(np.round(len(tweet_id_list) / float(total_data[int(1 * len_t)]),2))

            # mplpl.scatter(gt_l_t, pt_l_t,c=col_l[count-1],marker=marker_l[count-1], s=60, label = m_label)
            # mplpl.scatter(pt_l_t,gt_l_t, c=col_l[count - 1], marker=marker_l[count - 1], s=60, label=m_label)


            mplpl.scatter(gt_fnb,pt_fnb, c='r', marker='s', s=200)#, label=m_label)
            mplpl.scatter(gt_fpb,pt_fpb, c='r', marker='d', s=200)#, label=m_label)
            mplpl.scatter(gt_pt,pt_pt, c='orange', marker='^', s=200)#, label=m_label)
            mplpl.scatter(gt_l_t,pt_l_t, c='g', marker='o', s=60)#, label=m_label)


            mplpl.plot([-1.1, 1.1], [-1.1,1.1],c='k',linewidth=4)
            # mplpl.plot([0,0],[-1.1, 1.1],c='k',linewidth=4)

            mplpl.xlim([-1.04, 1.04])
            mplpl.ylim([-1.04, 1.04])

            font = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 16,
                    }

            font_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 16,
                    }


            font_t = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'bold',
                    'size': 12,
                    }

            font_t_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 12,
                    }
            # mplpl.text(-0.7, 0.85, ' Mostly deserve to pick', fontdict=font_t)
            # # mplpl.text(-0.3, -0.9, '# news has negative perception truth value', fontdict=font_t_1)
            # mplpl.annotate('', xy=(-1, 0.38), xytext=(0, 0.82),
            #             arrowprops=dict(facecolor='r', shrink=0.1))
            #
            # mplpl.annotate('', xy=(1, -0.25), xytext=(0, 0.82),
            #             arrowprops=dict(facecolor='r', shrink=0.1))

            mplpl.subplots_adjust(bottom=0.32)

            mplpl.subplots_adjust(left=0.2)
            mplpl.title(data_name)
            labels = ['-1\nFalse', '-.05\nMostly\nFalse', '0\nMixture', '0.5\nMostly\n True', '1\nTrue']
            x = [ -1, -.5, 0, 0.5, 1]
            mplpl.xticks(x, labels)

            # mplpl.ylabel('Composition of news stories \n in different quadrants', fontsize=14,fontweight = 'bold')
            mplpl.xlabel('Ground Truth Level', fontsize=18,fontweight = 'bold')
            mplpl.ylabel('Perceived Truth Level', fontsize=18,fontweight = 'bold')
            # mplpl.xlabel('Top k news stories ranked by NAPB', fontsize=18)

            # mplpl.legend(loc="center", ncol=2, fontsize='small')



            mplpl.grid()
            pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_gt_pt_compos_true-false_scatter_1'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_gt_pt_compos_true-false_scatter_1_weighted'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_napb_compos_true-false_scatter'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_vote_compos_true-false_scatter'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_disp_compos_true-false_scatter'
            mplpl.savefig(pp + '.pdf', format='pdf')
            mplpl.savefig(pp+ '.png', format='png')

            mplpl.figure()


            exit()





























            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l = []
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            thr = 5
            prec_rnd_acc = 0
            abs_prec_rnd_acc = 0
            norm_abs_perc_bias_l = []
            abs_perc_bias_l = []
            norm_abs_perc_bias_ll = []
            abs_perc_bias_ll = []
            k_l = []
            # for i in range(int(len(tweet_disp_sort)/float(thr))):
            #     k = (i+1)*thr
            #     perc_rnd_l = []
            #     abs_perc_rnd_l = []
            #     disputability_l = []
            #     above_avg = 0
            #     less_avg = 0
            #     above_avg_rnd = 0
            #     less_avg_rnd = 0
            #     above_avg = 0
            #     less_avg = 0
            #     for j in range(k):
            #
            #         perc_rnd_l.append(tweet_abs_dev_avg[tweet_disp_sort[j]])
            #         abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[tweet_disp_sort[j]])
            #
            #         if tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg_rnd.values()):
            #             above_avg_rnd+=1
            #         elif tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg_rnd.values()):
            #             less_avg_rnd+=1
            #
            #
            #         if tweet_abs_dev_avg[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg.values()):
            #             above_avg+=1
            #         elif tweet_abs_dev_avg[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg.values()):
            #             less_avg+=1
            #
            #     norm_abs_perc_bias_ll.append(np.mean(abs_perc_rnd_l))
            #     abs_perc_bias_ll.append(np.mean(perc_rnd_l))
            #
            #     # perc_rnd_acc = len(set(disputability_l).intersection(perc_rnd_l))/ float(len(perc_rnd_l))
            #     # abs_perc_rnd_acc = len(set(disputability_l).intersection(abs_perc_rnd_l))/ float(len(abs_perc_rnd_l))
            #     # print('---------- k = ' +str(k/float(len(tweet_disp_sort)))+ '-------------')
            #     # print(str(above_avg_rnd/float(above_avg_rnd+less_avg_rnd)))
            #     # print(str(above_avg/float(above_avg+less_avg)))
            #     # print(perc_rnd_acc)
            #     # print(abs_perc_rnd_acc)
            #     # print('------------------------------')
            #
            #     norm_abs_perc_bias_l.append(above_avg_rnd/float(above_avg_rnd+less_avg_rnd))
            #     abs_perc_bias_l.append(above_avg/float(above_avg+less_avg))
            #     k_l.append(np.round(k/float(len(tweet_disp_sort)), 3))
            #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_l ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_l, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their NAPB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_l ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_l, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their APB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_abs_perception'
            # # mplpl.savefig(pp, format='png')
            # #
            # # mplpl.figure()
            # #
            # #
            # #
            # #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_ll ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_ll, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg NAPB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_ll ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_ll, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg APB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-abs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.show()
            #
            #
            #
            #     ##################################################
            #
            # print(np.mean(tweet_abs_dev_avg_rnd.values()))
            # exit()


            # tweet_avg_group = collections.defaultdict()
            # tweet_med_group = collections.defaultdict()
            # tweet_var_group = collections.defaultdict()
            #
            # tweet_kldiv_group= collections.defaultdict()

            tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l=[]
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            popularity_l= []
            vote_l = []
            gr_disp_l = []
            for t_id in tweet_l_sort:
                gt_l.append(tweet_gt_var[t_id])
                pt_l.append(tweet_avg[t_id])
                disputability_l.append(tweet_var[t_id])
                perc_l.append(tweet_dev_avg[t_id])
                abs_perc_l.append(tweet_abs_dev_avg[t_id])
                # popularity_l.append(tweet_popularity[t_id])
                vote_l.append(tweet_vote_avg[t_id])
                # vote_l.append(tweet_vote_var[t_id])
                perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
                abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
                tweet_skew_ll.append(tweet_skew[t_id])
                gr_disp_l.append(tweet_skew[t_id])
            value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l, vote_l,gr_disp_l]#,popularity_l,tweet_skew_ll]
            value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
                          'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'vote', 'group_disputability']#,'popularity' 'skewness']

            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(9):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(9):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')


            exit()
            tweet_group_gt = collections.defaultdict(list)
            gt_set = sorted(set(gt_l))
            for gt_e in gt_set:
                for t_id in tweet_l_sort:
                    if tweet_gt_var[t_id]==gt_e:
                        tweet_group_gt[gt_e].append(t_id)

            pt_mean = []
            disp_mean = []
            perc_mean = []
            abs_perc_mean = []
            for gt_e in gt_set:
                t_id_l = tweet_group_gt[gt_e]
                pt_mean.append(np.mean([tweet_avg[x] for x in t_id_l]))
                disp_mean.append(np.mean([tweet_var[x] for x in t_id_l]))
                perc_mean.append(np.mean([tweet_dev_avg[x] for x in t_id_l]))
                abs_perc_mean.append(np.mean([tweet_abs_dev_avg[x] for x in t_id_l]))

            # outF = open(remotedir + 'table_out.txt', 'w')

            # outF.write('== ' + data_name + ' ==\n')
            font = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 16,
                    }

            font_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 16,
                    }


            font_t = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 12,
                    }

            font_t_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 12,
                    }



            # fig_f = True
            fig_f = False
            if fig_f==True:

                mplpl.scatter(gt_l, pt_l,  s=40,color='r',marker='o', label='All users')
                mplpl.scatter(gt_set, pt_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]

                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_avg[x]>0:
                            num_pos+=1
                        elif tweet_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e , 0.85, str(num_pos) , fontdict=font)
                    mplpl.text(gt_e , 0.75, str(num_neg) , fontdict=font_1)

                mplpl.text(-0.3, -0.75, ' # news has positive perception truth value', fontdict=font_t)
                mplpl.text(-0.3, -0.9,   '# news has negative perception truth value', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_pt| alt text| width = 500px}}')


                mplpl.scatter(gt_l,disputability_l ,  s=40,color='g',marker='o', label='All users')
                mplpl.scatter(gt_set, disp_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, disp_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                # for gt_e in gt_set:
                #     t_id_l = tweet_group_gt[gt_e]

                #     num_pos=0
                #     num_neg = 0
                #
                #     for x in t_id_l:
                #         if tweet_var[x]>0:
                #             num_pos+=1
                #         elif tweet_var[x]<0:
                #             num_neg+=1
                #     # mplpl.text(gt_e-0.2, 0.85, str(num_pos) + r'$ > 0$', fontdict=font)
                #     # mplpl.text(gt_e-0.2, 0.75, str(num_neg) + r'$ < 0$', fontdict=font_1)

                #
                # mplpl.text(0, 0.9, ' # news perceived true(postitive)', fontdict=font_t)
                # mplpl.text(0, 0.75,   '# news perceived false(negative)', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_disputability'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_disputability| alt text| width = 500px}}')


                gt_l_dict[data_name] = gt_l
                perc_l_dict[data_name] = perc_l
                gt_set_dict[data_name] = gt_set
                perc_mean_dict[data_name] = perc_mean

                mplpl.scatter(gt_l, perc_l,  s=40,color='b',marker='o', label='All users')
                mplpl.scatter(gt_set, perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, perc_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]
                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_dev_avg[x]>0:
                            num_pos+=1
                        elif tweet_dev_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e, 1.65, str(num_pos), fontdict=font)
                    mplpl.text(gt_e, 1.3, str(num_neg), fontdict=font_1)

                mplpl.text(-1, -1, '# news that has positive perception bias value', fontdict=font_t)
                mplpl.text(-1, -1.5, '# news that has nigative perception bias value', fontdict=font_t_1)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_perception| alt text| width = 500px}}')


                mplpl.scatter(gt_l,abs_perc_l ,  s=40,color='c',marker='o', label='All users')
                mplpl.scatter(gt_set, abs_perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, abs_perc_mean, color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)


                abs_perc_l_dict[data_name] = abs_perc_l
                abs_perc_mean_dict[data_name] = abs_perc_mean


                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_abs_perception| alt text| width = 500px}} ||\n')

                # outF.write('|| Table ||\n\n')

            # exit
        ########################

                tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)
                pt_l = []
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_avg[t_id])

                mplpl.scatter(range(len(pt_l)), pt_l,  s=40,color='r',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PTL', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_pt| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=False)
                disputability_l = []
                for t_id in tweet_l_sort:
                    disputability_l.append(tweet_var[t_id])


                mplpl.scatter(range(len(pt_l)), disputability_l ,  s=40,color='g',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ranked news stories according Disputability', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_disput'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_disput| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=False)
                perc_l = []
                for t_id in tweet_l_sort:
                    perc_l.append(tweet_dev_avg[t_id])

                mplpl.scatter(range(len(pt_l)), perc_l,  s=40,color='b',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias (PB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_percep| alt text| width = 500px}}')



                tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=False)
                perc_abs_l = []
                for t_id in tweet_l_sort:
                    perc_abs_l.append(tweet_abs_dev_avg[t_id])

                mplpl.scatter(range(len(perc_abs_l)), perc_abs_l,  s=40,color='c',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according APB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_abs_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_abs-percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_abs-percep| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



        #####################################################33


                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='r', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')


                num_bins = len(disputability_l)
                counts, bin_edges = np.histogram(disputability_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Disputability', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_cdf| alt text| width = 500px}}')


                num_bins = len(perc_l)
                counts, bin_edges = np.histogram(perc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='b', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception bias (PB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_cdf| alt text| width = 500px}}')

                num_bins = len(perc_abs_l)
                counts, bin_edges = np.histogram(perc_abs_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_cdf| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



                # mplpl.show()

                col_l = ['r', 'b', 'g']

                i = 0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=400, color=col_l[i], marker='*')
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i = 0
                mark_l = ['*', 'o', '^']
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i=0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o')
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=300, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                i=0
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()











                mplpl.show()

            else:

                AVG_list = []
                print(np.mean(all_acc))
                outF = open(remotedir + 'output.txt', 'w')

                tweet_all_var = {}
                tweet_all_dev_avg = {}
                tweet_all_avg = {}
                tweet_all_gt_var = {}
                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []
                tweet_all_avg_l = []
                tweet_all_med_l = []
                tweet_all_var_l = []
                tweet_all_gt_var_l = []
                diff_group_disp_l = []
                dem_disp_l = []
                rep_disp_l = []

                tweet_all_dev_avg = {}
                tweet_all_dev_med = {}
                tweet_all_dev_var = {}

                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []

                tweet_all_abs_dev_avg = {}
                tweet_all_abs_dev_med = {}
                tweet_all_abs_dev_var = {}

                tweet_all_abs_dev_avg_l = []
                tweet_all_abs_dev_med_l = []
                tweet_all_abs_dev_var_l = []
                tweet_all_dev_avg_rnd = {}
                tweet_all_abs_dev_avg_rnd = {}

                diff_group_disp_dict = {}
                if dataset == 'snopes':
                    data_n = 'sp'
                    news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                    ind_l = [1, 2, 3]
                elif dataset == 'politifact':
                    data_n = 'pf'
                    news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                    ind_l = [1, 2, 3]
                elif dataset == 'mia':
                    data_n = 'mia'
                    news_cat_list = ['rumor', 'non-rumor']
                    ind_l = [1]

                for cat_l in news_cat_list:
                    outF.write('== ' + str(cat_l) + ' ==\n\n')
                    print('== ' + str(cat_l) + ' ==')
                    tweet_dev_avg = {}
                    tweet_dev_med = {}
                    tweet_dev_var = {}
                    tweet_abs_dev_avg = {}
                    tweet_abs_dev_med = {}
                    tweet_abs_dev_var = {}

                    tweet_avg = {}
                    tweet_med = {}
                    tweet_var = {}
                    tweet_gt_var = {}

                    tweet_dev_avg_rnd = {}
                    tweet_abs_dev_avg_rnd = {}


                    tweet_dev_avg_l = []
                    tweet_dev_med_l = []
                    tweet_dev_var_l = []
                    tweet_abs_dev_avg_l = []
                    tweet_abs_dev_med_l = []
                    tweet_abs_dev_var_l = []

                    tweet_avg_l = []
                    tweet_med_l = []
                    tweet_var_l = []
                    tweet_gt_var_l = []
                    AVG_susc_list = []
                    AVG_wl_list = []
                    all_acc = []
                    AVG_dev_list = []
                    # for lean in [-1, 0, 1]:

                        # AVG_susc_list = []
                        # AVG_wl_list = []
                        # all_acc = []
                        # df_m = df_m[df_m['leaning'] == lean]
                        # if lean == 0:
                        #     col = 'g'
                        #     lean_cat = 'neutral'
                        # elif lean == 1:
                        #     col = 'b'
                        #     lean_cat = 'democrat'
                        # elif lean == -1:
                        #     col = 'r'
                        #     lean_cat = 'republican'
                        # print(lean_cat)
                    for ind in ind_l:

                        if balance_f == 'balanced':
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
                        else:
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'

                        inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
                        df[ind] = pd.read_csv(inp1, sep="\t")
                        df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                        df_m = df[ind].copy()
                        df_mm = df_m.copy()

                        df_m = df_m[df_m['ra_gt'] == cat_l]
                        # df_mm = df_m[df_m['ra_gt']==cat_l]
                        # df_m = df_m[df_m['leaning'] == lean]

                        groupby_ftr = 'tweet_id'
                        grouped = df_m.groupby(groupby_ftr, sort=False)
                        grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                        for t_id in grouped.groups.keys():
                            df_tmp = df_m[df_m['tweet_id'] == t_id]

                            df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
                            df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
                            df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []
                            df_tmp = df_m[df_m['tweet_id'] == t_id]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []

                            weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                            val_list = list(df_tmp['rel_v'])
                            tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_avg[t_id] = np.mean(val_list)
                            tweet_med[t_id] = np.median(val_list)
                            tweet_var[t_id] = np.var(val_list)
                            tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_avg_l.append(np.mean(val_list))
                            tweet_med_l.append(np.median(val_list))
                            tweet_var_l.append(np.var(val_list))
                            tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])




                            tweet_all_avg[t_id] = np.mean(val_list)
                            tweet_all_var[t_id] = np.var(val_list)
                            tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_all_avg_l.append(np.mean(val_list))
                            tweet_all_med_l.append(np.median(val_list))
                            tweet_all_var_l.append(np.var(val_list))
                            tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])



                            val_list = list(df_tmp['err'])
                            abs_var_err = [np.abs(x) for x in val_list]
                            tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_dev_avg[t_id] = np.mean(val_list)
                            tweet_dev_med[t_id] = np.median(val_list)
                            tweet_dev_var[t_id] = np.var(val_list)

                            tweet_dev_avg_l.append(np.mean(val_list))
                            tweet_dev_med_l.append(np.median(val_list))
                            tweet_dev_var_l.append(np.var(val_list))

                            tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_abs_dev_var_l.append(np.var(abs_var_err))


                            tweet_all_dev_avg[t_id] = np.mean(val_list)
                            tweet_all_dev_med[t_id] = np.median(val_list)
                            tweet_all_dev_var[t_id] = np.var(val_list)

                            tweet_all_dev_avg_l.append(np.mean(val_list))
                            tweet_all_dev_med_l.append(np.median(val_list))
                            tweet_all_dev_var_l.append(np.var(val_list))

                            tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_all_abs_dev_var_l.append(np.var(abs_var_err))



                            sum_rnd_abs_perc = 0
                            sum_rnd_perc = 0
                            for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                                sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                                sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                            random_perc = np.abs(sum_rnd_perc / float(7))
                            random_abs_perc = sum_rnd_abs_perc / float(7)

                            tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                            tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                    gt_l = []
                    pt_l = []
                    disputability_l = []
                    perc_l = []
                    abs_perc_l = []
                    # for t_id in tweet_l_sort:
                    #     gt_l.append(tweet_gt_var[t_id])
                    #     pt_l.append(tweet_avg[t_id])
                    #     disputability_l.append(tweet_var[t_id])
                    #     perc_l.append(tweet_dev_avg[t_id])
                    #     abs_perc_l.append(tweet_abs_dev_avg[t_id])



                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
                    tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)

                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)


                    if dataset == 'snopes':
                        data_addr = 'snopes'
                    elif dataset == 'politifact':
                        data_addr = 'politifact/fig'
                    elif dataset == 'mia':
                        data_addr = 'mia/fig'

                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')

                    for t_id in tweet_l_sort:
                        count+=1
                        if balance_f=='balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
                                       + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
                                       # str(np.round(diff_group_disp_dict[t_id], 3)) +
                                       '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
                                        + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
                                        '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')




                if dataset == 'snopes':
                    data_addr = 'snopes'
                elif dataset == 'politifact':
                    data_addr = 'politifact/fig'
                elif dataset == 'mia':
                    data_addr = 'mia/fig'

                # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
                tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)

                # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)

                tweet_napb_dict_high_disp = {}
                tweet_napb_dict_low_disp = {}
                for t_id in tweet_l_sort[:20]:
                    # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                for t_id in tweet_l_sort[-20:]:
                    # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                kk = 0

                for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
                    if kk==0:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
                    else:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)

                    kk+=1
                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    for t_id in tweet_l_sort:
                        count += 1
                        # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
                        if balance_f == 'balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
                                       str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +
                            #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
                            #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                                       str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                        # +
                        # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                        # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_true-false(gt-pt)_news_ktop_nptl_scatter_old":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        # dataset = 'snopes'
        # dataset = 'mia'
        dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes_nonpol','mia','politifact','snopes','mia','politifact']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            if dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpol'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()
            tweet_var_diff_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            tweet_chi_group = {}
            tweet_chi_group_1 = {}
            tweet_chi_group_2 = {}
            tweet_skew = {}
            news_cat_list_tf = [4,2,3,1]
            t_f_dict_len = collections.defaultdict(int)
            t_f_dict = {}
            if dataset=='snopes' or dataset=='snopes_nonpol':
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
            if dataset=='politifact':
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]

            if dataset=='mia':
                news_cat_list_t_f = [['rumor'], ['non-rumor']]

            w_fnb_dict= collections.defaultdict()
            w_fpb_dict= collections.defaultdict()
            w_apb_dict = collections.defaultdict()

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    # if t_id == 1367:
                    #     continue
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]


                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])
                    # vot_list_tmp = []



                    w_sus_list = list(df_tmp['susc'])
                    # w_norm_err_list = list(df_tmp['norm_err'])
                    # w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    # w_cyn_list = list(df_tmp['cyn'])
                    # w_gull_list = list(df_tmp['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])


                    df_cyn = df_tmp[df_tmp['cyn']>0]
                    df_gull = df_tmp[df_tmp['gull']>0]

                    w_cyn_list_t = list(df_cyn['cyn'])
                    w_gull_list_t = list(df_gull['gull'])

                    w_cyn_list = []
                    w_gull_list = []

                    for tt in w_cyn_list_t:
                        if tt>0 or tt<=0:
                            w_cyn_list.append(tt)


                    for tt in w_gull_list_t:
                        if tt>0 or tt<=0:
                            w_gull_list.append(tt)
                    w_fnb_dict[t_id] = np.mean(w_cyn_list)
                    w_fpb_dict[t_id] = np.mean(w_gull_list)
                    w_apb_dict[t_id] = np.mean(w_sus_list)



                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))
                    # ['FALSE-FALSE', 'FALSE-TRUE', 'TRUE-FALSE', 'TRUE-TRUE']

                    if tweet_lable_dict[t_id] in news_cat_list_t_f[0] and np.mean(df_tmp['rel_v']) < 0:
                        t_f_dict[t_id] = 4
                        t_f_dict_len[4]+=1
                    elif tweet_lable_dict[t_id] in news_cat_list_t_f[0] and np.mean(df_tmp['rel_v']) > 0:
                        t_f_dict[t_id] = 2
                        t_f_dict_len[2]+=1
                    elif tweet_lable_dict[t_id] in news_cat_list_t_f[1] and np.mean(df_tmp['rel_v']) > 0:
                        t_f_dict[t_id] = 1
                        t_f_dict_len[1]+=1
                    elif tweet_lable_dict[t_id] in news_cat_list_t_f[1] and np.mean(df_tmp['rel_v']) < 0:
                        t_f_dict[t_id] = 3
                        t_f_dict_len[3]+=1
                    else:
                        t_f_dict[t_id] = -10
                        t_f_dict_len[-10] += 1

                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    val_list = list(df_tmp['rel_v'])




                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_diff_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) + np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)



                    # tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew[t_id] = scipy.stats.skew(dem_val_list) + scipy.stats.skew(rep_val_list)
                    # tweet_skew_l.append(tweet_skew[t_id])

                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])




                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    # tweet_skew[t_id] = scipy.stats.skew(val_list)
                    # tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)




            ##################################################
            gt_fpb = []
            pt_fpb = []
            gt_fnb = []
            pt_fnb = []
            gt_pt = []
            pt_pt = []
            len_cat_dict = {}
            # if dataset=='snopes' or dataset=='politifact':
            for cat in news_cat_list_tf:
                len_cat_dict[cat]=t_f_dict_len[cat]
            # elif dataset=='mia':
            #     for cat in news_cat_list_tf:
            #         if cat=='rumor':
            #             len_cat_dict[cat]=t_f_dict_len[cat]
            #         else:
            #             len_cat_dict[cat] = t_f_dict_len[cat]
            tweet_vote_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)
            tweet_pick =tweet_vote_sort[:2]
            for t_id in tweet_pick:
                print(t_id)
                gt_pt.append(tweet_gt_var[t_id])
                pt_pt.append(tweet_avg[t_id])







            # tweet_vote_sort = sorted(tweet_avg_group, key=tweet_avg_group.get, reverse=True)
            tweet_vote_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)



            tweet_apb_sort = sorted(w_apb_dict, key=w_apb_dict.get, reverse=True)
            tweet_fpb_sort = sorted(w_fpb_dict, key=w_fpb_dict.get, reverse=True)
            tweet_fnb_sort = sorted(w_fnb_dict, key=w_fnb_dict.get, reverse=True)
            # tweet_vote_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_vote_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            top_pic_des_FPB = tweet_fpb_sort[:1]
            for t_id in top_pic_des_FPB:
                gt_fpb.append(tweet_gt_var[t_id])
                pt_fpb.append(tweet_avg[t_id])
            top_pic_des_FNB = tweet_fnb_sort[:1]
            for t_id in top_pic_des_FNB:
                gt_fnb.append(tweet_gt_var[t_id])
                pt_fnb.append(tweet_avg[t_id])


            thr = 10
            thr_list = []
            categ_dict = collections.defaultdict(int)
            categ_dict_n = collections.defaultdict(int)
            len_t = len(tweet_vote_sort)
            k_list = [int(0.1*len_t), int(0.2*len_t), int(0.3*len_t), int(0.4*len_t), int(0.5*len_t), int(1*len_t) ]
            count = 0
            for k in k_list:
                thr_list.append(k)
                perc_rnd_l = []
                abs_perc_rnd_l = []
                disputability_l = []
                above_avg = 0
                less_avg = 0
                above_avg_rnd = 0
                less_avg_rnd = 0
                above_avg = 0
                less_avg = 0
                categ_dict[k] = collections.defaultdict(float)
                categ_dict_n[k] = collections.defaultdict(list)
                for j in range(k):
                    for cat_n in news_cat_list_tf:
                        if cat_n==-10:
                            continue
                        if t_f_dict[tweet_vote_sort[j]] == cat_n:
                            categ_dict[k][cat_n]+=1/float(len_cat_dict[cat_n])

                            categ_dict_n[k][cat_n].append(tweet_vote_sort[j])
            # if dataset=='mia':
            total_data = collections.defaultdict(int)
            for j in categ_dict:
                sum = np.sum(categ_dict[j].values())
                # for cat_n in categ_dict[j]:
                for cat_n in [4, 2, 3, 1]:
                    categ_dict[j][cat_n] =  categ_dict[j][cat_n] / sum

                    total_data[j] += len(categ_dict_n[k][cat_n])


            width = 0.03
            pr = -10
            title_l = news_cat_list_tf
            outp = {}
            # news_cat_list = ['pants-fire', 'false', 'mostly_false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            if dataset=='snopes' or dataset=='snopes_nonpol':
                # col_l = ['b', 'g', 'c', 'y', 'r']
                col_l = ['red', 'magenta', 'lime', 'green']
                marker_l=['s', '*', 'o', 'd']
                news_cat_list_n = ['GT:FALSE, PT:FALSE', 'GT:FALSE, PT:TRUE', 'GT:TRUE, PT:FALSE', 'GT:TRUE, PT:TRUE']
            if dataset=='politifact':
                # col_l = ['grey','b', 'g', 'c', 'y', 'r']
                col_l = ['red', 'magenta', 'lime', 'green']

                news_cat_list_n = ['FALSE-FALSE', 'FALSE-TRUE', 'TRUE-FALSE', 'TRUE-TRUE']

            if dataset=='mia':
                # col_l = ['b', 'r']
                col_l = ['red', 'magenta', 'lime', 'green']
                news_cat_list_n = ['FALSE-FALSE', 'FALSE-TRUE', 'TRUE-FALSE', 'TRUE-TRUE']
            count = 0
            Y = [0]*len(thr_list)
            mplpl.rcParams['figure.figsize'] = 4.8, 4
            mplpl.rc('xtick', labelsize='large')
            mplpl.rc('ytick', labelsize='large')
            mplpl.rc('legend', fontsize='small')
            # for cat_m in news_cat_list_tf:
            #     count+=1
            #     outp[cat_m] = []
            #     for i in thr_list:
            #         outp[cat_m].append(categ_dict[i][cat_m])
            #     mplpl.bar([0.1, 0.2, 0.3, 0.4,0.5,0.6], outp[cat_m], width, bottom= np.array(Y), color=col_l[count-1], label=news_cat_list_n[count-1])
            #     Y = np.array(Y) + np.array(outp[cat_m])

            tweet_vote_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)

            gt_l_t = []
            pt_l_t = []
            apb_l_t = []
            for i in [int(0.1*len_t)]:
                tweet_id_list = tweet_vote_sort[:int(0.1*len_t)]
                for t_id in tweet_id_list:
                    gt_l_t.append(tweet_gt_var[t_id])
                    pt_l_t.append(tweet_avg[t_id])
                    apb_l_t.append(w_apb_dict[t_id])


            tweet_vote_sort = sorted(tweet_vote_avg, key=tweet_vote_avg.get, reverse=True)

            gt_l_t_vot = []
            pt_l_t_vot = []
            apb_l_t_vot = []
            for i in [int(0.1 * len_t)]:
                tweet_id_list = tweet_vote_sort[:int(0.1 * len_t)]
                for t_id in tweet_id_list:
                    gt_l_t_vot.append(tweet_gt_var[t_id])
                    pt_l_t_vot.append(tweet_avg[t_id])
                    apb_l_t_vot.append(w_apb_dict[t_id])

            #
            # for cat_m in news_cat_list_tf:
            #     count+=1
            #     outp[cat_m] = []
            #     gt_l_t = []
            #     pt_l_t = []
            #     for i in [int(1*len_t)]:
            #         tweet_id_list = categ_dict_n[i][cat_m]
            #         for t_id in tweet_id_list:
            #             gt_l_t.append(tweet_gt_var[t_id])
            #             pt_l_t.append(tweet_avg[t_id])
            #
            #     m_label = str(np.round(len(tweet_id_list) / float(total_data[int(1 * len_t)]),2))

            # mplpl.scatter(gt_l_t, pt_l_t,c=col_l[count-1],marker=marker_l[count-1], s=60, label = m_label)
            # mplpl.scatter(pt_l_t,gt_l_t, c=col_l[count - 1], marker=marker_l[count - 1], s=60, label=m_label)


            # mplpl.scatter(gt_fnb,pt_fnb, c='r', marker='s', s=200)#, label=m_label)
            # mplpl.scatter(gt_fpb,pt_fpb, c='r', marker='d', s=200)#, label=m_label)
            # mplpl.scatter(gt_pt,pt_pt, c='orange', marker='^', s=200)#, label=m_label)
            # mplpl.scatter(gt_l_t,pt_l_t, c='g', marker='o', s=60)#, label=m_label)
            # mplpl.scatter(gt_l_t,apb_l_t, c='g', marker='o', s=60)#, label=m_label)
            mplpl.scatter(gt_l_t_vot,apb_l_t_vot, c='r', marker='+', s=60)#, label=m_label)


            # mplpl.plot([-1.1, 1.1], [-1.1,1.1],c='k',linewidth=4)
            # mplpl.plot([0,0],[-1.1, 1.1],c='k',linewidth=4)

            mplpl.xlim([-1.04, 1.04])
            mplpl.ylim([0, 1.5])

            font = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 16,
                    }

            font_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 16,
                    }


            font_t = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'bold',
                    'size': 12,
                    }

            font_t_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 12,
                    }
            # mplpl.text(-0.7, 0.85, ' Mostly deserve to pick', fontdict=font_t)
            # # mplpl.text(-0.3, -0.9, '# news has negative perception truth value', fontdict=font_t_1)
            # mplpl.annotate('', xy=(-1, 0.38), xytext=(0, 0.82),
            #             arrowprops=dict(facecolor='r', shrink=0.1))
            #
            # mplpl.annotate('', xy=(1, -0.25), xytext=(0, 0.82),
            #             arrowprops=dict(facecolor='r', shrink=0.1))

            mplpl.subplots_adjust(bottom=0.32)

            mplpl.subplots_adjust(left=0.2)
            mplpl.title(data_name)
            labels = ['-1\nFalse', '-.05\nMostly\nFalse', '0\nMixture', '0.5\nMostly\n True', '1\nTrue']
            x = [ -1, -.5, 0, 0.5, 1]
            mplpl.xticks(x, labels)

            # mplpl.ylabel('Composition of news stories \n in different quadrants', fontsize=16,fontweight = 'bold')
            mplpl.xlabel('Ground Truth Level', fontsize=16,fontweight = 'bold')
            # mplpl.ylabel('Perceived Truth Level', fontsize=16,fontweight = 'bold')
            mplpl.ylabel('Total Perception Bias', fontsize=16,fontweight = 'bold')
            # mplpl.xlabel('Top k news stories ranked by NAPB', fontsize=18)

            # mplpl.legend(loc="center", ncol=2, fontsize='small')



            mplpl.grid()
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_10_top_disp_gt_pt_compos_true-false_scatter'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_20_top_ideolg_disp_gt_pt_compos_true-false_scatter'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_20_top_ideolg_disp_gt_apb_compos_true-false_scatter'
            pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_10_top_disp_gt_apb_compos_true-false_scatter'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_napb_compos_true-false_scatter'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_vote_compos_true-false_scatter'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_disp_compos_true-false_scatter'
            mplpl.savefig(pp + '.pdf', format='pdf')
            mplpl.savefig(pp+ '.png', format='png')

            mplpl.figure()


            exit()





























            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l = []
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            thr = 5
            prec_rnd_acc = 0
            abs_prec_rnd_acc = 0
            norm_abs_perc_bias_l = []
            abs_perc_bias_l = []
            norm_abs_perc_bias_ll = []
            abs_perc_bias_ll = []
            k_l = []
            # for i in range(int(len(tweet_disp_sort)/float(thr))):
            #     k = (i+1)*thr
            #     perc_rnd_l = []
            #     abs_perc_rnd_l = []
            #     disputability_l = []
            #     above_avg = 0
            #     less_avg = 0
            #     above_avg_rnd = 0
            #     less_avg_rnd = 0
            #     above_avg = 0
            #     less_avg = 0
            #     for j in range(k):
            #
            #         perc_rnd_l.append(tweet_abs_dev_avg[tweet_disp_sort[j]])
            #         abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[tweet_disp_sort[j]])
            #
            #         if tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg_rnd.values()):
            #             above_avg_rnd+=1
            #         elif tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg_rnd.values()):
            #             less_avg_rnd+=1
            #
            #
            #         if tweet_abs_dev_avg[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg.values()):
            #             above_avg+=1
            #         elif tweet_abs_dev_avg[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg.values()):
            #             less_avg+=1
            #
            #     norm_abs_perc_bias_ll.append(np.mean(abs_perc_rnd_l))
            #     abs_perc_bias_ll.append(np.mean(perc_rnd_l))
            #
            #     # perc_rnd_acc = len(set(disputability_l).intersection(perc_rnd_l))/ float(len(perc_rnd_l))
            #     # abs_perc_rnd_acc = len(set(disputability_l).intersection(abs_perc_rnd_l))/ float(len(abs_perc_rnd_l))
            #     # print('---------- k = ' +str(k/float(len(tweet_disp_sort)))+ '-------------')
            #     # print(str(above_avg_rnd/float(above_avg_rnd+less_avg_rnd)))
            #     # print(str(above_avg/float(above_avg+less_avg)))
            #     # print(perc_rnd_acc)
            #     # print(abs_perc_rnd_acc)
            #     # print('------------------------------')
            #
            #     norm_abs_perc_bias_l.append(above_avg_rnd/float(above_avg_rnd+less_avg_rnd))
            #     abs_perc_bias_l.append(above_avg/float(above_avg+less_avg))
            #     k_l.append(np.round(k/float(len(tweet_disp_sort)), 3))
            #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_l ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_l, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their NAPB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_l ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_l, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their APB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_abs_perception'
            # # mplpl.savefig(pp, format='png')
            # #
            # # mplpl.figure()
            # #
            # #
            # #
            # #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_ll ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_ll, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg NAPB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_ll ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_ll, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg APB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-abs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.show()
            #
            #
            #
            #     ##################################################
            #
            # print(np.mean(tweet_abs_dev_avg_rnd.values()))
            # exit()


            # tweet_avg_group = collections.defaultdict()
            # tweet_med_group = collections.defaultdict()
            # tweet_var_group = collections.defaultdict()
            #
            # tweet_kldiv_group= collections.defaultdict()

            tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l=[]
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            popularity_l= []
            vote_l = []
            gr_disp_l = []
            for t_id in tweet_l_sort:
                gt_l.append(tweet_gt_var[t_id])
                pt_l.append(tweet_avg[t_id])
                disputability_l.append(tweet_var[t_id])
                perc_l.append(tweet_dev_avg[t_id])
                abs_perc_l.append(tweet_abs_dev_avg[t_id])
                # popularity_l.append(tweet_popularity[t_id])
                vote_l.append(tweet_vote_avg[t_id])
                # vote_l.append(tweet_vote_var[t_id])
                perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
                abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
                tweet_skew_ll.append(tweet_skew[t_id])
                gr_disp_l.append(tweet_skew[t_id])
            value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l, vote_l,gr_disp_l]#,popularity_l,tweet_skew_ll]
            value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
                          'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'vote', 'group_disputability']#,'popularity' 'skewness']

            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(9):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(9):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')


            exit()
            tweet_group_gt = collections.defaultdict(list)
            gt_set = sorted(set(gt_l))
            for gt_e in gt_set:
                for t_id in tweet_l_sort:
                    if tweet_gt_var[t_id]==gt_e:
                        tweet_group_gt[gt_e].append(t_id)

            pt_mean = []
            disp_mean = []
            perc_mean = []
            abs_perc_mean = []
            for gt_e in gt_set:
                t_id_l = tweet_group_gt[gt_e]
                pt_mean.append(np.mean([tweet_avg[x] for x in t_id_l]))
                disp_mean.append(np.mean([tweet_var[x] for x in t_id_l]))
                perc_mean.append(np.mean([tweet_dev_avg[x] for x in t_id_l]))
                abs_perc_mean.append(np.mean([tweet_abs_dev_avg[x] for x in t_id_l]))

            # outF = open(remotedir + 'table_out.txt', 'w')

            # outF.write('== ' + data_name + ' ==\n')
            font = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 16,
                    }

            font_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 16,
                    }


            font_t = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 12,
                    }

            font_t_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 12,
                    }



            # fig_f = True
            fig_f = False
            if fig_f==True:

                mplpl.scatter(gt_l, pt_l,  s=40,color='r',marker='o', label='All users')
                mplpl.scatter(gt_set, pt_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]

                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_avg[x]>0:
                            num_pos+=1
                        elif tweet_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e , 0.85, str(num_pos) , fontdict=font)
                    mplpl.text(gt_e , 0.75, str(num_neg) , fontdict=font_1)

                mplpl.text(-0.3, -0.75, ' # news has positive perception truth value', fontdict=font_t)
                mplpl.text(-0.3, -0.9,   '# news has negative perception truth value', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_pt| alt text| width = 500px}}')


                mplpl.scatter(gt_l,disputability_l ,  s=40,color='g',marker='o', label='All users')
                mplpl.scatter(gt_set, disp_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, disp_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                # for gt_e in gt_set:
                #     t_id_l = tweet_group_gt[gt_e]

                #     num_pos=0
                #     num_neg = 0
                #
                #     for x in t_id_l:
                #         if tweet_var[x]>0:
                #             num_pos+=1
                #         elif tweet_var[x]<0:
                #             num_neg+=1
                #     # mplpl.text(gt_e-0.2, 0.85, str(num_pos) + r'$ > 0$', fontdict=font)
                #     # mplpl.text(gt_e-0.2, 0.75, str(num_neg) + r'$ < 0$', fontdict=font_1)

                #
                # mplpl.text(0, 0.9, ' # news perceived true(postitive)', fontdict=font_t)
                # mplpl.text(0, 0.75,   '# news perceived false(negative)', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_disputability'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_disputability| alt text| width = 500px}}')


                gt_l_dict[data_name] = gt_l
                perc_l_dict[data_name] = perc_l
                gt_set_dict[data_name] = gt_set
                perc_mean_dict[data_name] = perc_mean

                mplpl.scatter(gt_l, perc_l,  s=40,color='b',marker='o', label='All users')
                mplpl.scatter(gt_set, perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, perc_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]
                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_dev_avg[x]>0:
                            num_pos+=1
                        elif tweet_dev_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e, 1.65, str(num_pos), fontdict=font)
                    mplpl.text(gt_e, 1.3, str(num_neg), fontdict=font_1)

                mplpl.text(-1, -1, '# news that has positive perception bias value', fontdict=font_t)
                mplpl.text(-1, -1.5, '# news that has nigative perception bias value', fontdict=font_t_1)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_perception| alt text| width = 500px}}')


                mplpl.scatter(gt_l,abs_perc_l ,  s=40,color='c',marker='o', label='All users')
                mplpl.scatter(gt_set, abs_perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, abs_perc_mean, color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)


                abs_perc_l_dict[data_name] = abs_perc_l
                abs_perc_mean_dict[data_name] = abs_perc_mean


                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_abs_perception| alt text| width = 500px}} ||\n')

                # outF.write('|| Table ||\n\n')

            # exit
        ########################

                tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)
                pt_l = []
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_avg[t_id])

                mplpl.scatter(range(len(pt_l)), pt_l,  s=40,color='r',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PTL', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_pt| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=False)
                disputability_l = []
                for t_id in tweet_l_sort:
                    disputability_l.append(tweet_var[t_id])


                mplpl.scatter(range(len(pt_l)), disputability_l ,  s=40,color='g',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ranked news stories according Disputability', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_disput'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_disput| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=False)
                perc_l = []
                for t_id in tweet_l_sort:
                    perc_l.append(tweet_dev_avg[t_id])

                mplpl.scatter(range(len(pt_l)), perc_l,  s=40,color='b',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias (PB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_percep| alt text| width = 500px}}')



                tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=False)
                perc_abs_l = []
                for t_id in tweet_l_sort:
                    perc_abs_l.append(tweet_abs_dev_avg[t_id])

                mplpl.scatter(range(len(perc_abs_l)), perc_abs_l,  s=40,color='c',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according APB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_abs_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_abs-percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_abs-percep| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



        #####################################################33


                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='r', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')


                num_bins = len(disputability_l)
                counts, bin_edges = np.histogram(disputability_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Disputability', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_cdf| alt text| width = 500px}}')


                num_bins = len(perc_l)
                counts, bin_edges = np.histogram(perc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='b', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception bias (PB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_cdf| alt text| width = 500px}}')

                num_bins = len(perc_abs_l)
                counts, bin_edges = np.histogram(perc_abs_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_cdf| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



                # mplpl.show()

                col_l = ['r', 'b', 'g']

                i = 0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=400, color=col_l[i], marker='*')
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i = 0
                mark_l = ['*', 'o', '^']
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i=0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o')
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=300, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                i=0
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()











                mplpl.show()

            else:

                AVG_list = []
                print(np.mean(all_acc))
                outF = open(remotedir + 'output.txt', 'w')

                tweet_all_var = {}
                tweet_all_dev_avg = {}
                tweet_all_avg = {}
                tweet_all_gt_var = {}
                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []
                tweet_all_avg_l = []
                tweet_all_med_l = []
                tweet_all_var_l = []
                tweet_all_gt_var_l = []
                diff_group_disp_l = []
                dem_disp_l = []
                rep_disp_l = []

                tweet_all_dev_avg = {}
                tweet_all_dev_med = {}
                tweet_all_dev_var = {}

                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []

                tweet_all_abs_dev_avg = {}
                tweet_all_abs_dev_med = {}
                tweet_all_abs_dev_var = {}

                tweet_all_abs_dev_avg_l = []
                tweet_all_abs_dev_med_l = []
                tweet_all_abs_dev_var_l = []
                tweet_all_dev_avg_rnd = {}
                tweet_all_abs_dev_avg_rnd = {}

                diff_group_disp_dict = {}
                if dataset == 'snopes':
                    data_n = 'sp'
                    news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                    ind_l = [1, 2, 3]
                elif dataset == 'politifact':
                    data_n = 'pf'
                    news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                    ind_l = [1, 2, 3]
                elif dataset == 'mia':
                    data_n = 'mia'
                    news_cat_list = ['rumor', 'non-rumor']
                    ind_l = [1]

                for cat_l in news_cat_list:
                    outF.write('== ' + str(cat_l) + ' ==\n\n')
                    print('== ' + str(cat_l) + ' ==')
                    tweet_dev_avg = {}
                    tweet_dev_med = {}
                    tweet_dev_var = {}
                    tweet_abs_dev_avg = {}
                    tweet_abs_dev_med = {}
                    tweet_abs_dev_var = {}

                    tweet_avg = {}
                    tweet_med = {}
                    tweet_var = {}
                    tweet_gt_var = {}

                    tweet_dev_avg_rnd = {}
                    tweet_abs_dev_avg_rnd = {}


                    tweet_dev_avg_l = []
                    tweet_dev_med_l = []
                    tweet_dev_var_l = []
                    tweet_abs_dev_avg_l = []
                    tweet_abs_dev_med_l = []
                    tweet_abs_dev_var_l = []

                    tweet_avg_l = []
                    tweet_med_l = []
                    tweet_var_l = []
                    tweet_gt_var_l = []
                    AVG_susc_list = []
                    AVG_wl_list = []
                    all_acc = []
                    AVG_dev_list = []
                    # for lean in [-1, 0, 1]:

                        # AVG_susc_list = []
                        # AVG_wl_list = []
                        # all_acc = []
                        # df_m = df_m[df_m['leaning'] == lean]
                        # if lean == 0:
                        #     col = 'g'
                        #     lean_cat = 'neutral'
                        # elif lean == 1:
                        #     col = 'b'
                        #     lean_cat = 'democrat'
                        # elif lean == -1:
                        #     col = 'r'
                        #     lean_cat = 'republican'
                        # print(lean_cat)
                    for ind in ind_l:

                        if balance_f == 'balanced':
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
                        else:
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'

                        inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
                        df[ind] = pd.read_csv(inp1, sep="\t")
                        df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                        df_m = df[ind].copy()
                        df_mm = df_m.copy()

                        df_m = df_m[df_m['ra_gt'] == cat_l]
                        # df_mm = df_m[df_m['ra_gt']==cat_l]
                        # df_m = df_m[df_m['leaning'] == lean]

                        groupby_ftr = 'tweet_id'
                        grouped = df_m.groupby(groupby_ftr, sort=False)
                        grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                        for t_id in grouped.groups.keys():
                            df_tmp = df_m[df_m['tweet_id'] == t_id]

                            df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
                            df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
                            df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []
                            df_tmp = df_m[df_m['tweet_id'] == t_id]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []

                            weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                            val_list = list(df_tmp['rel_v'])
                            tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_avg[t_id] = np.mean(val_list)
                            tweet_med[t_id] = np.median(val_list)
                            tweet_var[t_id] = np.var(val_list)
                            tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_avg_l.append(np.mean(val_list))
                            tweet_med_l.append(np.median(val_list))
                            tweet_var_l.append(np.var(val_list))
                            tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])




                            tweet_all_avg[t_id] = np.mean(val_list)
                            tweet_all_var[t_id] = np.var(val_list)
                            tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_all_avg_l.append(np.mean(val_list))
                            tweet_all_med_l.append(np.median(val_list))
                            tweet_all_var_l.append(np.var(val_list))
                            tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])



                            val_list = list(df_tmp['err'])
                            abs_var_err = [np.abs(x) for x in val_list]
                            tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_dev_avg[t_id] = np.mean(val_list)
                            tweet_dev_med[t_id] = np.median(val_list)
                            tweet_dev_var[t_id] = np.var(val_list)

                            tweet_dev_avg_l.append(np.mean(val_list))
                            tweet_dev_med_l.append(np.median(val_list))
                            tweet_dev_var_l.append(np.var(val_list))

                            tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_abs_dev_var_l.append(np.var(abs_var_err))


                            tweet_all_dev_avg[t_id] = np.mean(val_list)
                            tweet_all_dev_med[t_id] = np.median(val_list)
                            tweet_all_dev_var[t_id] = np.var(val_list)

                            tweet_all_dev_avg_l.append(np.mean(val_list))
                            tweet_all_dev_med_l.append(np.median(val_list))
                            tweet_all_dev_var_l.append(np.var(val_list))

                            tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_all_abs_dev_var_l.append(np.var(abs_var_err))



                            sum_rnd_abs_perc = 0
                            sum_rnd_perc = 0
                            for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                                sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                                sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                            random_perc = np.abs(sum_rnd_perc / float(7))
                            random_abs_perc = sum_rnd_abs_perc / float(7)

                            tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                            tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                    gt_l = []
                    pt_l = []
                    disputability_l = []
                    perc_l = []
                    abs_perc_l = []
                    # for t_id in tweet_l_sort:
                    #     gt_l.append(tweet_gt_var[t_id])
                    #     pt_l.append(tweet_avg[t_id])
                    #     disputability_l.append(tweet_var[t_id])
                    #     perc_l.append(tweet_dev_avg[t_id])
                    #     abs_perc_l.append(tweet_abs_dev_avg[t_id])



                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
                    tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)

                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)


                    if dataset == 'snopes':
                        data_addr = 'snopes'
                    elif dataset == 'politifact':
                        data_addr = 'politifact/fig'
                    elif dataset == 'mia':
                        data_addr = 'mia/fig'

                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')

                    for t_id in tweet_l_sort:
                        count+=1
                        if balance_f=='balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
                                       + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
                                       # str(np.round(diff_group_disp_dict[t_id], 3)) +
                                       '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
                                        + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
                                        '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')




                if dataset == 'snopes':
                    data_addr = 'snopes'
                elif dataset == 'politifact':
                    data_addr = 'politifact/fig'
                elif dataset == 'mia':
                    data_addr = 'mia/fig'

                # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
                tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)

                # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)

                tweet_napb_dict_high_disp = {}
                tweet_napb_dict_low_disp = {}
                for t_id in tweet_l_sort[:20]:
                    # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                for t_id in tweet_l_sort[-20:]:
                    # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                kk = 0

                for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
                    if kk==0:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
                    else:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)

                    kk+=1
                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    for t_id in tweet_l_sort:
                        count += 1
                        # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
                        if balance_f == 'balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
                                       str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +
                            #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
                            #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                                       str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                        # +
                        # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                        # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')



    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_composition_true-false(gt-pt)_news_ktop_nptl_scatter":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        # dataset = 'snopes'
        # dataset = 'mia'
        dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','snopes_nonpol','mia','politifact','snopes','mia','politifact']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            elif dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpol'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()
            tweet_var_diff_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            tweet_chi_group = {}
            tweet_chi_group_1 = {}
            tweet_chi_group_2 = {}
            tweet_skew = {}
            news_cat_list_tf = [4,2,3,1]
            t_f_dict_len = collections.defaultdict(int)
            t_f_dict = {}
            if dataset=='snopes' or dataset=='snopes_nonpol':
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
            if dataset=='politifact':
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]

            if dataset=='mia':
                news_cat_list_t_f = [['rumor'], ['non-rumor']]

            w_fnb_dict= collections.defaultdict()
            w_fpb_dict= collections.defaultdict()
            w_apb_dict = collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            w_apb_dict = {}
            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                    # inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_weighted.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)

                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)


                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)



                    val_list = list(df_tmp['susc'])
                    w_apb_dict[t_id] = np.mean(val_list)


            ##################################################
            gt_fpb = []
            pt_fpb = []
            gt_fnb = []
            pt_fnb = []
            gt_pt = []
            pt_pt = []
            len_cat_dict = {}



            # Y = [0]*len(thr_list)
            mplpl.rcParams['figure.figsize'] = 4.8, 4
            mplpl.rc('xtick', labelsize='large')
            mplpl.rc('ytick', labelsize='large')
            mplpl.rc('legend', fontsize='small')


            tweet_vote_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)

            gt_l_t = []
            pt_l_t = []
            apb_l_t = []
            for i in [int(0.1*len(tweet_vote_sort))]:
                tweet_id_list = tweet_vote_sort[:int(0.1*len(tweet_vote_sort))]
                for t_id in tweet_id_list:
                    gt_l_t.append(tweet_gt_var[t_id])
                    pt_l_t.append(tweet_avg[t_id])
                    apb_l_t.append(w_apb_dict[t_id])


            tweet_vote_sort = sorted(tweet_vote_avg, key=tweet_vote_avg.get, reverse=False)

            gt_l_t_vot = []
            pt_l_t_vot = []
            apb_l_t_vot = []
            for i in [int(0.1 * len(tweet_vote_sort))]:
                tweet_id_list = tweet_vote_sort[:int(0.1 * len(tweet_vote_sort))]
                for t_id in tweet_id_list:
                    gt_l_t_vot.append(tweet_gt_var[t_id])
                    pt_l_t_vot.append(tweet_avg[t_id])
                    apb_l_t_vot.append(w_apb_dict[t_id])

            #
            # for cat_m in news_cat_list_tf:
            #     count+=1
            #     outp[cat_m] = []
            #     gt_l_t = []
            #     pt_l_t = []
            #     for i in [int(1*len_t)]:
            #         tweet_id_list = categ_dict_n[i][cat_m]
            #         for t_id in tweet_id_list:
            #             gt_l_t.append(tweet_gt_var[t_id])
            #             pt_l_t.append(tweet_avg[t_id])
            #
            #     m_label = str(np.round(len(tweet_id_list) / float(total_data[int(1 * len_t)]),2))

            # mplpl.scatter(gt_l_t, pt_l_t,c=col_l[count-1],marker=marker_l[count-1], s=60, label = m_label)
            # mplpl.scatter(pt_l_t,gt_l_t, c=col_l[count - 1], marker=marker_l[count - 1], s=60, label=m_label)


            # mplpl.scatter(gt_fnb,pt_fnb, c='r', marker='s', s=200)#, label=m_label)
            # mplpl.scatter(gt_fpb,pt_fpb, c='r', marker='d', s=200)#, label=m_label)
            # mplpl.scatter(gt_pt,pt_pt, c='orange', marker='^', s=200)#, label=m_label)
            # mplpl.scatter(gt_l_t,pt_l_t, c='g', marker='o', s=60)#, label=m_label)
            mplpl.scatter(gt_l_t,apb_l_t, c='g', marker='o', s=60, label='Picked by Disputability')
            mplpl.scatter(gt_l_t_vot,apb_l_t_vot, c='r', marker='+', s=120, label='Picked by Negative PTL')


            # mplpl.plot([-1.1, 1.1], [-1.1,1.1],c='k',linewidth=4)
            # mplpl.plot([0,0],[-1.1, 1.1],c='k',linewidth=4)

            mplpl.xlim([-1.04, 1.04])
            mplpl.ylim([0, 1.5])

            font = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 16,
                    }

            font_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 16,
                    }


            font_t = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'bold',
                    'size': 12,
                    }

            font_t_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 12,
                    }
            # mplpl.text(-0.7, 0.85, ' Mostly deserve to pick', fontdict=font_t)
            # # mplpl.text(-0.3, -0.9, '# news has negative perception truth value', fontdict=font_t_1)
            # mplpl.annotate('', xy=(-1, 0.38), xytext=(0, 0.82),
            #             arrowprops=dict(facecolor='r', shrink=0.1))
            #
            # mplpl.annotate('', xy=(1, -0.25), xytext=(0, 0.82),
            #             arrowprops=dict(facecolor='r', shrink=0.1))

            mplpl.subplots_adjust(bottom=0.32)

            mplpl.subplots_adjust(left=0.2)
            mplpl.title(data_name)
            labels = ['-1\nFalse', '-.05\nMostly\nFalse', '0\nMixture', '0.5\nMostly\n True', '1\nTrue']
            x = [ -1, -.5, 0, 0.5, 1]
            mplpl.xticks(x, labels)

            # mplpl.ylabel('Composition of news stories \n in different quadrants', fontsize=16,fontweight = 'bold')
            mplpl.xlabel('Ground Truth Level', fontsize=16,fontweight = 'bold')
            # mplpl.ylabel('Perceived Truth Level', fontsize=16,fontweight = 'bold')
            mplpl.ylabel('Total Perception Bias', fontsize=16,fontweight = 'bold')
            # mplpl.xlabel('Top k news stories ranked by NAPB', fontsize=18)

            mplpl.legend(loc="upper center", ncol=1, fontsize='medium')



            mplpl.grid()
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_10_top_disp_gt_pt_compos_true-false_scatter'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_20_top_ideolg_disp_gt_pt_compos_true-false_scatter'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_20_top_ideolg_disp_gt_apb_compos_true-false_scatter'
            pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_10_top_disp_gt_apb_compos_true-false_scatter'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_10_top_disp_gt_apb_compos_true-false_scatter_weighted'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_napb_compos_true-false_scatter'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_vote_compos_true-false_scatter'
            # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + data_n + '_disp_compos_true-false_scatter'
            mplpl.savefig(pp + '.pdf', format='pdf')
            mplpl.savefig(pp+ '.png', format='png')

            mplpl.figure()


            exit()





























            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l = []
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            thr = 5
            prec_rnd_acc = 0
            abs_prec_rnd_acc = 0
            norm_abs_perc_bias_l = []
            abs_perc_bias_l = []
            norm_abs_perc_bias_ll = []
            abs_perc_bias_ll = []
            k_l = []
            # for i in range(int(len(tweet_disp_sort)/float(thr))):
            #     k = (i+1)*thr
            #     perc_rnd_l = []
            #     abs_perc_rnd_l = []
            #     disputability_l = []
            #     above_avg = 0
            #     less_avg = 0
            #     above_avg_rnd = 0
            #     less_avg_rnd = 0
            #     above_avg = 0
            #     less_avg = 0
            #     for j in range(k):
            #
            #         perc_rnd_l.append(tweet_abs_dev_avg[tweet_disp_sort[j]])
            #         abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[tweet_disp_sort[j]])
            #
            #         if tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg_rnd.values()):
            #             above_avg_rnd+=1
            #         elif tweet_abs_dev_avg_rnd[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg_rnd.values()):
            #             less_avg_rnd+=1
            #
            #
            #         if tweet_abs_dev_avg[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg.values()):
            #             above_avg+=1
            #         elif tweet_abs_dev_avg[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg.values()):
            #             less_avg+=1
            #
            #     norm_abs_perc_bias_ll.append(np.mean(abs_perc_rnd_l))
            #     abs_perc_bias_ll.append(np.mean(perc_rnd_l))
            #
            #     # perc_rnd_acc = len(set(disputability_l).intersection(perc_rnd_l))/ float(len(perc_rnd_l))
            #     # abs_perc_rnd_acc = len(set(disputability_l).intersection(abs_perc_rnd_l))/ float(len(abs_perc_rnd_l))
            #     # print('---------- k = ' +str(k/float(len(tweet_disp_sort)))+ '-------------')
            #     # print(str(above_avg_rnd/float(above_avg_rnd+less_avg_rnd)))
            #     # print(str(above_avg/float(above_avg+less_avg)))
            #     # print(perc_rnd_acc)
            #     # print(abs_perc_rnd_acc)
            #     # print('------------------------------')
            #
            #     norm_abs_perc_bias_l.append(above_avg_rnd/float(above_avg_rnd+less_avg_rnd))
            #     abs_perc_bias_l.append(above_avg/float(above_avg+less_avg))
            #     k_l.append(np.round(k/float(len(tweet_disp_sort)), 3))
            #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_l ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_l, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their NAPB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_l ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_l, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their APB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_abs_perception'
            # # mplpl.savefig(pp, format='png')
            # #
            # # mplpl.figure()
            # #
            # #
            # #
            # #
            # # mplpl.scatter(k_l,norm_abs_perc_bias_ll ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # # mplpl.plot(k_l, norm_abs_perc_bias_ll, color='c')
            # #
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg NAPB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_ll ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_ll, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0.5, 1.02])
            # # mplpl.ylabel('Avg APB of news stories', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_avg-abs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.show()
            #
            #
            #
            #     ##################################################
            #
            # print(np.mean(tweet_abs_dev_avg_rnd.values()))
            # exit()


            # tweet_avg_group = collections.defaultdict()
            # tweet_med_group = collections.defaultdict()
            # tweet_var_group = collections.defaultdict()
            #
            # tweet_kldiv_group= collections.defaultdict()

            tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l=[]
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            popularity_l= []
            vote_l = []
            gr_disp_l = []
            for t_id in tweet_l_sort:
                gt_l.append(tweet_gt_var[t_id])
                pt_l.append(tweet_avg[t_id])
                disputability_l.append(tweet_var[t_id])
                perc_l.append(tweet_dev_avg[t_id])
                abs_perc_l.append(tweet_abs_dev_avg[t_id])
                # popularity_l.append(tweet_popularity[t_id])
                vote_l.append(tweet_vote_avg[t_id])
                # vote_l.append(tweet_vote_var[t_id])
                perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
                abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
                tweet_skew_ll.append(tweet_skew[t_id])
                gr_disp_l.append(tweet_skew[t_id])
            value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l, vote_l,gr_disp_l]#,popularity_l,tweet_skew_ll]
            value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
                          'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'vote', 'group_disputability']#,'popularity' 'skewness']

            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(9):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(9):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')


            exit()
            tweet_group_gt = collections.defaultdict(list)
            gt_set = sorted(set(gt_l))
            for gt_e in gt_set:
                for t_id in tweet_l_sort:
                    if tweet_gt_var[t_id]==gt_e:
                        tweet_group_gt[gt_e].append(t_id)

            pt_mean = []
            disp_mean = []
            perc_mean = []
            abs_perc_mean = []
            for gt_e in gt_set:
                t_id_l = tweet_group_gt[gt_e]
                pt_mean.append(np.mean([tweet_avg[x] for x in t_id_l]))
                disp_mean.append(np.mean([tweet_var[x] for x in t_id_l]))
                perc_mean.append(np.mean([tweet_dev_avg[x] for x in t_id_l]))
                abs_perc_mean.append(np.mean([tweet_abs_dev_avg[x] for x in t_id_l]))

            # outF = open(remotedir + 'table_out.txt', 'w')

            # outF.write('== ' + data_name + ' ==\n')
            font = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 16,
                    }

            font_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 16,
                    }


            font_t = {'family': 'serif',
                    'color': 'darkred',
                    'weight': 'normal',
                    'size': 12,
                    }

            font_t_1 = {'family': 'serif',
                    'color': 'darkblue',
                    'weight': 'normal',
                    'size': 12,
                    }



            # fig_f = True
            fig_f = False
            if fig_f==True:

                mplpl.scatter(gt_l, pt_l,  s=40,color='r',marker='o', label='All users')
                mplpl.scatter(gt_set, pt_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]

                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_avg[x]>0:
                            num_pos+=1
                        elif tweet_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e , 0.85, str(num_pos) , fontdict=font)
                    mplpl.text(gt_e , 0.75, str(num_neg) , fontdict=font_1)

                mplpl.text(-0.3, -0.75, ' # news has positive perception truth value', fontdict=font_t)
                mplpl.text(-0.3, -0.9,   '# news has negative perception truth value', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_pt| alt text| width = 500px}}')


                mplpl.scatter(gt_l,disputability_l ,  s=40,color='g',marker='o', label='All users')
                mplpl.scatter(gt_set, disp_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, disp_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                # for gt_e in gt_set:
                #     t_id_l = tweet_group_gt[gt_e]

                #     num_pos=0
                #     num_neg = 0
                #
                #     for x in t_id_l:
                #         if tweet_var[x]>0:
                #             num_pos+=1
                #         elif tweet_var[x]<0:
                #             num_neg+=1
                #     # mplpl.text(gt_e-0.2, 0.85, str(num_pos) + r'$ > 0$', fontdict=font)
                #     # mplpl.text(gt_e-0.2, 0.75, str(num_neg) + r'$ < 0$', fontdict=font_1)

                #
                # mplpl.text(0, 0.9, ' # news perceived true(postitive)', fontdict=font_t)
                # mplpl.text(0, 0.75,   '# news perceived false(negative)', fontdict=font_t_1)

                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_disputability'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_disputability| alt text| width = 500px}}')


                gt_l_dict[data_name] = gt_l
                perc_l_dict[data_name] = perc_l
                gt_set_dict[data_name] = gt_set
                perc_mean_dict[data_name] = perc_mean

                mplpl.scatter(gt_l, perc_l,  s=40,color='b',marker='o', label='All users')
                mplpl.scatter(gt_set, perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, perc_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                for gt_e in gt_set:
                    t_id_l = tweet_group_gt[gt_e]
                    num_pos=0
                    num_neg = 0

                    for x in t_id_l:
                        if tweet_dev_avg[x]>0:
                            num_pos+=1
                        elif tweet_dev_avg[x]<0:
                            num_neg+=1

                    mplpl.text(gt_e, 1.65, str(num_pos), fontdict=font)
                    mplpl.text(gt_e, 1.3, str(num_neg), fontdict=font_1)

                mplpl.text(-1, -1, '# news that has positive perception bias value', fontdict=font_t)
                mplpl.text(-1, -1.5, '# news that has nigative perception bias value', fontdict=font_t_1)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_perception| alt text| width = 500px}}')


                mplpl.scatter(gt_l,abs_perc_l ,  s=40,color='c',marker='o', label='All users')
                mplpl.scatter(gt_set, abs_perc_mean,  s=300,color='k',marker='*', label='All users')
                mplpl.plot(gt_set, abs_perc_mean, color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias', fontsize=18)
                mplpl.xlabel('Ground truth value (GTL)', fontsize=18)


                abs_perc_l_dict[data_name] = abs_perc_l
                abs_perc_mean_dict[data_name] = abs_perc_mean


                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_gt_abs_perception| alt text| width = 500px}} ||\n')

                # outF.write('|| Table ||\n\n')

            # exit
        ########################

                tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)
                pt_l = []
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_avg[t_id])

                mplpl.scatter(range(len(pt_l)), pt_l,  s=40,color='r',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PTL', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_pt| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=False)
                disputability_l = []
                for t_id in tweet_l_sort:
                    disputability_l.append(tweet_var[t_id])


                mplpl.scatter(range(len(pt_l)), disputability_l ,  s=40,color='g',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Disputability', fontsize=18)
                mplpl.xlabel('Ranked news stories according Disputability', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(disputability_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_disput'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_disput| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=False)
                perc_l = []
                for t_id in tweet_l_sort:
                    perc_l.append(tweet_dev_avg[t_id])

                mplpl.scatter(range(len(pt_l)), perc_l,  s=40,color='b',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias (PB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according PB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_percep| alt text| width = 500px}}')



                tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=False)
                perc_abs_l = []
                for t_id in tweet_l_sort:
                    perc_abs_l.append(tweet_abs_dev_avg[t_id])

                mplpl.scatter(range(len(perc_abs_l)), perc_abs_l,  s=40,color='c',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according APB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(perc_abs_l),4)))
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_abs-percep'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_abs-percep| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



        #####################################################33


                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='r', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')


                num_bins = len(disputability_l)
                counts, bin_edges = np.histogram(disputability_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Disputability', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_disput_cdf| alt text| width = 500px}}')


                num_bins = len(perc_l)
                counts, bin_edges = np.histogram(perc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='b', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception bias (PB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_percep_cdf| alt text| width = 500px}}')

                num_bins = len(perc_abs_l)
                counts, bin_edges = np.histogram(perc_abs_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 2])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_abs-percep_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_abs-percep_cdf| alt text| width = 500px}}||\n')

                # outF.write('|| Table ||\n\n')



                # mplpl.show()

                col_l = ['r', 'b', 'g']

                i = 0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=400, color=col_l[i], marker='*')
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i = 0
                mark_l = ['*', 'o', '^']
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([-2, 2])
                    mplpl.ylabel('Perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i+=1

                mplpl.legend(loc="upper right")

                mplpl.grid()
                # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()



                i=0
                for data_s in gt_l_dict.keys():

                    mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o')
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=300, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception_scatter'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                i=0
                for data_s in gt_l_dict.keys():

                    # mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                    mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                    mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                    mplpl.xlim([-1.2, 1.2])
                    mplpl.ylim([0, 2])
                    mplpl.ylabel('Absolute perception bias', fontsize=18)
                    mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                    i += 1

                mplpl.grid()
                mplpl.legend(loc="upper right")

                # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception'
                mplpl.savefig(pp, format='png')
                mplpl.figure()











                mplpl.show()

            else:

                AVG_list = []
                print(np.mean(all_acc))
                outF = open(remotedir + 'output.txt', 'w')

                tweet_all_var = {}
                tweet_all_dev_avg = {}
                tweet_all_avg = {}
                tweet_all_gt_var = {}
                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []
                tweet_all_avg_l = []
                tweet_all_med_l = []
                tweet_all_var_l = []
                tweet_all_gt_var_l = []
                diff_group_disp_l = []
                dem_disp_l = []
                rep_disp_l = []

                tweet_all_dev_avg = {}
                tweet_all_dev_med = {}
                tweet_all_dev_var = {}

                tweet_all_dev_avg_l = []
                tweet_all_dev_med_l = []
                tweet_all_dev_var_l = []

                tweet_all_abs_dev_avg = {}
                tweet_all_abs_dev_med = {}
                tweet_all_abs_dev_var = {}

                tweet_all_abs_dev_avg_l = []
                tweet_all_abs_dev_med_l = []
                tweet_all_abs_dev_var_l = []
                tweet_all_dev_avg_rnd = {}
                tweet_all_abs_dev_avg_rnd = {}

                diff_group_disp_dict = {}
                if dataset == 'snopes':
                    data_n = 'sp'
                    news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                    ind_l = [1, 2, 3]
                elif dataset == 'politifact':
                    data_n = 'pf'
                    news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                    ind_l = [1, 2, 3]
                elif dataset == 'mia':
                    data_n = 'mia'
                    news_cat_list = ['rumor', 'non-rumor']
                    ind_l = [1]

                for cat_l in news_cat_list:
                    outF.write('== ' + str(cat_l) + ' ==\n\n')
                    print('== ' + str(cat_l) + ' ==')
                    tweet_dev_avg = {}
                    tweet_dev_med = {}
                    tweet_dev_var = {}
                    tweet_abs_dev_avg = {}
                    tweet_abs_dev_med = {}
                    tweet_abs_dev_var = {}

                    tweet_avg = {}
                    tweet_med = {}
                    tweet_var = {}
                    tweet_gt_var = {}

                    tweet_dev_avg_rnd = {}
                    tweet_abs_dev_avg_rnd = {}


                    tweet_dev_avg_l = []
                    tweet_dev_med_l = []
                    tweet_dev_var_l = []
                    tweet_abs_dev_avg_l = []
                    tweet_abs_dev_med_l = []
                    tweet_abs_dev_var_l = []

                    tweet_avg_l = []
                    tweet_med_l = []
                    tweet_var_l = []
                    tweet_gt_var_l = []
                    AVG_susc_list = []
                    AVG_wl_list = []
                    all_acc = []
                    AVG_dev_list = []
                    # for lean in [-1, 0, 1]:

                        # AVG_susc_list = []
                        # AVG_wl_list = []
                        # all_acc = []
                        # df_m = df_m[df_m['leaning'] == lean]
                        # if lean == 0:
                        #     col = 'g'
                        #     lean_cat = 'neutral'
                        # elif lean == 1:
                        #     col = 'b'
                        #     lean_cat = 'democrat'
                        # elif lean == -1:
                        #     col = 'r'
                        #     lean_cat = 'republican'
                        # print(lean_cat)
                    for ind in ind_l:

                        if balance_f == 'balanced':
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
                        else:
                            inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'

                        inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
                        df[ind] = pd.read_csv(inp1, sep="\t")
                        df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                        df_m = df[ind].copy()
                        df_mm = df_m.copy()

                        df_m = df_m[df_m['ra_gt'] == cat_l]
                        # df_mm = df_m[df_m['ra_gt']==cat_l]
                        # df_m = df_m[df_m['leaning'] == lean]

                        groupby_ftr = 'tweet_id'
                        grouped = df_m.groupby(groupby_ftr, sort=False)
                        grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                        for t_id in grouped.groups.keys():
                            df_tmp = df_m[df_m['tweet_id'] == t_id]

                            df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
                            df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
                            df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []
                            df_tmp = df_m[df_m['tweet_id'] == t_id]
                            ind_t = df_tmp.index.tolist()[0]
                            weights = []

                            weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                            val_list = list(df_tmp['rel_v'])
                            tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_avg[t_id] = np.mean(val_list)
                            tweet_med[t_id] = np.median(val_list)
                            tweet_var[t_id] = np.var(val_list)
                            tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_avg_l.append(np.mean(val_list))
                            tweet_med_l.append(np.median(val_list))
                            tweet_var_l.append(np.var(val_list))
                            tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])




                            tweet_all_avg[t_id] = np.mean(val_list)
                            tweet_all_var[t_id] = np.var(val_list)
                            tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                            tweet_all_avg_l.append(np.mean(val_list))
                            tweet_all_med_l.append(np.median(val_list))
                            tweet_all_var_l.append(np.var(val_list))
                            tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])



                            val_list = list(df_tmp['err'])
                            abs_var_err = [np.abs(x) for x in val_list]
                            tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                            tweet_dev_avg[t_id] = np.mean(val_list)
                            tweet_dev_med[t_id] = np.median(val_list)
                            tweet_dev_var[t_id] = np.var(val_list)

                            tweet_dev_avg_l.append(np.mean(val_list))
                            tweet_dev_med_l.append(np.median(val_list))
                            tweet_dev_var_l.append(np.var(val_list))

                            tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_abs_dev_var_l.append(np.var(abs_var_err))


                            tweet_all_dev_avg[t_id] = np.mean(val_list)
                            tweet_all_dev_med[t_id] = np.median(val_list)
                            tweet_all_dev_var[t_id] = np.var(val_list)

                            tweet_all_dev_avg_l.append(np.mean(val_list))
                            tweet_all_dev_med_l.append(np.median(val_list))
                            tweet_all_dev_var_l.append(np.var(val_list))

                            tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
                            tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
                            tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)

                            tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
                            tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
                            tweet_all_abs_dev_var_l.append(np.var(abs_var_err))



                            sum_rnd_abs_perc = 0
                            sum_rnd_perc = 0
                            for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                                sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                                sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                            random_perc = np.abs(sum_rnd_perc / float(7))
                            random_abs_perc = sum_rnd_abs_perc / float(7)

                            tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                            tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                            tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                    gt_l = []
                    pt_l = []
                    disputability_l = []
                    perc_l = []
                    abs_perc_l = []
                    # for t_id in tweet_l_sort:
                    #     gt_l.append(tweet_gt_var[t_id])
                    #     pt_l.append(tweet_avg[t_id])
                    #     disputability_l.append(tweet_var[t_id])
                    #     perc_l.append(tweet_dev_avg[t_id])
                    #     abs_perc_l.append(tweet_abs_dev_avg[t_id])



                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
                    tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
                    # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)

                    # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)


                    if dataset == 'snopes':
                        data_addr = 'snopes'
                    elif dataset == 'politifact':
                        data_addr = 'politifact/fig'
                    elif dataset == 'mia':
                        data_addr = 'mia/fig'

                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')

                    for t_id in tweet_l_sort:
                        count+=1
                        if balance_f=='balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
                                       + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
                                       # str(np.round(diff_group_disp_dict[t_id], 3)) +
                                       '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
                                        + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
                                        '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')




                if dataset == 'snopes':
                    data_addr = 'snopes'
                elif dataset == 'politifact':
                    data_addr = 'politifact/fig'
                elif dataset == 'mia':
                    data_addr = 'mia/fig'

                # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
                tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)

                # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)

                tweet_napb_dict_high_disp = {}
                tweet_napb_dict_low_disp = {}
                for t_id in tweet_l_sort[:20]:
                    # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                for t_id in tweet_l_sort[-20:]:
                    # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                    tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                kk = 0

                for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
                    if kk==0:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
                    else:
                        tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)

                    kk+=1
                    count = 0
                    outF.write(
                        '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                    for t_id in tweet_l_sort:
                        count += 1
                        # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
                        if balance_f == 'balanced':
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                       + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
                                       str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                            # +
                            #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
                            #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

                        else:
                            outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                                       str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
                                       str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                       str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                       '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                       str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                        # +
                        # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                        # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_stacked_bar_true-false_fig":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']
                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)

                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)


                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

            # news_cat_list_f = ['pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['false', 'mostly_false', 'mixture', 'mostly_true', 'true']

            if dataset=='snopes':
                col_l = ['red', 'magenta', 'gray', 'lime', 'green']
                col = 'purple'
                news_cat_list_n = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']
            if dataset=='politifact':
                col_l = ['darkred', 'red', 'magenta', 'gray', 'lime', 'green']
                news_cat_list_n = ['PANTS ON FIRE', 'FALSE', 'MOSTLY FALSE', 'HALF TRUE', 'MOSTLY TRUE', 'TRUE']
                col = 'c'
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            if dataset=='mia':
                col_l = ['red', 'green']
                news_cat_list_n = ['RUMORS', 'NON RUMORS']
                col = 'brown'
                col_t_f = ['red', 'green']
                news_cat_list_t_f = [['rumors'], ['non-rumors']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            count = 0
            # Y = [0]*len(thr_list)




            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []

            # fig_cdf = True
            fig_cdf = False
            if fig_cdf==True:

        #####################################################33



                tweet_vote_sort = sorted(tweet_vote_avg, key=tweet_vote_avg.get, reverse=False)
                thr = 10
                thr_list = []
                categ_dict = collections.defaultdict(int)
                for i in range(int(len(tweet_vote_sort) / float(thr))):
                    k = (i + 1) * thr
                    thr_list.append(k)
                    perc_rnd_l = []
                    abs_perc_rnd_l = []
                    disputability_l = []
                    above_avg = 0
                    less_avg = 0
                    above_avg_rnd = 0
                    less_avg_rnd = 0
                    above_avg = 0
                    less_avg = 0
                    categ_dict[k] = collections.defaultdict(float)
                    for j in range(k):
                        for cat_n in news_cat_list:
                            if tweet_lable_dict[tweet_vote_sort[j]] == cat_n:
                                categ_dict[k][cat_n] += 1 / float(k)

                width = 0.02
                pr = -10
                title_l = news_cat_list
                outp = {}
                # news_cat_list = ['pants-fire', 'false', 'mostly_false', 'half-true', 'mostly-true', 'true']
                # news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                if dataset == 'snopes':
                    col_l = ['b', 'g', 'c', 'y', 'r']
                    news_cat_list_n = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                if dataset == 'politifact':
                    col_l = ['grey', 'b', 'g', 'c', 'y', 'r']
                    news_cat_list_n = ['PANTS ON FIRE', 'FALSE', 'MOSTLY FALSE', 'HALF TRUE', 'MOSTLY TRUE', 'TRUE']

                if dataset == 'mia':
                    col_l = ['b', 'r']
                    news_cat_list_n = ['RUMORS', 'NON RUMORS']
                count = 0
                Y = [0] * len(thr_list)
                for cat_m in news_cat_list:
                    count += 1
                    outp[cat_m] = []
                    for i in thr_list:
                        outp[cat_m].append(categ_dict[i][cat_m])
                    mplpl.bar([xx / float(len(tweet_vote_sort)) for xx in thr_list], outp[cat_m], width, bottom=np.array(Y),
                              color=col_l[count - 1], label=news_cat_list_n[count - 1])
                    Y = np.array(Y) + np.array(outp[cat_m])

                    tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)





                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_avg[t_id])
                count=0

                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5,linestyle='--', label='All news stories')

                for cat_m in news_cat_list:
                    count += 1
                    pt_l_dict[cat_m] = []
                    for t_id in tweet_l_sort:
                        if tweet_lable_dict[t_id]==cat_m:
                            pt_l_dict[cat_m].append(tweet_avg[t_id])


                    num_bins = len(pt_l_dict[cat_m])
                    counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_l[count-1], lw=5, label=cat_m)
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perceived truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                mplpl.legend(loc="lower right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()




                tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_var[t_id])
                count = 0

                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, linestyle='--', label='All news stories')

                for cat_m in news_cat_list:
                    count += 1
                    pt_l_dict[cat_m] = []
                    for t_id in tweet_l_sort:
                        if tweet_lable_dict[t_id] == cat_m:
                            pt_l_dict[cat_m].append(tweet_var[t_id])

                    num_bins = len(pt_l_dict[cat_m])
                    counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_l[count - 1], lw=5, label=cat_m)
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Disputability', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                mplpl.legend(loc="lower right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_diput_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()  # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_abs_dev_avg_rnd[t_id])
                count = 0

                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, linestyle='--', label='All news stories')

                for cat_m in news_cat_list:
                    count += 1
                    pt_l_dict[cat_m] = []
                    for t_id in tweet_l_sort:
                        if tweet_lable_dict[t_id] == cat_m:
                            pt_l_dict[cat_m].append(tweet_abs_dev_avg_rnd[t_id])

                    num_bins = len(pt_l_dict[cat_m])
                    counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_l[count - 1], lw=5, label=cat_m)
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Normalized absolute perception bias', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                mplpl.legend(loc="lower right")
                mplpl.xlim([0, 2])
                mplpl.ylim([0, 1])
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_NAPB_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()  # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')






                tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_avg[t_id])
                count = 0

                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, linestyle='--', label='All news stories')

                for cat_m_l in news_cat_list_t_f:
                    count += 1
                    pt_l_dict[count] = []
                    for cat_m in cat_m_l:
                        for t_id in tweet_l_sort:
                            if tweet_lable_dict[t_id] == cat_m:
                                pt_l_dict[count].append(tweet_avg[t_id])

                    num_bins = len(pt_l_dict[count])
                    counts, bin_edges = np.histogram(pt_l_dict[count], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_t_f[count - 1], lw=5, label=cat_list_t_f[count-1])
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perceived truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                mplpl.legend(loc="lower right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_true-false_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()


                tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_var[t_id])
                count = 0

                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, linestyle='--', label='All news stories')

                for cat_m_l in news_cat_list_t_f:
                    count += 1
                    pt_l_dict[count] = []
                    for cat_m in cat_m_l:
                        for t_id in tweet_l_sort:
                            if tweet_lable_dict[t_id] == cat_m:
                                pt_l_dict[count].append(tweet_var[t_id])

                    num_bins = len(pt_l_dict[count])
                    counts, bin_edges = np.histogram(pt_l_dict[count], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_t_f[count - 1], lw=5, label=cat_list_t_f[count-1])
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Disputability', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                mplpl.legend(loc="lower right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_true-false_diput_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()  # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')

                tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_abs_dev_avg_rnd[t_id])
                count = 0

                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, linestyle='--', label='All news stories')
                for cat_m_l in news_cat_list_t_f:
                    count += 1
                    pt_l_dict[count] = []
                    for cat_m in cat_m_l:
                        for t_id in tweet_l_sort:
                            if tweet_lable_dict[t_id] == cat_m:
                                pt_l_dict[count].append(tweet_abs_dev_avg_rnd[t_id])

                    num_bins = len(pt_l_dict[count])
                    counts, bin_edges = np.histogram(pt_l_dict[count], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_t_f[count - 1], lw=5, label=cat_list_t_f[count - 1])
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Normalized absolute perception bias', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                mplpl.legend(loc="lower right")
                mplpl.xlim([0, 2])
                mplpl.ylim([0, 1])
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_true-false_NAPB_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()  # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')


    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_CDF_fig":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []




        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']
                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)

                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)


                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

            # news_cat_list_f = ['pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['false', 'mostly_false', 'mixture', 'mostly_true', 'true']

            if dataset=='snopes':
                col_l = ['darkred', 'orange', 'gray', 'lime', 'green']
                # col = 'purple'
                col = 'k'
                news_cat_list_n = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']
            if dataset=='politifact':
                col_l = ['darkred', 'red', 'orange', 'gray', 'lime', 'green']
                news_cat_list_n = ['PANTS ON FIRE', 'FALSE', 'MOSTLY FALSE', 'HALF TRUE', 'MOSTLY TRUE', 'TRUE']
                # col = 'c'
                col = 'k'
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            if dataset=='mia':
                col_l = ['red', 'green']
                news_cat_list_n = ['RUMORS', 'NON RUMORS']
                # col = 'brown'
                col = 'k'
                col_t_f = ['red', 'green']
                news_cat_list_t_f = [['rumors'], ['non-rumors']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            count = 0
            # Y = [0]*len(thr_list)




            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []

            fig_cdf = True
            # fig_cdf = False
            if fig_cdf==True:

        #####################################################33

                tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_dev_avg[t_id])
                count=0

                # num_bins = len(pt_l)
                # counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5,linestyle='--', label='All news stories')
                mplpl.rcParams['figure.figsize'] = 5.4, 3.2
                mplpl.rc('xtick', labelsize='xx-large')
                mplpl.rc('ytick', labelsize='xx-large')
                mplpl.rc('legend', fontsize='xx-large')
                for cat_m in news_cat_list:
                    count += 1
                    pt_l_dict[cat_m] = []
                    for t_id in tweet_l_sort:
                        if tweet_lable_dict[t_id]==cat_m:
                            pt_l_dict[cat_m].append(tweet_dev_avg[t_id])


                    num_bins = len(pt_l_dict[cat_m])
                    counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_l[count-1], lw=5, label=cat_m)


                mplpl.ylabel('CDF', fontsize=28)
                mplpl.xlabel('Mean pereption bias', fontsize=28)
                mplpl.grid()
                mplpl.title(data_name, fontsize='xx-large')
                mplpl.legend(loc="lower right",fontsize='x-large')
                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 1])
                mplpl.subplots_adjust(bottom=0.14)

                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_mpb_cdf.pdf'
                mplpl.savefig(pp, format='pdf')
                mplpl.figure()


                exit()



        #         mplpl.rcParams['figure.figsize'] = 5.4, 3.2
        #         mplpl.rc('xtick', labelsize='x-large')
        #         mplpl.rc('ytick', labelsize='x-large')
        #         mplpl.rc('legend', fontsize='x-large')
        #         w_err_avg_dict
        #         # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
        #         tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
        #         acc_l = []
        #         for t_id in tweet_l_sort:
        #             acc_l.append(w_err_avg_dict[t_id])
        #
        #         num_bins = len(acc_l)
        #         counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         cdf = np.cumsum(counts)
        #         scale = 1.0 / cdf[-1]
        #         ncdf = scale * cdf
        #         mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)
        #
        #
        #
        # #
        #         mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        #         mplpl.xlabel('Mean perception bias', fontsize=20, fontweight = 'bold')
        #         # mplpl.title(data_name)
        #         mplpl.legend(loc="upper left",fontsize = 'large')
        #         mplpl.xlim([-1.5, 1.5])
        #         mplpl.ylim([0, 1])
        #         mplpl.grid()
        #         mplpl.subplots_adjust(bottom=0.24)
        #         mplpl.subplots_adjust(left=0.18)
        #         # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        #         pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/MPB_cdf_alldataset'
        #         mplpl.savefig(pp + '.pdf', format='pdf')
        #         mplpl.savefig(pp + '.png', format='png')



                tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_var[t_id])
                count = 0

                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, linestyle='--', label='All news stories')

                for cat_m in news_cat_list:
                    count += 1
                    pt_l_dict[cat_m] = []
                    for t_id in tweet_l_sort:
                        if tweet_lable_dict[t_id] == cat_m:
                            pt_l_dict[cat_m].append(tweet_var[t_id])

                    num_bins = len(pt_l_dict[cat_m])
                    counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_l[count - 1], lw=5, label=cat_m)
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Disputability', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                mplpl.legend(loc="lower right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_diput_cdf.pdf'
                mplpl.savefig(pp, format='pdf')
                mplpl.figure()  # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')


                tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_abs_dev_avg_rnd[t_id])
                count = 0

                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, linestyle='--', label='All news stories')

                for cat_m in news_cat_list:
                    count += 1
                    pt_l_dict[cat_m] = []
                    for t_id in tweet_l_sort:
                        if tweet_lable_dict[t_id] == cat_m:
                            pt_l_dict[cat_m].append(tweet_abs_dev_avg_rnd[t_id])

                    num_bins = len(pt_l_dict[cat_m])
                    counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_l[count - 1], lw=5, label=cat_m)
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Normalized absolute perception bias', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                mplpl.legend(loc="lower right")
                mplpl.xlim([0, 2])
                mplpl.ylim([0, 1])
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_NAPB_cdf.pdf'
                mplpl.savefig(pp, format='pdf')
                mplpl.figure()  # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')






                tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_avg[t_id])
                count = 0

                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, linestyle='--', label='All news stories')

                for cat_m_l in news_cat_list_t_f:
                    count += 1
                    pt_l_dict[count] = []
                    for cat_m in cat_m_l:
                        for t_id in tweet_l_sort:
                            if tweet_lable_dict[t_id] == cat_m:
                                pt_l_dict[count].append(tweet_avg[t_id])

                    num_bins = len(pt_l_dict[count])
                    counts, bin_edges = np.histogram(pt_l_dict[count], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_t_f[count - 1], lw=5, label=cat_list_t_f[count-1])
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perceived truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                mplpl.legend(loc="lower right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_true-false_pt_cdf.pdf'
                mplpl.savefig(pp, format='pdf')
                mplpl.figure()


                tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_var[t_id])
                count = 0

                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, linestyle='--', label='All news stories')

                for cat_m_l in news_cat_list_t_f:
                    count += 1
                    pt_l_dict[count] = []
                    for cat_m in cat_m_l:
                        for t_id in tweet_l_sort:
                            if tweet_lable_dict[t_id] == cat_m:
                                pt_l_dict[count].append(tweet_var[t_id])

                    num_bins = len(pt_l_dict[count])
                    counts, bin_edges = np.histogram(pt_l_dict[count], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_t_f[count - 1], lw=5, label=cat_list_t_f[count-1])
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Disputability', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                mplpl.legend(loc="lower right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_true-false_diput_cdf.pdf'
                mplpl.savefig(pp, format='pdf')
                mplpl.figure()  # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')

                tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(tweet_abs_dev_avg_rnd[t_id])
                count = 0

                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, linestyle='--', label='All news stories')
                for cat_m_l in news_cat_list_t_f:
                    count += 1
                    pt_l_dict[count] = []
                    for cat_m in cat_m_l:
                        for t_id in tweet_l_sort:
                            if tweet_lable_dict[t_id] == cat_m:
                                pt_l_dict[count].append(tweet_abs_dev_avg_rnd[t_id])

                    num_bins = len(pt_l_dict[count])
                    counts, bin_edges = np.histogram(pt_l_dict[count], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_t_f[count - 1], lw=5, label=cat_list_t_f[count - 1])
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Normalized absolute perception bias', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                mplpl.legend(loc="lower right")
                mplpl.xlim([0, 2])
                mplpl.ylim([0, 1])
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_true-false_NAPB_cdf.pdf'
                mplpl.savefig(pp, format='pdf')
                mplpl.figure()  # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/initial/'+ data_n + '_pt_cdf| alt text| width = 500px}}')

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_PDF_CDF_MPB_fig":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        # dataset = 'snopes'
        dataset = 'snopes_nonpol'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes_nonpol':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable

        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes_nonpol']:#['snopes','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']
                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            if dataset=='snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpol'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)

                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)


                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

            # news_cat_list_f = ['pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['false', 'mostly_false', 'mixture', 'mostly_true', 'true']

            if dataset=='snopes' or dataset=='snopes_nonpol':
                col_l = ['darkred', 'orange', 'gray', 'lime', 'green']
                # col = 'purple'
                col = 'k'
                news_cat_list_n = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']
            if dataset=='politifact':
                col_l = ['darkred', 'red', 'orange', 'gray', 'lime', 'green']
                news_cat_list_n = ['PANTS ON FIRE', 'FALSE', 'MOSTLY FALSE', 'HALF TRUE', 'MOSTLY TRUE', 'TRUE']
                # col = 'c'
                col = 'k'
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            if dataset=='mia':
                col_l = ['red', 'green']
                news_cat_list_n = ['RUMORS', 'NON RUMORS']
                # col = 'brown'
                col = 'k'
                col_t_f = ['red', 'green']
                news_cat_list_t_f = [['rumors'], ['non-rumors']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            count = 0
            # Y = [0]*len(thr_list)




            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []

            fig_cdf = True
            # fig_cdf = False
            if fig_cdf==True:

        #####################################################33
                out_dict = tweet_dev_avg
                # out_dict = tweet_avg
                tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(out_dict[t_id])
                count=0

                # num_bins = len(pt_l)
                # counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5,linestyle='--', label='All news stories')
                mplpl.rcParams['figure.figsize'] = 5.8, 3.7
                mplpl.rcParams['figure.figsize'] =7, 5.3
                mplpl.rc('xtick', labelsize='large')
                mplpl.rc('ytick', labelsize='large')
                mplpl.rc('legend', fontsize='small')
                for cat_m in news_cat_list:
                    count += 1
                    pt_l_dict[cat_m] = []
                    for t_id in tweet_l_sort:
                        if tweet_lable_dict[t_id]==cat_m:
                            pt_l_dict[cat_m].append(out_dict[t_id])



                    df_tt = pd.DataFrame(np.array(pt_l_dict[cat_m]), columns=[cat_m])
                    df_tt[cat_m].plot(kind='kde', lw=6, color=col_l[count-1], label=cat_m)
                    print(cat_m +  ' : '+ str(np.min(list(df_tt[cat_m]))))
                    print(cat_m +  ' : '+ str(np.max(list(df_tt[cat_m]))))
                    # print(cat_m +  ' : '+ str(len(df_tt[df_tt[cat_m]>=0.5])))
                    # print(cat_m +  ' : '+ str(len(df_tt[df_tt[cat_m]<=-0.5])))
                    # num_bins = len(pt_l_dict[cat_m])
                    # counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
                    # cdf = np.cumsum(counts)
                    # scale = 1.0 / cdf[-1]
                    # ncdf = scale * cdf
                    # mplpl.plot(bin_edges[1:], ncdf, c=col_l[count-1], lw=5, label=cat_m)
                # exit()
                mplpl.ylabel('PDF', fontsize=22, fontweight = 'bold')
                # mplpl.ylabel('CDF', fontsize=22, fontweight = 'bold')
                # mplpl.xlabel('Perceived Truth Level', fontsize=22, fontweight = 'bold')
                mplpl.xlabel('Mean Perception Bias', fontsize=22, fontweight = 'bold')
                mplpl.grid()
                mplpl.title(data_name, fontsize='x-large')
                labels = ['0.0', '0.25', '0.5', '0.75', '1.0']
                y = [0.0, 0.5, 1, 1.5, 2]
                mplpl.yticks(y, labels)
                legend_properties = {'weight': 'bold'}


                # plt.legend(prop=legend_properties)
                mplpl.legend(loc="upper right",prop=legend_properties,fontsize='small', ncol=1)#, fontweight = 'bold')
                mplpl.xlim([-2, 2])
                mplpl.ylim([0, 2])
                # mplpl.xlim([-1, 1])
                # mplpl.ylim([0, 1])
                mplpl.subplots_adjust(bottom=0.24)
                mplpl.subplots_adjust(left=0.18)

                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_mpb_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_pt_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_pt_cdf'
                mplpl.savefig(pp + '.pdf', format='pdf')
                mplpl.savefig(pp + '.png', format='png')
                mplpl.figure()


                exit()



        #         mplpl.rcParams['figure.figsize'] = 5.4, 3.2
        #         mplpl.rc('xtick', labelsize='x-large')
        #         mplpl.rc('ytick', labelsize='x-large')
        #         mplpl.rc('legend', fontsize='x-large')
        #         w_err_avg_dict
        #         # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
        #         tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
        #         acc_l = []
        #         for t_id in tweet_l_sort:
        #             acc_l.append(w_err_avg_dict[t_id])
        #
        #         num_bins = len(acc_l)
        #         counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         cdf = np.cumsum(counts)
        #         scale = 1.0 / cdf[-1]
        #         ncdf = scale * cdf
        #         mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)
        #
        #
        #
        # #
        #         mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        #         mplpl.xlabel('Mean perception bias', fontsize=20, fontweight = 'bold')
        #         # mplpl.title(data_name)
        #         mplpl.legend(loc="upper left",fontsize = 'large')
        #         mplpl.xlim([-1.5, 1.5])
        #         mplpl.ylim([0, 1])
        #         mplpl.grid()
        #         mplpl.subplots_adjust(bottom=0.24)
        #         mplpl.subplots_adjust(left=0.18)
        #         # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        #         pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/MPB_cdf_alldataset'
        #         mplpl.savefig(pp + '.pdf', format='pdf')
        #         mplpl.savefig(pp + '.png', format='png')

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_PDF_CDF_APB_fig":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']
                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            w_cyn_dict= collections.defaultdict()
            w_gull_dict= collections.defaultdict()
            w_apb_dict= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)

                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)



                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    # w_abs_err_list = list(df_tmp['abs_err'])
                    w_sus_list = list(df_tmp['susc'])
                    # w_norm_err_list = list(df_tmp['norm_err'])
                    # w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    # w_cyn_list = list(df_tmp['cyn'])
                    # w_gull_list = list(df_tmp['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])


                    df_cyn = df_tmp[df_tmp['cyn']>0]
                    df_gull = df_tmp[df_tmp['gull']>0]

                    w_cyn_list = list(df_cyn['cyn'])
                    w_gull_list = list(df_gull['gull'])

                    w_cyn_dict[t_id] = np.mean(w_cyn_list)
                    w_gull_dict[t_id] = np.mean(w_gull_list)
                    w_apb_dict[t_id] = np.mean(w_sus_list)


                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

            # news_cat_list_f = ['pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['false', 'mostly_false', 'mixture', 'mostly_true', 'true']

            if dataset=='snopes':
                col_l = ['darkred', 'orange', 'gray', 'lime', 'green']
                # col = 'purple'
                col = 'k'
                news_cat_list_n = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']
            if dataset=='politifact':
                col_l = ['darkred', 'red', 'orange', 'gray', 'lime', 'green']
                news_cat_list_n = ['PANTS ON FIRE', 'FALSE', 'MOSTLY FALSE', 'HALF TRUE', 'MOSTLY TRUE', 'TRUE']
                # col = 'c'
                col = 'k'
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            if dataset=='mia':
                col_l = ['red', 'green']
                news_cat_list_n = ['RUMORS', 'NON RUMORS']
                # col = 'brown'
                col = 'k'
                col_t_f = ['red', 'green']
                news_cat_list_t_f = [['rumors'], ['non-rumors']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            count = 0
            # Y = [0]*len(thr_list)




            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []

            fig_cdf = True
            # fig_cdf = False
            if fig_cdf==True:

        #####################################################33

                # out_dict = w_apb_dict
                # out_dict = w_gull_dict
                out_dict = w_cyn_dict


                tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(out_dict[t_id])
                count=0

                # num_bins = len(pt_l)
                # counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5,linestyle='--', label='All news stories')
                # mplpl.rcParams['figure.figsize'] = 5.8, 3.7
                mplpl.rcParams['figure.figsize'] = 5.6, 3.4
                mplpl.rc('xtick', labelsize='x-large')
                mplpl.rc('ytick', labelsize='x-large')
                mplpl.rc('legend', fontsize='medium')
                for cat_m in news_cat_list:
                    count += 1
                    pt_l_dict[cat_m] = []
                    for t_id in tweet_l_sort:
                        if tweet_lable_dict[t_id]==cat_m:
                            if out_dict[t_id]>0 or out_dict[t_id]<=0:
                                pt_l_dict[cat_m].append(out_dict[t_id])



                    # df_tt = pd.DataFrame(np.array(pt_l_dict[cat_m]), columns=[cat_m])
                    # df_tt[cat_m].plot(kind='kde', lw=6, color=col_l[count-1], label=cat_m)

                    num_bins = len(pt_l_dict[cat_m])
                    counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_l[count-1], lw=5, label=cat_m)

                # mplpl.ylabel('PDF of Perceived Truth Level', fontsize=20, fontweight = 'bold')
                # mplpl.ylabel('CDF of \n Absolute Perception Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('CDF of \n False Positive Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('CDF of \n False Negative Bias', fontsize=13, fontweight = 'bold')

                # mplpl.ylabel('PDF of \n Absolute Perception Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('PDF of \n False Positive Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('PDF of \n False Negative Bias', fontsize=13, fontweight='bold')

                # mplpl.xlabel('Total Perception Bias', fontsize=24, fontweight = 'bold')
                # mplpl.xlabel('False Positive Bias', fontsize=24, fontweight = 'bold')
                mplpl.xlabel('False Negative Bias', fontsize=24, fontweight = 'bold')
                mplpl.ylabel('CDF', fontsize=24, fontweight = 'bold')
                mplpl.grid()
                mplpl.title(data_name, fontsize='x-large')
                labels = ['0.0', '0.25', '0.5', '0.75', '1.0']
                y = [0.0, 0.5, 1, 1.5, 2]
                # mplpl.yticks(y, labels)
                legend_properties = {'weight': 'bold'}


                # plt.legend(prop=legend_properties)
                # mplpl.legend(loc="upper left",prop=legend_properties,fontsize='small', ncol=1)#, fontweight = 'bold')
                mplpl.legend(loc="lower right",prop=legend_properties,fontsize='medium', ncol=1)#, fontweight = 'bold')
                mplpl.xlim([0, 2])
                # mplpl.ylim([0, 2])
                mplpl.ylim([0, 1])
                mplpl.subplots_adjust(bottom=0.24)
                mplpl.subplots_adjust(left=0.18)

                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_pt_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_APB_cdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_FPB_cdf'
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_FNB_cdf'

                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_APB_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_FPB_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_FNB_pdf'

                mplpl.savefig(pp + '.pdf', format='pdf')
                mplpl.savefig(pp + '.png', format='png')
                mplpl.figure()


                exit()

        #         mplpl.rcParams['figure.figsize'] = 4.5, 2.5
        #         mplpl.rc('xtick', labelsize='large')
        #         mplpl.rc('ytick', labelsize='large')
        #         mplpl.rc('legend', fontsize='medium')
        #         w_err_avg_dict
        #         # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
        #         tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
        #         # tweet_l_sort = [x for x in tweet_l_sort if x >= 0 or x < 0]
        #         acc_l = []
        #         for t_id in tweet_l_sort:
        #             if out_dict[t_id] >=0 or out_dict[t_id]<0:
        #                 acc_l.append(out_dict[t_id])
        #
        #         num_bins = len(acc_l)
        #         counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         cdf = np.cumsum(counts)
        #         scale = 1.0 / cdf[-1]
        #         ncdf = scale * cdf
        #         mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)
        #
        # legend_properties = {'weight': 'bold'}
        #
        # #
        # mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        # mplpl.xlabel('Total Perception Bias', fontsize=20, fontweight = 'bold')
        # # mplpl.xlabel('False Positive Bias', fontsize=20, fontweight = 'bold')
        # # mplpl.xlabel('False Negative Bias', fontsize=20, fontweight = 'bold')
        # mplpl.legend(loc="lower right", prop=legend_properties, fontsize='medium', ncol=1)
        # # mplpl.title(data_name)
        # # mplpl.legend(loc="upper left",fontsize = 'large')
        # mplpl.xlim([0, 2])
        # mplpl.ylim([0, 1])
        # mplpl.grid()
        # mplpl.subplots_adjust(bottom=0.24)
        # mplpl.subplots_adjust(left=0.18)

        #         mplpl.rcParams['figure.figsize'] = 5.4, 3.2
        #         mplpl.rc('xtick', labelsize='x-large')
        #         mplpl.rc('ytick', labelsize='x-large')
        #         mplpl.rc('legend', fontsize='x-large')
        #         w_err_avg_dict
        #         # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
        #         tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
        #         acc_l = []
        #         for t_id in tweet_l_sort:
        #             acc_l.append(w_err_avg_dict[t_id])
        #
        #         num_bins = len(acc_l)
        #         counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         cdf = np.cumsum(counts)
        #         scale = 1.0 / cdf[-1]
        #         ncdf = scale * cdf
        #         mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)
        #
        #
        #
        # #
        #         mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        #         mplpl.xlabel('Mean perception bias', fontsize=20, fontweight = 'bold')
        #         # mplpl.title(data_name)
        #         mplpl.legend(loc="upper left",fontsize = 'large')
        #         mplpl.xlim([-1.5, 1.5])
        #         mplpl.ylim([0, 1])
        #         mplpl.grid()
        #         mplpl.subplots_adjust(bottom=0.24)
        #         mplpl.subplots_adjust(left=0.18)
        #         # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        #         pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/MPB_cdf_alldataset'
        #         mplpl.savefig(pp + '.pdf', format='pdf')
        #         mplpl.savefig(pp + '.png', format='png')

    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_PDF_CDF_disp_fig":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes_nonpol','snopes','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']
                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            if dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpol'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            w_cyn_dict= collections.defaultdict()
            w_gull_dict= collections.defaultdict()
            w_apb_dict= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)

                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)



                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    # w_abs_err_list = list(df_tmp['abs_err'])
                    w_sus_list = list(df_tmp['susc'])
                    # w_norm_err_list = list(df_tmp['norm_err'])
                    # w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    # w_cyn_list = list(df_tmp['cyn'])
                    # w_gull_list = list(df_tmp['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])


                    df_cyn = df_tmp[df_tmp['cyn']>0]
                    df_gull = df_tmp[df_tmp['gull']>0]

                    w_cyn_list = list(df_cyn['cyn'])
                    w_gull_list = list(df_gull['gull'])

                    w_cyn_dict[t_id] = np.mean(w_cyn_list)
                    w_gull_dict[t_id] = np.mean(w_gull_list)
                    w_apb_dict[t_id] = np.mean(w_sus_list)


                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

            # news_cat_list_f = ['pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['false', 'mostly_false', 'mixture', 'mostly_true', 'true']

            if dataset=='snopes' or dataset=='snopes_nonpol':
                col_l = ['darkred', 'orange', 'gray', 'lime', 'green']
                # col = 'purple'
                col = 'k'
                news_cat_list_n = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']
            if dataset=='politifact':
                col_l = ['darkred', 'red', 'orange', 'gray', 'lime', 'green']
                news_cat_list_n = ['PANTS ON FIRE', 'FALSE', 'MOSTLY FALSE', 'HALF TRUE', 'MOSTLY TRUE', 'TRUE']
                # col = 'c'
                col = 'k'
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            if dataset=='mia':
                col_l = ['red', 'green']
                news_cat_list_n = ['RUMORS', 'NON RUMORS']
                # col = 'brown'
                col = 'k'
                col_t_f = ['red', 'green']
                news_cat_list_t_f = [['rumors'], ['non-rumors']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            count = 0
            # Y = [0]*len(thr_list)




            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []

            fig_cdf = True
            # fig_cdf = False
            if fig_cdf==True:

        #####################################################33

                # out_dict = w_apb_dict
                # out_dict = w_gull_dict
                # out_dict = w_cyn_dict
                out_dict = tweet_var

                tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
                pt_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(out_dict[t_id])
                count=0

                # num_bins = len(pt_l)
                # counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5,linestyle='--', label='All news stories')
                # mplpl.rcParams['figure.figsize'] = 5.8, 3.7
                mplpl.rcParams['figure.figsize'] = 4.5, 2.5
                mplpl.rc('xtick', labelsize='large')
                mplpl.rc('ytick', labelsize='large')
                mplpl.rc('legend', fontsize='small')
                for cat_m in news_cat_list:
                    count += 1
                    pt_l_dict[cat_m] = []
                    for t_id in tweet_l_sort:
                        if tweet_lable_dict[t_id]==cat_m:
                            if out_dict[t_id]>0 or out_dict[t_id]<=0:
                                pt_l_dict[cat_m].append(out_dict[t_id])



                    # df_tt = pd.DataFrame(np.array(pt_l_dict[cat_m]), columns=[cat_m])
                    # df_tt[cat_m].plot(kind='kde', lw=6, color=col_l[count-1], label=cat_m)

                    num_bins = len(pt_l_dict[cat_m])
                    counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
                    cdf = np.cumsum(counts)
                    scale = 1.0 / cdf[-1]
                    ncdf = scale * cdf
                    mplpl.plot(bin_edges[1:], ncdf, c=col_l[count-1], lw=5, label=cat_m)

                # mplpl.ylabel('PDF of Perceived Truth Level', fontsize=20, fontweight = 'bold')
                # mplpl.ylabel('CDF of \n Absolute Perception Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('CDF of \n False Positive Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('CDF of \n False Negative Bias', fontsize=13, fontweight = 'bold')

                # mplpl.ylabel('PDF of \n Absolute Perception Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('PDF of \n False Positive Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('PDF of \n False Negative Bias', fontsize=13, fontweight='bold')

                # mplpl.xlabel('Total Perception Bias', fontsize=24, fontweight = 'bold')
                # mplpl.xlabel('False Positive Bias', fontsize=24, fontweight = 'bold')
                mplpl.xlabel('Disputability', fontsize=20, fontweight = 'bold')
                mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
                mplpl.grid()
                mplpl.title(data_name, fontsize='x-large')
                labels = ['0.0', '0.25', '0.5', '0.75', '1.0']
                y = [0.0, 0.5, 1, 1.5, 2]
                # mplpl.yticks(y, labels)
                legend_properties = {'weight': 'bold'}


                # plt.legend(prop=legend_properties)
                # mplpl.legend(loc="upper left",prop=legend_properties,fontsize='small', ncol=1)#, fontweight = 'bold')
                mplpl.legend(loc="lower right",prop=legend_properties,fontsize='small', ncol=1)#, fontweight = 'bold')
                mplpl.xlim([0, 1])
                # mplpl.ylim([0, 2])
                mplpl.ylim([0, 1])
                mplpl.subplots_adjust(bottom=0.24)
                mplpl.subplots_adjust(left=0.18)

                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_pt_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_APB_cdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_FPB_cdf'
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_disput_cdf'

                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_APB_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_FPB_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_FNB_pdf'

                mplpl.savefig(pp + '.pdf', format='pdf')
                mplpl.savefig(pp + '.png', format='png')
                mplpl.figure()


                exit()



        # mplpl.rcParams['figure.figsize'] = 4.5, 2.5
        # mplpl.rc('xtick', labelsize='large')
        # mplpl.rc('ytick', labelsize='large')
        # mplpl.rc('legend', fontsize='medium')
        # mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        # mplpl.xlabel('Disputability', fontsize=20, fontweight = 'bold')
        # mplpl.legend(loc="lower right", prop=legend_properties, fontsize='medium', ncol=1)
        # # mplpl.title(data_name)
        # # mplpl.legend(loc="upper left",fontsize = 'large')
        # mplpl.xlim([0, 1])
        # mplpl.ylim([0, 1])
        # mplpl.grid()
        # mplpl.subplots_adjust(bottom=0.24)
        # mplpl.subplots_adjust(left=0.18)


        #         mplpl.rcParams['figure.figsize'] = 4.5, 2.5
        #         mplpl.rc('xtick', labelsize='large')
        #         mplpl.rc('ytick', labelsize='large')
        #         mplpl.rc('legend', fontsize='medium')
        #         w_err_avg_dict
        #         # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
        #         tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
        #         # tweet_l_sort = [x for x in tweet_l_sort if x >= 0 or x < 0]
        #         acc_l = []
        #         for t_id in tweet_l_sort:
        #             if out_dict[t_id] >=0 or out_dict[t_id]<0:
        #                 acc_l.append(out_dict[t_id])
        #
        #         num_bins = len(acc_l)
        #         counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         cdf = np.cumsum(counts)
        #         scale = 1.0 / cdf[-1]
        #         ncdf = scale * cdf
        #         mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)
        #
        # legend_properties = {'weight': 'bold'}
        #
        # #
        # mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        # mplpl.xlabel('Total Perception Bias', fontsize=20, fontweight = 'bold')
        # # mplpl.xlabel('False Positive Bias', fontsize=20, fontweight = 'bold')
        # # mplpl.xlabel('False Negative Bias', fontsize=20, fontweight = 'bold')
        # mplpl.legend(loc="lower right", prop=legend_properties, fontsize='medium', ncol=1)
        # # mplpl.title(data_name)
        # # mplpl.legend(loc="upper left",fontsize = 'large')
        # mplpl.xlim([0, 2])
        # mplpl.ylim([0, 1])
        # mplpl.grid()
        # mplpl.subplots_adjust(bottom=0.24)
        # mplpl.subplots_adjust(left=0.18)

        #         mplpl.rcParams['figure.figsize'] = 5.4, 3.2
        #         mplpl.rc('xtick', labelsize='x-large')
        #         mplpl.rc('ytick', labelsize='x-large')
        #         mplpl.rc('legend', fontsize='x-large')
        #         w_err_avg_dict
        #         # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
        #         tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
        #         acc_l = []
        #         for t_id in tweet_l_sort:
        #             acc_l.append(w_err_avg_dict[t_id])
        #
        #         num_bins = len(acc_l)
        #         counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         cdf = np.cumsum(counts)
        #         scale = 1.0 / cdf[-1]
        #         ncdf = scale * cdf
        #         mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)
        #
        #
        #
        # #
        #         mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        #         mplpl.xlabel('Mean perception bias', fontsize=20, fontweight = 'bold')
        #         # mplpl.title(data_name)
        #         mplpl.legend(loc="upper left",fontsize = 'large')
        #         mplpl.xlim([-1.5, 1.5])
        #         mplpl.ylim([0, 1])
        #         mplpl.grid()
        #         mplpl.subplots_adjust(bottom=0.24)
        #         mplpl.subplots_adjust(left=0.18)
        #         # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        #         pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/MPB_cdf_alldataset'
        #         mplpl.savefig(pp + '.pdf', format='pdf')
        #         mplpl.savefig(pp + '.png', format='png')


    if args.t == "AMT_dataset_reliable_news_processing_all_dataset_weighted_visualisation_initial_stastistics_DISP_TPB_scatter_fig":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes_nonpol']:#['snopes','politifact','mia',snopes_nonpol]:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']
                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            elif dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpol'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            w_cyn_dict= collections.defaultdict()
            w_gull_dict= collections.defaultdict()
            w_apb_dict= collections.defaultdict()
            tweet_avg_susc_group = collections.defaultdict()
            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []

            same_side = 0
            other_side = 0

            rep_b = 0
            dem_b = 0
            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_weighted.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]


                    dem_err = np.mean(dem_df['err'])
                    rep_err = np.mean(rep_df['err'])

                    if dem_err * rep_err >=0:
                        same_side+=1

                        if np.abs(dem_err)>np.abs(rep_err):
                            rep_b+=1
                        else:
                            dem_b+=1
                    else:
                        other_side+=1





                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])


                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)


                    dem_susc_list = list(dem_df['susc'])
                    rep_susc_list = list(rep_df['susc'])
                    neut_susc_list = list(neut_df['susc'])


                    tweet_avg_susc_group[t_id] = np.abs(np.mean(dem_susc_list) - np.mean(rep_susc_list))



                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    # w_abs_err_list = list(df_tmp['abs_err'])
                    w_sus_list = list(df_tmp['susc'])
                    # w_norm_err_list = list(df_tmp['norm_err'])
                    # w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    # w_cyn_list = list(df_tmp['cyn'])
                    # w_gull_list = list(df_tmp['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])


                    df_cyn = df_tmp[df_tmp['cyn']>0]
                    df_gull = df_tmp[df_tmp['gull']>0]

                    w_cyn_list = list(df_cyn['cyn'])
                    w_gull_list = list(df_gull['gull'])

                    w_cyn_dict[t_id] = np.mean(w_cyn_list)
                    w_gull_dict[t_id] = np.mean(w_gull_list)
                    w_apb_dict[t_id] = np.mean(w_sus_list)


                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

            # news_cat_list_f = ['pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true', 'true']
            # news_cat_list_f = ['false', 'mostly_false', 'mixture', 'mostly_true', 'true']
            print('same side ' + str(same_side))
            print('other side : ' + str(other_side))

            print('dem better : ' + str(dem_b))
            print('rep better : ' + str(rep_b))
            # exit()
            if dataset=='snopes':
                col_l = ['darkred', 'orange', 'gray', 'lime', 'green']
                # col = 'purple'
                col = 'k'
                news_cat_list_n = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']
            if dataset=='politifact':
                col_l = ['darkred', 'red', 'orange', 'gray', 'lime', 'green']
                news_cat_list_n = ['PANTS ON FIRE', 'FALSE', 'MOSTLY FALSE', 'HALF TRUE', 'MOSTLY TRUE', 'TRUE']
                # col = 'c'
                col = 'k'
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            if dataset=='mia':
                col_l = ['red', 'green']
                news_cat_list_n = ['RUMORS', 'NON RUMORS']
                # col = 'brown'
                col = 'k'
                col_t_f = ['red', 'green']
                news_cat_list_t_f = [['rumors'], ['non-rumors']]
                cat_list_t_f = ['FALSE', 'TRUE']
                col_t_f = ['red', 'green']

            count = 0
            # Y = [0]*len(thr_list)




            tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            gt_l = []
            pt_l = []

            fig_cdf = True
            # fig_cdf = False
            if fig_cdf==True:

        #####################################################33

                # out_dict = w_apb_dict
                # out_dict = w_gull_dict
                # out_dict = w_cyn_dict
                out_dict = tweet_avg_group
                # out_dict = tweet_avg
                # out_dict = w_apb_dict
                # out_dict = tweet_avg_susc_group
                tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=False)
                # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=False)
                pt_l = []
                t_var_l = []
                pt_l_dict = collections.defaultdict(list)
                for t_id in tweet_l_sort:
                    pt_l.append(out_dict[t_id])
                    t_var_l.append(tweet_var[t_id])
                count=0

                # num_bins = len(pt_l)
                # counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5,linestyle='--', label='All news stories')
                # mplpl.rcParams['figure.figsize'] = 5.8, 3.7
                # mplpl.rcParams['figure.figsize'] = 5.6, 3.6
                mplpl.rcParams['figure.figsize'] = 4.5, 2.5
                mplpl.rc('xtick', labelsize='large')
                mplpl.rc('ytick', labelsize='large')
                mplpl.rc('legend', fontsize='medium')

                # for t_id in tweet_l_sort:
                #     if w_apb_dict[t_id]>0 or w_apb_dict[t_id]<=0:
                #         pt_l.append(w_apb_dict[t_id])



                    # df_tt = pd.DataFrame(np.array(pt_l_dict[cat_m]), columns=[cat_m])
                    # df_tt[cat_m].plot(kind='kde', lw=6, color=col_l[count-1], label=cat_m)

                    # num_bins = len(pt_l_dict[cat_m])
                    # counts, bin_edges = np.histogram(pt_l_dict[cat_m], bins=num_bins, normed=True)
                    # cdf = np.cumsum(counts)
                    # scale = 1.0 / cdf[-1]
                    # ncdf = scale * cdf
                    # mplpl.plot(bin_edges[1:], ncdf, c=col_l[count-1], lw=5, label=cat_m)


                mplpl.scatter(range(len(pt_l)), pt_l, c='purple')
                z = np.polyfit(range(len(pt_l)), pt_l, 1)
                p = np.poly1d(z)
                mplpl.plot(range(len(pt_l)), p(range(len(pt_l))), 'r-', linewidth=4.0)

                print(np.corrcoef(pt_l, t_var_l))
                mplpl.xlabel('News stories ranked by disputability', fontsize=13, fontweight = 'bold')
                # mplpl.xlabel('News stories ranked by PTL', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('Total Perception Bias', fontsize=13, fontweight = 'bold')
                # mplpl.ylabel('Ideological Perception Bias', fontsize=12, fontweight = 'bold')
                mplpl.ylabel(r'$|MPB_{Dem} - MPB_{Rep}|$', fontsize=15, fontweight = 'bold')
                # mplpl.ylabel('|TPB of Dems - TPB of Reps|', fontsize=13, fontweight = 'bold')
                mplpl.grid()
                mplpl.title(data_name, fontsize='x-large')
                labels = ['0.0', '0.25', '0.5', '0.75', '1.0']
                y = [0.0, 0.5, 1, 1.5, 2]
                # mplpl.yticks(y, labels)
                legend_properties = {'weight': 'bold'}


                # plt.legend(prop=legend_properties)
                # mplpl.legend(loc="upper left",prop=legend_properties,fontsize='small', ncol=1)#, fontweight = 'bold')
                mplpl.legend(loc="lower right",prop=legend_properties,fontsize='medium', ncol=1)#, fontweight = 'bold')
                mplpl.xlim([0, 100])
                # mplpl.xlim([0, 60])
                # mplpl.ylim([0, 2])
                mplpl.ylim([0, 1])
                mplpl.subplots_adjust(bottom=0.24)
                mplpl.subplots_adjust(left=0.18)

                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_pt_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_APB_cdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_FPB_cdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_disput_TPB_scatter'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_PTL_TPB_scatter'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_disput_TPB_scatter_w'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_disput_ITPB_scatter'
                pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_disput_IPB_scatter'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_APB_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+dataset+'_FPB_pdf'
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/' + dataset + '_FNB_pdf'

                mplpl.savefig(pp + '.pdf', format='pdf')
                mplpl.savefig(pp + '.png', format='png')
                mplpl.figure()


                exit()

        #         mplpl.rcParams['figure.figsize'] = 4.5, 2.5
        #         mplpl.rc('xtick', labelsize='large')
        #         mplpl.rc('ytick', labelsize='large')
        #         mplpl.rc('legend', fontsize='medium')
        #         w_err_avg_dict
        #         # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
        #         tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
        #         # tweet_l_sort = [x for x in tweet_l_sort if x >= 0 or x < 0]
        #         acc_l = []
        #         for t_id in tweet_l_sort:
        #             if out_dict[t_id] >=0 or out_dict[t_id]<0:
        #                 acc_l.append(out_dict[t_id])
        #
        #         num_bins = len(acc_l)
        #         counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         cdf = np.cumsum(counts)
        #         scale = 1.0 / cdf[-1]
        #         ncdf = scale * cdf
        #         mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)
        #
        # legend_properties = {'weight': 'bold'}
        #
        # #
        # mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        # mplpl.xlabel('Total Perception Bias', fontsize=20, fontweight = 'bold')
        # # mplpl.xlabel('False Positive Bias', fontsize=20, fontweight = 'bold')
        # # mplpl.xlabel('False Negative Bias', fontsize=20, fontweight = 'bold')
        # mplpl.legend(loc="lower right", prop=legend_properties, fontsize='medium', ncol=1)
        # # mplpl.title(data_name)
        # # mplpl.legend(loc="upper left",fontsize = 'large')
        # mplpl.xlim([0, 2])
        # mplpl.ylim([0, 1])
        # mplpl.grid()
        # mplpl.subplots_adjust(bottom=0.24)
        # mplpl.subplots_adjust(left=0.18)

        #         mplpl.rcParams['figure.figsize'] = 5.4, 3.2
        #         mplpl.rc('xtick', labelsize='x-large')
        #         mplpl.rc('ytick', labelsize='x-large')
        #         mplpl.rc('legend', fontsize='x-large')
        #         w_err_avg_dict
        #         # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
        #         tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
        #         acc_l = []
        #         for t_id in tweet_l_sort:
        #             acc_l.append(w_err_avg_dict[t_id])
        #
        #         num_bins = len(acc_l)
        #         counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         cdf = np.cumsum(counts)
        #         scale = 1.0 / cdf[-1]
        #         ncdf = scale * cdf
        #         mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)
        #
        #
        #
        # #
        #         mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        #         mplpl.xlabel('Mean perception bias', fontsize=20, fontweight = 'bold')
        #         # mplpl.title(data_name)
        #         mplpl.legend(loc="upper left",fontsize = 'large')
        #         mplpl.xlim([-1.5, 1.5])
        #         mplpl.ylim([0, 1])
        #         mplpl.grid()
        #         mplpl.subplots_adjust(bottom=0.24)
        #         mplpl.subplots_adjust(left=0.18)
        #         # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        #         pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/MPB_cdf_alldataset'
        #         mplpl.savefig(pp + '.pdf', format='pdf')
        #         mplpl.savefig(pp + '.png', format='png')



    if args.t == "AMT_dataset_reliable_news_processing_all_data_weighted_demographic_visualisation":

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0
        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        balance_f = 'balanced'


        # balance_f = 'un_balanced'

        fig_f = True
        # fig_f = False
        if dataset=='snopes':
            data_n = 'sp'
        elif dataset=='politifact':
            data_n = 'pf'
        elif dataset=='mia':
            data_n = 'mia'

        # run = 'plot'
        run = 'analysis'
        run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_1={}
        tweet_dev_med={}
        tweet_dev_var={}
        tweet_avg={}
        tweet_med={}
        tweet_var={}
        tweet_gt_var = {}

        tweet_dev_avg_l=[]
        tweet_dev_med_l=[]
        tweet_dev_var_l=[]
        tweet_avg_l=[]
        tweet_med_l=[]
        tweet_var_l=[]
        tweet_gt_var_l=[]
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0
        # for ind in [1,2,3]:


        all_acc =[]
        balance_f = 'balanced'
        # balance_f = 'un_balanced'

        # fig_f = True
        fig_f = False
        if dataset=='snopes':
            data_n = 'sp'
        elif dataset=='politifact':
            data_n = 'pf'
        elif dataset=='mia':
            data_n = 'mia'
        for ind in [1]:
            if balance_f=='balanced':
                inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv'
            else:
                inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'

            inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
            df[ind] = pd.read_csv(inp1, sep="\t")
            df_w[ind] = pd.read_csv(inp1_w, sep="\t")

            df_m = df[ind].copy()

            for lean in [-1,0,1]:
                df_m = df[ind].copy()
                df_mm = df[ind].copy()

                if lean==0:
                    col = 'g'
                    lean_cat = 'neutral'
                elif lean==1:
                    col = 'b'
                    lean_cat = 'democrat'
                elif lean == -1:
                    col = 'r'
                    lean_cat = 'republican'

                df_m = df_m[df_m['leaning']==lean]

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                # df_tmp = df_m[df_m['tweet_id'] == t_id]
                for t_id in grouped.groups.keys():


                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []
                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list),np.median(val_list),np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    df_tt = df_tmp[df_tmp['acc']>-1]

                    accuracy = np.sum(df_tt['acc'])/float(len(df_tt))

                    all_acc.append(accuracy)
                    if fig_f==True:
                        try:
                            df_tmp['rel_v'].plot(kind='kde', lw=4, color=col, label = 'Worker Judgment')
                        except:
                            print('hmm')

                        mplpl.hist(list(df_tmp['rel_v']), weights=weights, color=col)

                        mplpl.plot([df_tmp['rel_gt_v'][ind_t],df_tmp['rel_gt_v'][ind_t]],[0,0.5], color = 'k', label='Ground Truth',linewidth=10)

                        mplpl.ylabel('Frequency')
                        mplpl.xlabel('Truth value of claim')
                        mplpl.title(df_tmp['ra_gt'][ind_t] + '  Accuracy : ' + str(np.round(accuracy,3)) + ' workers leaning : ' + lean_cat
                                    +'\n Number : ' + str(len(val_list)) +' Avg : ' + str(np.round(np.mean(val_list),3))
                                    + ', med : '+ str(np.round(np.median(val_list),3)) + ', Var : ' + str(np.round(np.var(val_list),3)))
                        mplpl.legend(loc="upper right")
                        mplpl.xlim([-2,2])
                        mplpl.ylim([0,1])
                        # pp = remotedir  + '/fig/fig_exp1/news_based/'+ str(t_id)+'_rel_dist'
                        pp = remotedir  + '/fig/fig_exp1/news_based/leaning_demographic/balanced/'+ str(t_id)+'_rel_dist_' + lean_cat
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()

                    weights = []
                    weights.append(np.ones_like(list(df_tmp['err'])) / float(len(df_tmp)))

                    ##################
                    val_list = list(df_tmp_m['susc'])
                    # val_list = list(df_tmp['err'])
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list),np.median(val_list),np.var(val_list)]
                    tweet_dev_avg_1[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)

                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

        # exit()
        AVG_list = []
        print(np.mean(all_acc))
        outF = open(remotedir + 'table_out.txt', 'w')
        tweet_all_var = {}
        tweet_all_dev_avg = {}
        tweet_all_avg = {}
        tweet_all_dev_avg_l = []
        tweet_all_dev_med_l = []
        tweet_all_dev_var_l = []
        tweet_all_avg_l = []
        tweet_all_med_l = []
        tweet_all_var_l = []
        tweet_all_gt_var_l = []
        diff_all_group_disp_l = []
        dem_disp_l = []
        rep_disp_l = []

        tweet_all_dev_avg_rnd = {}
        tweet_all_abs_dev_avg_rnd = {}
        tweet_all_dev_avg_rnd_l = []
        tweet_all_abs_dev_avg_rnd_l = []




        tweet_all_dev_avg_rnd_dem = {}
        tweet_all_abs_dev_avg_rnd_dem = {}
        tweet_all_dev_avg_rnd_l_dem = []
        tweet_all_abs_dev_avg_rnd_l_dem = []

        tweet_all_dev_avg_rnd_rep = {}
        tweet_all_abs_dev_avg_rnd_rep = {}
        tweet_all_dev_avg_rnd_l_rep = []
        tweet_all_abs_dev_avg_rnd_l_rep = []

        tweet_all_dev_avg_rnd_neut = {}
        tweet_all_abs_dev_avg_rnd_neut = {}
        tweet_all_dev_avg_rnd_l_neut = []
        tweet_all_abs_dev_avg_rnd_l_enut = []

        test_list = []

        diff_group_disp_dict = {}
        if dataset=='snopes':
            data_n = 'sp'
            news_cat_list = ['FALSE','MOSTLY FALSE',  'MIXTURE', 'MOSTLY TRUE','TRUE']
            ind_l = [1,2,3]
        elif dataset=='politifact':
            data_n = 'pf'
            news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            ind_l = [1,2,3]
        elif dataset=='mia':
            data_n = 'mia'
            news_cat_list = ['rumor', 'non-rumor']
            ind_l = [1]

        for cat_l in news_cat_list:
            outF.write('== ' + str(cat_l) + ' ==\n\n')
            print('== ' + str(cat_l) + ' ==')
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            AVG_susc_list = []
            AVG_wl_list = []
            all_acc = []
            AVG_dev_list=[]
            diff_group_disp_l = []

            tweet_dev_avg_rnd = {}
            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd_l = []
            tweet_abs_dev_avg_rnd_l = []

            tweet_dev_avg_rnd_l_dem = []
            tweet_abs_dev_avg_rnd_l_dem = []
            tweet_dev_avg_rnd_dem = {}
            tweet_abs_dev_avg_rnd_dem = {}

            tweet_dev_avg_rnd_l_rep = []
            tweet_abs_dev_avg_rnd_l_rep = []
            tweet_dev_avg_rnd_rep = {}
            tweet_abs_dev_avg_rnd_rep = {}

            tweet_dev_avg_rnd_l_neut = []
            tweet_abs_dev_avg_rnd_l_neut = []
            tweet_dev_avg_rnd_neut = {}
            tweet_abs_dev_avg_rnd_neut = {}
            val_list_neut = []
            for lean in [-1,0,1]:

                AVG_susc_list = []
                AVG_wl_list = []
                all_acc = []
                df_m = df_m[df_m['leaning']==lean]
                if lean==0:
                    col = 'g'
                    lean_cat = 'neutral'
                elif lean==1:
                    col = 'b'
                    lean_cat = 'democrat'
                elif lean == -1:
                    col = 'r'
                    lean_cat = 'republican'
                # print(lean_cat)
                for ind in ind_l:
                # for ind in [3]:

                    if balance_f == 'balanced':
                        inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                    else:
                        inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'

                    inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
                    df[ind] = pd.read_csv(inp1, sep="\t")
                    df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                    df_m = df[ind].copy()
                    df_mm = df_m.copy()

                    df_m = df_m[df_m['ra_gt']==cat_l]
                    # df_mm = df_m[df_m['ra_gt']==cat_l]
                    df_m = df_m[df_m['leaning']==lean]
                    groupby_ftr = 'tweet_id'
                    grouped = df_m.groupby(groupby_ftr, sort=False)
                    grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                    for t_id in grouped.groups.keys():
                        df_tmp = df_m[df_m['tweet_id'] == t_id]

                        df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
                        df_tmp_dem = df_tmp_m[df_tmp_m['leaning']==1]
                        df_tmp_rep = df_tmp_m[df_tmp_m['leaning']==-1]
                        df_tmp_neut = df_tmp_m[df_tmp_m['leaning']==0]
                        ind_t = df_tmp.index.tolist()[0]
                        weights = []
                        val_list = list(df_tmp_m['rel_v'])
                        val_list_dem = list(df_tmp_dem['rel_v'])
                        val_list_rep = list(df_tmp_rep['rel_v'])
                        val_list_neut = list(df_tmp_rep['rel_v'])
                        # diff_group_disp = np.abs(np.mean(val_list_dem) - np.mean(val_list_rep)) \
                        #                   + np.abs(np.mean(val_list_dem) - np.mean(val_list_neut)) \
                        #                   + np.abs(np.mean(val_list_rep) - np.mean(val_list_neut))
                        diff_group_disp = np.abs(np.mean(val_list_dem) - np.mean(val_list_rep)) \
                                          + np.abs(np.mean(val_list_dem) - np.mean(val_list_neut)) \
                                          + np.abs(np.mean(val_list_rep) - np.mean(val_list_neut))
                        diff_group_disp_dict[t_id] = diff_group_disp
                        diff_group_disp_l.append(diff_group_disp)
                        diff_all_group_disp_l.append(diff_group_disp)
                        dem_disp_l.append(np.var(val_list_dem))
                        rep_disp_l.append(np.var(val_list_rep))
                        tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                        # tweet_avg[t_id] = [np.mean(val_list)]
                        # tweet_med[t_id] = [np.median(val_list)]
                        # tweet_var[t_id] = [np.var(val_list)]

                        tweet_avg_l.append(np.mean(val_list))
                        tweet_med_l.append(np.median(val_list))
                        tweet_var_l.append(np.var(val_list))
                        tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])


                        tweet_all_avg_l.append(np.mean(val_list))
                        tweet_all_med_l.append(np.median(val_list))
                        tweet_all_var_l.append(np.var(val_list))
                        tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])


                        AVG_wl_list+=list(df_tmp['rel_v'])


                        AVG_dev_list += list(df_tmp['err'])

                        #####################
                        val_list = list(df_tmp_m['rel_v'])
                        tweet_all_var[t_id] = np.var(val_list)
                        tweet_all_avg[t_id] = np.mean(val_list)

                        val_list_dev = list(df_tmp_m['err'])
                        err_list_dev_dem = list(df_tmp_dem['err'])
                        err_list_dev_rep = list(df_tmp_rep['err'])
                        err_list_dev_neut = list(df_tmp_neut['err'])




                        abs_var_err = [np.abs(x) for x in val_list_dev]
                        abs_var_err_dem = [np.abs(x) for x in err_list_dev_dem]
                        abs_var_err_rep = [np.abs(x) for x in err_list_dev_rep]
                        abs_var_err_neut = [np.abs(x) for x in err_list_dev_neut]





                        tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                        tweet_dev_avg[t_id] = np.mean(val_list_dev)
                        tweet_all_dev_avg[t_id] = np.mean(val_list_dev)
                        tweet_dev_med[t_id] = np.median(val_list_dev)
                        tweet_dev_var[t_id] = np.var(val_list_dev)
                        tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                        tweet_avg[t_id] = np.mean(val_list)
                        tweet_med[t_id] = np.median(val_list)
                        tweet_var[t_id] = np.var(val_list)

                        df_tt = df_tmp[df_tmp['acc'] > -1]

                        accuracy = np.sum(df_tt['acc']) / float(len(df_tt))

                        all_acc.append(accuracy)

                        # all_acc.append(np.sum(df_tmp['acc'])/float(len(df_tmp)))
                        #
                        tweet_dev_avg_l.append(np.mean(val_list_dev))
                        tweet_dev_med_l.append(np.median(val_list_dev))
                        tweet_dev_var_l.append(np.var(val_list_dev))

                        tweet_all_dev_avg_l.append(np.mean(val_list_dev))
                        tweet_all_dev_med_l.append(np.median(val_list_dev))
                        tweet_all_dev_var_l.append(np.var(val_list_dev))

                        sum_rnd_abs_perc = 0
                        sum_rnd_perc = 0
                        for val in [-1, -2 / float(3), -1 / float(3), 0, 1 / float(3), 2 / float(3), 1]:
                            sum_rnd_perc += val - df_tmp['rel_gt_v'][ind_t]
                            sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                        random_perc = np.abs(sum_rnd_perc / float(7))
                        random_abs_perc = sum_rnd_abs_perc / float(7)

                        tweet_dev_avg_rnd[t_id] = np.mean(val_list_dev) / float(random_perc)
                        tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                        tweet_all_dev_avg_rnd[t_id] = np.mean(val_list_dev) / float(random_perc)
                        tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                        tweet_dev_avg_rnd_l.append(np.mean(val_list_dev) / float(random_perc))
                        tweet_abs_dev_avg_rnd_l.append(np.mean(abs_var_err) / float(random_abs_perc))

                        tweet_all_dev_avg_rnd_l.append(np.mean(val_list_dev) / float(random_perc))
                        tweet_all_abs_dev_avg_rnd_l.append(np.mean(abs_var_err) / float(random_abs_perc))


                        #Democrats
                        tweet_dev_avg_rnd_dem[t_id] = np.mean(err_list_dev_dem) / float(random_perc)
                        tweet_abs_dev_avg_rnd_dem[t_id] = np.mean(abs_var_err_dem) / float(random_abs_perc)

                        tweet_all_dev_avg_rnd_dem[t_id] = np.mean(err_list_dev_dem) / float(random_perc)
                        tweet_all_abs_dev_avg_rnd_dem[t_id] = np.mean(abs_var_err_dem) / float(random_abs_perc)

                        tweet_dev_avg_rnd_l_dem.append(np.mean(err_list_dev_dem) / float(random_perc))
                        tweet_abs_dev_avg_rnd_l_dem.append(np.mean(abs_var_err_dem) / float(random_abs_perc))

                        tweet_all_dev_avg_rnd_l_dem.append(np.mean(err_list_dev_dem) / float(random_perc))
                        tweet_all_abs_dev_avg_rnd_l_dem.append(np.mean(abs_var_err_dem) / float(random_abs_perc))

                        #Republicans

                        tweet_dev_avg_rnd_rep[t_id] = np.mean(err_list_dev_rep) / float(random_perc)
                        tweet_abs_dev_avg_rnd_rep[t_id] = np.mean(abs_var_err_rep) / float(random_abs_perc)

                        tweet_all_dev_avg_rnd_rep[t_id] = np.mean(err_list_dev_rep) / float(random_perc)
                        tweet_all_abs_dev_avg_rnd_rep[t_id] = np.mean(abs_var_err_rep) / float(random_abs_perc)

                        tweet_dev_avg_rnd_l_rep.append(np.mean(err_list_dev_rep) / float(random_perc))
                        tweet_abs_dev_avg_rnd_l_rep.append(np.mean(abs_var_err_rep) / float(random_abs_perc))

                        tweet_all_dev_avg_rnd_l_rep.append(np.mean(err_list_dev_rep) / float(random_perc))
                        tweet_all_abs_dev_avg_rnd_l_rep.append(np.mean(abs_var_err_rep) / float(random_abs_perc))

                        #Neturals
                        tweet_dev_avg_rnd_neut[t_id] = np.mean(err_list_dev_neut) / float(random_perc)
                        tweet_abs_dev_avg_rnd_neut[t_id] = np.mean(abs_var_err_neut) / float(random_abs_perc)

                        tweet_all_dev_avg_rnd_neut[t_id] = np.mean(err_list_dev_neut) / float(random_perc)
                        tweet_all_abs_dev_avg_rnd_neut[t_id] = np.mean(abs_var_err_neut) / float(random_abs_perc)

                        tweet_dev_avg_rnd_l_neut.append(np.mean(err_list_dev_neut) / float(random_perc))
                        tweet_abs_dev_avg_rnd_l_neut.append(np.mean(abs_var_err_neut) / float(random_abs_perc))

                        tweet_all_dev_avg_rnd_l_neut.append(np.mean(err_list_dev_neut) / float(random_perc))
                        # tweet_all_abs_dev_avg_rnd_l_neut.append(np.mean(abs_var_err_neut) / float(random_abs_perc))

                        # test_list.append(np.mean(abs_var_err_dem) / float(random_abs_perc) +
                        #                  np.mean(abs_var_err_rep) / float(random_abs_perc) +
                        #                  np.mean(abs_var_err_neut) / float(random_abs_perc) )

                        # test_list.append(np.mean(abs_var_err_dem) / float(random_abs_perc) )
                                         # np.mean(abs_var_err_rep) / float(random_abs_perc))
                        test_list.append(np.mean(abs_var_err_dem) / float(random_abs_perc) )

                        # print(np.mean(all_acc))
                print('correlation between disputability and grouped disputibality : ' + str(
                    np.round(np.corrcoef(tweet_var_l, diff_group_disp_l)[0][1], 4)))
                weights=[]
                weights.append(np.ones_like(AVG_wl_list) / float(len(AVG_wl_list)))

                df_tt = pd.DataFrame(np.array(AVG_wl_list), columns=["wl_g"])
                # df_tt['wl_g'] = df_tt['wl_g']/([np.max(df_tt['wl_g'])]*len(df_tt))
                # df_tt['wl_g'] = df_tt['wl_g']/([2]*len(df_tt))

                if fig_f==True:
                    try:
                        df_tt['wl_g'].plot(kind='kde', lw=4, color=col, label='Worker Judgment')
                    except:
                        print('hmm')

                    mplpl.hist(AVG_wl_list, weights=weights, color=col)
                    #
                    mplpl.ylabel('Frequency')
                    mplpl.xlabel('Truth value of claim')
                    mplpl.title('Avg : ' + str(np.round(np.mean(AVG_wl_list),3))
                    + ', med : '+ str(np.round(np.median(AVG_wl_list),3)) + ', Var : ' + str(np.round(np.var(AVG_wl_list),3)))
                    mplpl.legend(loc="upper right")
                    mplpl.xlim([-2,2])
                    mplpl.ylim([0,1])
                    if balance_f=='balanced':
                        pp = remotedir  + '/fig/fig_exp1/news_based/leaning_demographic/'+ str(cat_l)+'_rel_dist_' + lean_cat
                    else:
                        pp = remotedir  + '/fig/fig_exp1/news_based/leaning_demographic/balanced/'+ str(cat_l)+'_rel_dist_' + lean_cat
                    mplpl.savefig(pp, format='png')
                    mplpl.figure()
                #
                weights = []


                # print(np.corrcoef(tweet_avg_l,tweet_gt_var_l)[0][1])
                # print(np.corrcoef(tweet_med_l,tweet_gt_var_l)[0][1])
                # print(np.corrcoef(tweet_var_l,tweet_gt_var_l)[0][1])
                #
                # print(np.corrcoef(tweet_dev_avg_l, tweet_gt_var_l)[0][1])
                # print(np.corrcoef(tweet_dev_med_l, tweet_gt_var_l)[0][1])
                # print(np.corrcoef(tweet_dev_var_l, tweet_gt_var_l)[0][1])




            # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get,reverse=True)
            # tweet_l_sort = sorted(tweet_med, key=tweet_med.get,reverse=True)
            tweet_l_sort = sorted(tweet_var, key=tweet_var.get,reverse=True)

            # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get,reverse=False)
            # tweet_l_sort = sorted(tweet_dev_med, key=tweet_dev_med.get,reverse=False)


            # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get,reverse=True)

            if dataset == 'snopes':
                data_addr = 'snopes'
            elif dataset == 'politifact':
                data_addr = 'politifact/fig'
            elif dataset == 'mia':
                data_addr = 'mia/fig'

            count = 0
            # outF.write(
            #     '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #
            # for t_id in tweet_l_sort:
            #     count+=1
            #     if balance_f=='balanced':
            #         outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                    + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'
            #                                                                     '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                    str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                    '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                    str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                    '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                    str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                    '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                    str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #         # +
            #         #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
            #         #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')
            #
            #     else:
            #         outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
            #                    str(np.round(diff_group_disp_dict[t_id], 3)) + '||'
            #                                                                   '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                    str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                    '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                    str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                    '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                    str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                    '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                    str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                    # +
                    # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                    # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')



        # exit()


        # print('judgment_variance : ' + str((np.mean( tweet_all_var_l))))
        # print('judgment_variance : ' + str((np.median( tweet_all_var_l))))
        print('correlation between ground truth and pt : ' + str(np.round(np.corrcoef(tweet_all_gt_var_l, tweet_all_avg_l)[0][1],4)))
        print('correlation between ground truth and grouped disputibality : ' + str(np.round(np.corrcoef(tweet_all_gt_var_l, diff_all_group_disp_l)[0][1],4)))
        print('correlation between gt and grouped disputibality : ' + str(np.round(np.corrcoef(tweet_all_gt_var_l, diff_all_group_disp_l)[0][1],4)))




        print('correlation between pt and grouped disputibality : ' + str(np.round(np.corrcoef(tweet_all_avg_l, diff_all_group_disp_l)[0][1],4)))
        print('correlation between disputability and grouped disputibality : ' + str(np.round(np.corrcoef(tweet_all_var_l, diff_all_group_disp_l)[0][1],4)))





        print('correlation between perception_bias_rnd and grouped disputibality : ' + str(np.round(np.corrcoef(tweet_all_dev_avg_rnd_l, diff_all_group_disp_l)[0][1],4)))
        print('correlation between abs_perception_bias_rnd and grouped disputibality : ' + str(np.round(np.corrcoef(tweet_all_abs_dev_avg_rnd_l, diff_all_group_disp_l)[0][1],4)))

        print('correlation between abs_perception_bias_rnd and disputibality : ' + str(np.round(np.corrcoef(tweet_all_abs_dev_avg_rnd_l, tweet_all_var_l)[0][1],4)))
        print('correlation between abs_perception_bias_rnd and disputibality : ' + str(np.round(np.corrcoef(tweet_all_dev_avg_rnd_l, tweet_all_var_l)[0][1],4)))



        print('correlation between abs_perception_bias_rnd and disputibality : ' + str(np.round(np.corrcoef(test_list, tweet_all_var_l)[0][1],4)))
        print('correlation between abs_perception_bias_rnd and disputibality : ' + str(np.round(np.corrcoef(tweet_all_dev_avg_rnd_l, test_list)[0][1],4)))


        # print('judgment_disputabili ' + str((np.mean(tweet_all_dev_var_l))))
        # print('correlation between ground truth and judgment_disputability_avg : ' + str(np.round(np.corrcoef(tweet_all_gt_var_l, tweet_all_dev_avg_l)[0][1],4)))
        # print('correlation between judgment_var and judgment_disputability_var : ' + str(np.round(np.corrcoef(tweet_all_var_l, tweet_all_dev_avg_l)[0][1],4)))
        #
        #
        # print('correlation between ground truth and judgment_variance : ' + str(np.round(np.corrcoef(tweet_all_gt_var_l, tweet_all_var_l)[0][1],4)))
        # print('correlation between ground truth and judgment_avg : ' + str(np.round(np.corrcoef(tweet_all_gt_var_l, tweet_all_avg_l)[0][1],4)))
        # print('correlation between ground truth and grouped disputibality : ' + str(np.round(np.corrcoef(tweet_all_gt_var_l, diff_group_disp_l)[0][1],4)))
        # print('correlation between ground truth and judgment_disputability_variance : ' + str(np.round(np.corrcoef(tweet_all_gt_var_l, tweet_all_dev_var_l)[0][1],4)))
        # print('correlation between ground truth and judgment_disputability_avg : ' + str(np.round(np.corrcoef(tweet_all_gt_var_l, tweet_all_dev_avg_l)[0][1],4)))
        # print('correlation between judgment_var and judgment_disputability_var : ' + str(np.round(np.corrcoef(tweet_all_var_l, tweet_all_dev_avg_l)[0][1],4)))
        #
        #
        # print('correlation between ground truth and judgment_variance : ' + str(np.round(np.corrcoef(tweet_all_gt_var_l, dem_disp_l)[0][1],4)))
        # print('correlation between ground truth and judgment_avg : ' + str(np.round(np.corrcoef(tweet_all_gt_var_l, rep_disp_l)[0][1],4)))
        # dem_disp_l = []
        # rep_disp_l = []


        if dataset=='snopes':
            data_addr = 'snopes'
        elif dataset=='politifact':
            data_addr = 'politifact/fig'
        elif dataset=='mia':
            data_addr = 'mia/fig'


        tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
        # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
        # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
        # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
        count = 0
        outF.write('|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
        for t_id in tweet_l_sort:
            count += 1
            # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
            if balance_f == 'balanced':
                outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                           + str(np.round(diff_group_disp_dict[t_id],3)) +'||'
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/'+data_addr+'/fig_exp1/news_based/balanced/' +
                           str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/'+data_addr+'/fig_exp1/news_based/leaning_demographic/balanced/' +
                           str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/'+data_addr+'/fig_exp1/news_based/leaning_demographic/balanced/' +
                           str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/'+data_addr+'/fig_exp1/news_based/leaning_demographic/balanced/' +
                           str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                # +
                #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
                #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

            else:
                outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                           str(np.round(diff_group_disp_dict[t_id],3)) +'||'
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/'+data_addr+'/fig_exp1/news_based/' +
                           str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/'+data_addr+'/fig_exp1/news_based/leaning_demographic/' +
                           str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/'+data_addr+'/fig_exp1/news_based/leaning_demographic/' +
                           str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/'+data_addr+'/fig_exp1/news_based/leaning_demographic/' +
                           str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                           # +
                           # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                           # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')



    if args.t == "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_mpb_cdf_toghether":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                #     tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes_nonpol':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable

        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','snopes_nonpol','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                tweet_id = 100010
                publisher_name = 110
                tweet_popularity = {}
                tweet_text_dic = {}
                for input_file in [input_rumor, input_non_rumor]:
                    for line in input_file:
                        line.replace('\n', '')
                        line_splt = line.split('\t')
                        tweet_txt = line_splt[1]
                        tweet_link = line_splt[1]
                        tweet_id += 1
                        publisher_name += 1
                        tweet_popularity[tweet_id] = int(line_splt[2])
                        tweet_text_dic[tweet_id] = tweet_txt



                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                    # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    #     tweet_lable_dict[tweet_id] = 'undecided'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                col = 'r'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')



            if dataset == 'snopes_nonpol':
                claims_list = []
                col = 'r'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'politifact':
                col = 'g'

                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                col = 'purple'

                data_name = 'Snopes'
            elif dataset=='snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                col = 'green'

                data_name = 'Snopes_nonpol'
            elif dataset=='politifact':
                col = 'c'
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                col = 'orange'
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()
                df[ind].loc[:, 'abs_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_abs_err'] = df[ind]['tweet_id'] * 0.0

                groupby_ftr = 'tweet_id'
                grouped = df[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df[ind].groupby(groupby_ftr, sort=False).sum()


                for ind_t in df[ind].index.tolist():
                    t_id = df[ind]['tweet_id'][ind_t]
                    err = df[ind]['err'][ind_t]
                    abs_err = np.abs(err)
                    df[ind]['abs_err'][ind_t] = abs_err
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= (val - df[ind]['rel_gt_v'][ind_t])
                        sum_rnd_abs_perc += np.abs(val - df[ind]['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)


                    norm_err = err / float(random_perc)
                    norm_abs_err = abs_err / float(random_abs_perc)
                    df[ind]['norm_err'][ind_t] = norm_err
                    df[ind]['norm_abs_err'][ind_t] = norm_abs_err

                # df[ind] = df[ind].copy()

            w_pt_avg_l = []
            w_err_avg_l = []
            w_abs_err_avg_l = []
            w_norm_err_avg_l = []
            w_norm_abs_err_avg_l = []
            w_acc_avg_l = []

            w_pt_std_l = []
            w_err_std_l = []
            w_abs_err_std_l = []
            w_norm_err_std_l = []
            w_norm_abs_err_std_l = []
            w_acc_std_l = []

            w_pt_avg_dict = collections.defaultdict()
            w_err_avg_dict = collections.defaultdict()
            w_abs_err_avg_dict = collections.defaultdict()
            w_norm_err_avg_dict = collections.defaultdict()
            w_norm_abs_err_avg_dict = collections.defaultdict()
            w_acc_avg_dict = collections.defaultdict()

            w_pt_std_dict = collections.defaultdict()
            w_err_std_dict = collections.defaultdict()
            w_abs_err_std_dict = collections.defaultdict()
            w_norm_err_std_dict = collections.defaultdict()
            w_norm_abs_err_std_dict = collections.defaultdict()
            w_acc_std_dict = collections.defaultdict()

            all_w_pt_list  = []
            all_w_err_list = []
            all_w_abs_err_list = []
            all_w_norm_err_list = []
            all_w_norm_abs_err_list  = []
            all_w_acc_list = []

            all_w_cyn_list = []
            all_w_gull_list = []
            w_cyn_avg_l = []
            w_gull_avg_l = []
            w_cyn_std_l= []
            w_gull_std_l = []
            w_cyn_avg_dict =collections.defaultdict()
            w_gull_avg_dict =collections.defaultdict()
            w_cyn_std_dict =collections.defaultdict()
            w_gull_std_dict = collections.defaultdict()
            for ind in ind_l:

                df_m = df[ind].copy()
                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]


                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    w_abs_err_list = list(df_tmp['abs_err'])
                    w_norm_err_list = list(df_tmp['norm_err'])
                    w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    w_cyn_list = list(df_tmp['cyn'])
                    w_gull_list = list(df_tmp['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])
                    w_acc_list = []
                    # w_ind_acc_list
                    acc_c = 0
                    nacc_c = 0




                    all_w_pt_list += list(df_tmp['rel_v'])
                    all_w_err_list += list(df_tmp['err'])
                    all_w_abs_err_list += list(df_tmp['abs_err'])
                    all_w_norm_err_list += list(df_tmp['norm_err'])
                    all_w_norm_abs_err_list += list(df_tmp['norm_abs_err'])
                    all_w_cyn_list += list(df_tmp['cyn'])
                    all_w_gull_list += list(df_tmp['gull'])
                    all_w_acc_list += list(w_acc_list)


                    w_pt_avg_l.append(np.mean(w_pt_list))
                    w_err_avg_l.append(np.mean(w_err_list))
                    w_abs_err_avg_l.append(np.mean(w_abs_err_list))
                    w_norm_err_avg_l.append(np.mean(w_norm_err_list))
                    w_norm_abs_err_avg_l.append(np.mean(w_norm_abs_err_list))
                    w_cyn_avg_l.append(np.mean(w_cyn_list))
                    w_gull_avg_l.append(np.mean(w_gull_list))
                    # w_acc_avg_l.append(w_ind_acc_list)

                    w_pt_std_l.append(np.std(w_pt_list))
                    w_err_std_l.append(np.std(w_err_list))
                    w_abs_err_std_l.append(np.std(w_abs_err_list))
                    w_norm_err_std_l.append(np.std(w_norm_err_list))
                    w_norm_abs_err_std_l.append(np.std(w_norm_abs_err_list))
                    w_cyn_std_l.append(np.std(w_cyn_list))
                    w_gull_std_l.append(np.std(w_gull_list))
                    # w_acc_std_l.append(np.std(w_ind_acc_list))


                    w_pt_avg_dict[t_id] = np.mean(w_pt_list)
                    w_err_avg_dict[t_id] = np.mean(w_err_list)
                    w_abs_err_avg_dict[t_id] = np.mean(w_abs_err_list)
                    w_norm_err_avg_dict[t_id] = np.mean(w_norm_err_list)
                    w_norm_abs_err_avg_dict[t_id] = np.mean(w_norm_abs_err_list)
                    w_cyn_avg_dict[t_id] = np.mean(w_cyn_list)
                    w_gull_avg_dict[t_id] = np.mean(w_gull_list)
                    # w_acc_avg_dict[t_id] = w_ind_acc_list

                    w_pt_std_dict[t_id] = np.std(w_pt_list)
                    w_err_std_dict[t_id] = np.std(w_err_list)
                    w_abs_err_std_dict[t_id] = np.std(w_abs_err_list)
                    w_norm_err_std_dict[t_id] = np.std(w_norm_err_list)
                    w_norm_abs_err_std_dict[t_id] = np.std(w_norm_abs_err_list)
                    w_cyn_std_dict[t_id] = np.std(w_cyn_list)
                    w_gull_std_dict[t_id] = np.std(w_gull_list)
                    w_acc_std_dict[t_id] = np.std(w_acc_list)
                # ind_
            ##################################################
            #
            # tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            # gt_l = []
            # pt_l = []
            # disputability_l = []
            # perc_l = []
            # abs_perc_l=[]
            # abs_perc_rnd_l = []
            # perc_rnd_l = []
            # tweet_skew_ll = []
            # for t_id in tweet_l_sort:
            #     gt_l.append(tweet_gt_var[t_id])
            #     pt_l.append(tweet_avg[t_id])
            #     disputability_l.append(tweet_var[t_id])
            #     perc_l.append(tweet_dev_avg[t_id])
            #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #     perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
            #     abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
            #     tweet_skew_ll.append(tweet_skew[t_id])
            # value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l,tweet_skew_ll]
            # value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
            #               'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'skewness']
            #
            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(8):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(8):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')
            #
            #
            # exit()

            # fig_f = True
            fig_f = False
            # fig_f_1 = True
            fig_f_1 = False
            fig_f_together = True
            if fig_f==True:

                df_tmp = pd.DataFrame({'val' : all_w_acc_list})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), weights=weights, color='g')
                # mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 1.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Readers accuracy', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_acc_list),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_acc_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_acc_density| alt text| width = 500px}}')



                # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
                df_tmp = pd.DataFrame({'val' : w_acc_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                # mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers accuracy', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_acc_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_acc_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_acc_pt| alt text| width = 500px}}')




                df_tmp = pd.DataFrame({'val' : all_w_pt_list})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), weights=weights, color='c')
                # mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 1.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Readers percevied truth value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_pt_list),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_pt_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_pt_density| alt text| width = 500px}}')




                # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
                df_tmp = pd.DataFrame({'val' : w_pt_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                mplpl.hist(list(df_tmp['val']), normed=1, color='c')


                # mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 3.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers percevied truth value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_pt_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_pt_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_pt_density| alt text| width = 500px}}')




                df_tmp = pd.DataFrame({'val' : w_err_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='y')


                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 3.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers perception bias value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_err_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_pb_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_pb_density| alt text| width = 500px}}')



                df_tmp = pd.DataFrame({'val' : w_abs_err_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='y')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers absolute perception bias value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_abs_err_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_apb_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_apb_density| alt text| width = 500px}}||')



                df_tmp = pd.DataFrame({'val' : w_gull_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='m')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 10])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers gullibility value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_gull_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_gull_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_gull_density| alt text| width = 500px}}||')




                df_tmp = pd.DataFrame({'val' : w_cyn_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='k')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 10])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers cynicallity value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_cyn_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_cyn_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_cyn_density| alt text| width = 500px}}||\n\n')



        ########################

                tweet_l_sort = sorted(w_pt_avg_dict, key=w_pt_avg_dict.get, reverse=False)
                pt_l = []
                for t_id in tweet_l_sort:
                    pt_l.append(w_pt_avg_dict[t_id])

                mplpl.scatter(range(len(pt_l)), pt_l,  s=40,color='c',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ranked readers according PTL', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_pt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_pt_pt| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_acc_avg_dict, key=w_acc_avg_dict.get, reverse=False)
                acc_l = []
                for t_id in tweet_l_sort:
                    acc_l.append(w_acc_avg_dict[t_id])


                mplpl.scatter(range(len(acc_l)), acc_l ,  s=40,color='g',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Accuracy', fontsize=18)
                mplpl.xlabel('Ranked readers according accuracy', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(acc_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_acc_acc'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_acc_acc| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
                err_l = []
                for t_id in tweet_l_sort:
                    err_l.append(w_err_avg_dict[t_id])

                mplpl.scatter(range(len(err_l)), err_l,  s=40,color='y',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias (PB)', fontsize=18)
                mplpl.xlabel('Ranked readers according PB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(err_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_err_err'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_err_err| alt text| width = 500px}}')



                tweet_l_sort = sorted(w_abs_err_avg_dict, key=w_abs_err_avg_dict.get, reverse=False)
                abs_err_l = []
                for t_id in tweet_l_sort:
                    abs_err_l.append(w_abs_err_avg_dict[t_id])

                mplpl.scatter(range(len(abs_err_l)), abs_err_l,  s=40,color='y',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according APB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_err_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_abs-err_abs-err'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_abs-err_abs-err| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_gull_avg_dict, key=w_gull_avg_dict.get, reverse=False)
                gull_l = []
                for t_id in tweet_l_sort:
                    gull_l.append(w_gull_avg_dict[t_id])

                mplpl.scatter(range(len(gull_l)), gull_l,  s=40,color='m',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Gullibility', fontsize=18)
                mplpl.xlabel('Ranked readers according gullibility', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(gull_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_gull_gull'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_gull_gull| alt text| width = 500px}}')



                tweet_l_sort = sorted(w_cyn_avg_dict, key=w_cyn_avg_dict.get, reverse=False)
                cyn_l = []
                for t_id in tweet_l_sort:
                    cyn_l.append(w_cyn_avg_dict[t_id])

                mplpl.scatter(range(len(cyn_l)), cyn_l,  s=40,color='k',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Cynicality', fontsize=18)
                mplpl.xlabel('Ranked readers according cynicality', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(cyn_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_cyn_cyn'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_cyn_cyn| alt text| width = 500px}}||\n\n')



                # outF.write('|| Table ||\n\n')

                # mplpl.show()
                # exit()
        #####################################################33


                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_pt_cdf| alt text| width = 500px}}')


                num_bins = len(acc_l)
                counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Accuracy', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_acc_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_acc_cdf| alt text| width = 500px}}')


                num_bins = len(err_l)
                counts, bin_edges = np.histogram(err_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception bias (PB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_err_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_err_cdf| alt text| width = 500px}}')

                num_bins = len(abs_err_l)
                counts, bin_edges = np.histogram(abs_err_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_abs-err_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_abs-err_cdf| alt text| width = 500px}}')

                # outF.write('|| Table ||\n\n')

                num_bins = len(gull_l)
                counts, bin_edges = np.histogram(gull_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='m', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Gullibility', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_gull_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_gull_cdf| alt text| width = 500px}}')

                num_bins = len(cyn_l)
                counts, bin_edges = np.histogram(cyn_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='k', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Cynicality', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_cyn_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_cyn_cdf| alt text| width = 500px}}||\n')


                # mplpl.show()
                # exit()
                # col_l = ['r', 'b', 'g']
                #
                # i = 0
                # for data_s in gt_l_dict.keys():
                #
                #     mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=400, color=col_l[i], marker='*')
                #     mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([-2, 2])
                #     mplpl.ylabel('Perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i+=1
                #
                # mplpl.legend(loc="upper right")
                #
                # mplpl.grid()
                # # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception_scatter'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                # i = 0
                # mark_l = ['*', 'o', '^']
                # for data_s in gt_l_dict.keys():
                #
                #     # mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([-2, 2])
                #     mplpl.ylabel('Perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i+=1
                #
                # mplpl.legend(loc="upper right")
                #
                # mplpl.grid()
                # # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                # i=0
                # for data_s in gt_l_dict.keys():
                #
                #     mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o')
                #     mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=300, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([0, 2])
                #     mplpl.ylabel('Absolute perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i += 1
                #
                # mplpl.grid()
                # mplpl.legend(loc="upper right")
                #
                # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception_scatter'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                # i=0
                # for data_s in gt_l_dict.keys():
                #
                #     # mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([0, 2])
                #     mplpl.ylabel('Absolute perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i += 1
                #
                # mplpl.grid()
                # mplpl.legend(loc="upper right")
                #
                # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                #





            elif fig_f_together==True:

                ####ptl_cdf
                mplpl.rcParams['figure.figsize'] = 5.4, 3.2
                mplpl.rc('xtick', labelsize='x-large')
                mplpl.rc('ytick', labelsize='x-large')
                mplpl.rc('legend', fontsize='medium')
                w_err_avg_dict
                # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
                tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
                acc_l = []
                for t_id in tweet_l_sort:
                    acc_l.append(w_err_avg_dict[t_id])

                num_bins = len(acc_l)
                counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)

        legend_properties = {'weight': 'bold'}

        #
        mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        mplpl.xlabel('Mean Perception Bias', fontsize=20, fontweight = 'bold')
        mplpl.legend(loc="upper left", prop=legend_properties, fontsize='medium', ncol=1)
        # mplpl.title(data_name)
        # mplpl.legend(loc="upper left",fontsize = 'large')
        mplpl.xlim([-1.5, 1.5])
        mplpl.ylim([0, 1])
        mplpl.grid()
        mplpl.subplots_adjust(bottom=0.24)
        mplpl.subplots_adjust(left=0.18)
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/MPB_cdf_alldataset_new'
        mplpl.savefig(pp + '.pdf', format='pdf')
        mplpl.savefig(pp + '.png', format='png')



        # elif fig_f_1==True:
            #     balance_f = 'un_balanced'
            #     # balance_f = 'balanced'
            #
            #     # fig_f = True
            #     # fig_f = False
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #
            #     fig_p = 7
            #
            #     for ind in ind_l:
            #
            #         df_m = df[ind].copy()
            #
            #         groupby_ftr = 'worker_id'
            #         grouped = df_m.groupby(groupby_ftr, sort=False)
            #         grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #         # df_tmp = df_m[df_m['tweet_id'] == t_id]
            #         w_cc=0
            #         for w_id in grouped.groups.keys():
            #             print(w_cc)
            #             w_cc+=1
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             df_tmp = df_m[df_m['worker_id'] == w_id]
            #             ind_t = df_tmp.index.tolist()[0]
            #             weights = []
            #             w_acc_list_tmp = list(df_tmp['acc'])
            #             w_acc_list = []
            #             for el in w_acc_list_tmp:
            #                 if el == 0:
            #                     w_acc_list.append(-1)
            #                 elif el == 1:
            #                     w_acc_list.append(1)
            #                 else:
            #                     w_acc_list.append(0)
            #             df_tt = pd.DataFrame({'val' : w_acc_list})
            #
            #             weights.append(np.ones_like(list(df_tt['val'])) / float(len(list(df_tt['val']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(w_acc_list), np.median(w_acc_list), np.var(w_acc_list)]
            #             # tweet_avg[t_id] = np.mean(w_acc_list)
            #             # tweet_med[t_id] = np.median(w_acc_list)
            #             # tweet_var[t_id] = np.var(w_acc_list)
            #             #
            #             # tweet_avg_l.append(np.mean(w_acc_list))
            #             # tweet_med_l.append(np.median(w_acc_list))
            #             # tweet_var_l.append(np.var(w_acc_list))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==1:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tt['val'].plot(kind='kde', lw=4, color='g', label='Accuracy')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tt['val']), weights=weights, color='g')
            #
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Accuracy')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(w_acc_list), 3))+', Var : '
            #                             + str(np.round(np.var(w_acc_list), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_acc_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_acc_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #
            #             weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(list(df_tmp['rel_v']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['rel_v'])), np.median(list(df_tmp['rel_v'])), np.var(list(df_tmp['rel_v']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['rel_v']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['rel_v']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['rel_v']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['rel_v'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['rel_v'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['rel_v'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==6:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['rel_v'].plot(kind='kde', lw=4, color='c', label='Perceive truth value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['rel_v']), weights=weights, color='c')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perceive truth value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['rel_v'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['rel_v'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pt_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pt_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['err'])) / float(len(list(df_tmp['err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['err'])), np.median(list(df_tmp['err'])),
            #             #                            np.var(list(df_tmp['err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==2:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['err'].plot(kind='kde', lw=4, color='y', label='Perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['abs_err'])) / float(len(list(df_tmp['abs_err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['abs_err'])), np.median(list(df_tmp['abs_err'])),
            #             #                            np.var(list(df_tmp['abs_err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['abs_err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['abs_err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['abs_err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['abs_err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['abs_err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['abs_err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==3:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['abs_err'].plot(kind='kde', lw=4, color='y', label='Absolute perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['abs_err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Absolute perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['abs_err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['abs_err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_apb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_apb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['gull'])) / float(len(list(df_tmp['gull']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['gull'])), np.median(list(df_tmp['gull'])),
            #             #                            np.var(list(df_tmp['gull']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['gull']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['gull']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['gull']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['gull'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['gull'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['gull'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==4:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['gull'].plot(kind='kde', lw=4, color='k', label='Gullibility value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['gull']), weights=weights, color='k')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Gullibility value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['gull'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['gull'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_gull_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_gull_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['cyn'])) / float(len(list(df_tmp['cyn']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['cyn'])), np.median(list(df_tmp['cyn'])),
            #             #                            np.var(list(df_tmp['cyn']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['cyn']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['cyn']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['cyn']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['cyn'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['cyn'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['cyn'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==5:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['cyn'].plot(kind='kde', lw=4, color='m', label='Cynicality value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['cyn']), weights=weights, color='m')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Cynicality value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['cyn'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['cyn'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_cyn_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_cyn_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #     exit()
            # else:
            #
            #     AVG_list = []
            #     print(np.mean(all_acc))
            #     outF = open(remotedir + 'output.txt', 'w')
            #
            #     tweet_all_var = {}
            #     tweet_all_dev_avg = {}
            #     tweet_all_avg = {}
            #     tweet_all_gt_var = {}
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #     tweet_all_avg_l = []
            #     tweet_all_med_l = []
            #     tweet_all_var_l = []
            #     tweet_all_gt_var_l = []
            #     diff_group_disp_l = []
            #     dem_disp_l = []
            #     rep_disp_l = []
            #
            #     tweet_all_dev_avg = {}
            #     tweet_all_dev_med = {}
            #     tweet_all_dev_var = {}
            #
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #
            #     tweet_all_abs_dev_avg = {}
            #     tweet_all_abs_dev_med = {}
            #     tweet_all_abs_dev_var = {}
            #
            #     tweet_all_abs_dev_avg_l = []
            #     tweet_all_abs_dev_med_l = []
            #     tweet_all_abs_dev_var_l = []
            #     tweet_all_dev_avg_rnd = {}
            #     tweet_all_abs_dev_avg_rnd = {}
            #
            #     diff_group_disp_dict = {}
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #         news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #         news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #         news_cat_list = ['rumor', 'non-rumor']
            #         ind_l = [1]
            #
            #     for cat_l in news_cat_list:
            #         outF.write('== ' + str(cat_l) + ' ==\n\n')
            #         print('== ' + str(cat_l) + ' ==')
            #         tweet_dev_avg = {}
            #         tweet_dev_med = {}
            #         tweet_dev_var = {}
            #         tweet_abs_dev_avg = {}
            #         tweet_abs_dev_med = {}
            #         tweet_abs_dev_var = {}
            #
            #         tweet_avg = {}
            #         tweet_med = {}
            #         tweet_var = {}
            #         tweet_gt_var = {}
            #
            #         tweet_dev_avg_rnd = {}
            #         tweet_abs_dev_avg_rnd = {}
            #
            #
            #         tweet_dev_avg_l = []
            #         tweet_dev_med_l = []
            #         tweet_dev_var_l = []
            #         tweet_abs_dev_avg_l = []
            #         tweet_abs_dev_med_l = []
            #         tweet_abs_dev_var_l = []
            #
            #         tweet_avg_l = []
            #         tweet_med_l = []
            #         tweet_var_l = []
            #         tweet_gt_var_l = []
            #         AVG_susc_list = []
            #         AVG_wl_list = []
            #         all_acc = []
            #         AVG_dev_list = []
            #         # for lean in [-1, 0, 1]:
            #
            #             # AVG_susc_list = []
            #             # AVG_wl_list = []
            #             # all_acc = []
            #             # df_m = df_m[df_m['leaning'] == lean]
            #             # if lean == 0:
            #             #     col = 'g'
            #             #     lean_cat = 'neutral'
            #             # elif lean == 1:
            #             #     col = 'b'
            #             #     lean_cat = 'democrat'
            #             # elif lean == -1:
            #             #     col = 'r'
            #             #     lean_cat = 'republican'
            #             # print(lean_cat)
            #         for ind in ind_l:
            #
            #             if balance_f == 'balanced':
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
            #             else:
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'
            #
            #             inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
            #             df[ind] = pd.read_csv(inp1, sep="\t")
            #             df_w[ind] = pd.read_csv(inp1_w, sep="\t")
            #
            #             df_m = df[ind].copy()
            #             df_mm = df_m.copy()
            #
            #             df_m = df_m[df_m['ra_gt'] == cat_l]
            #             # df_mm = df_m[df_m['ra_gt']==cat_l]
            #             # df_m = df_m[df_m['leaning'] == lean]
            #
            #             groupby_ftr = 'tweet_id'
            #             grouped = df_m.groupby(groupby_ftr, sort=False)
            #             grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #             for t_id in grouped.groups.keys():
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #
            #                 df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
            #                 df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
            #                 df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #
            #                 weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
            #                 val_list = list(df_tmp['rel_v'])
            #                 tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_avg[t_id] = np.mean(val_list)
            #                 tweet_med[t_id] = np.median(val_list)
            #                 tweet_var[t_id] = np.var(val_list)
            #                 tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_avg_l.append(np.mean(val_list))
            #                 tweet_med_l.append(np.median(val_list))
            #                 tweet_var_l.append(np.var(val_list))
            #                 tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #
            #                 tweet_all_avg[t_id] = np.mean(val_list)
            #                 tweet_all_var[t_id] = np.var(val_list)
            #                 tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_all_avg_l.append(np.mean(val_list))
            #                 tweet_all_med_l.append(np.median(val_list))
            #                 tweet_all_var_l.append(np.var(val_list))
            #                 tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #                 val_list = list(df_tmp['err'])
            #                 abs_var_err = [np.abs(x) for x in val_list]
            #                 tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_dev_med[t_id] = np.median(val_list)
            #                 tweet_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_dev_avg_l.append(np.mean(val_list))
            #                 tweet_dev_med_l.append(np.median(val_list))
            #                 tweet_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #                 tweet_all_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_all_dev_med[t_id] = np.median(val_list)
            #                 tweet_all_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_all_dev_avg_l.append(np.mean(val_list))
            #                 tweet_all_dev_med_l.append(np.median(val_list))
            #                 tweet_all_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_all_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #
            #                 sum_rnd_abs_perc = 0
            #                 sum_rnd_perc = 0
            #                 for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
            #                     sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
            #                     sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
            #                 random_perc = np.abs(sum_rnd_perc / float(7))
            #                 random_abs_perc = sum_rnd_abs_perc / float(7)
            #
            #                 tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #                 tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #         gt_l = []
            #         pt_l = []
            #         disputability_l = []
            #         perc_l = []
            #         abs_perc_l = []
            #         # for t_id in tweet_l_sort:
            #         #     gt_l.append(tweet_gt_var[t_id])
            #         #     pt_l.append(tweet_avg[t_id])
            #         #     disputability_l.append(tweet_var[t_id])
            #         #     perc_l.append(tweet_dev_avg[t_id])
            #         #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #         tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #
            #
            #         if dataset == 'snopes':
            #             data_addr = 'snopes'
            #         elif dataset == 'politifact':
            #             data_addr = 'politifact/fig'
            #         elif dataset == 'mia':
            #             data_addr = 'mia/fig'
            #
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #
            #         for t_id in tweet_l_sort:
            #             count+=1
            #             if balance_f=='balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
            #                            + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
            #                            # str(np.round(diff_group_disp_dict[t_id], 3)) +
            #                            '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
            #                             + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
            #                             '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #
            #
            #
            #
            #     if dataset == 'snopes':
            #         data_addr = 'snopes'
            #     elif dataset == 'politifact':
            #         data_addr = 'politifact/fig'
            #     elif dataset == 'mia':
            #         data_addr = 'mia/fig'
            #
            #     # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
            #     tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #
            #     # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #
            #     tweet_napb_dict_high_disp = {}
            #     tweet_napb_dict_low_disp = {}
            #     for t_id in tweet_l_sort[:20]:
            #         # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     for t_id in tweet_l_sort[-20:]:
            #         # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     kk = 0
            #
            #     for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
            #         if kk==0:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
            #         else:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)
            #
            #         kk+=1
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         for t_id in tweet_l_sort:
            #             count += 1
            #             # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
            #             if balance_f == 'balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
            #                            str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #                 #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
            #                 #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
            #                            str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #             # +
            #             # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
            #             # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_apb_cdf_toghether":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                #     tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','snopes_nonpol','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                tweet_id = 100010
                publisher_name = 110
                tweet_popularity = {}
                tweet_text_dic = {}
                for input_file in [input_rumor, input_non_rumor]:
                    for line in input_file:
                        line.replace('\n', '')
                        line_splt = line.split('\t')
                        tweet_txt = line_splt[1]
                        tweet_link = line_splt[1]
                        tweet_id += 1
                        publisher_name += 1
                        tweet_popularity[tweet_id] = int(line_splt[2])
                        tweet_text_dic[tweet_id] = tweet_txt



                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                    # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    #     tweet_lable_dict[tweet_id] = 'undecided'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes_nonpol':
                claims_list = []
                col = 'r'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'snopes':
                claims_list = []
                col = 'r'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'politifact':
                col = 'g'

                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                col = 'purple'

                data_name = 'Snopes'
            if dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                col = 'green'

                data_name = 'Snopes\nnonpolitical'
            elif dataset=='politifact':
                col = 'c'
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                col = 'orange'
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()
                df[ind].loc[:, 'abs_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_abs_err'] = df[ind]['tweet_id'] * 0.0

                groupby_ftr = 'tweet_id'
                grouped = df[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df[ind].groupby(groupby_ftr, sort=False).sum()


                for ind_t in df[ind].index.tolist():
                    t_id = df[ind]['tweet_id'][ind_t]
                    err = df[ind]['err'][ind_t]
                    abs_err = np.abs(err)
                    df[ind]['abs_err'][ind_t] = abs_err
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= (val - df[ind]['rel_gt_v'][ind_t])
                        sum_rnd_abs_perc += np.abs(val - df[ind]['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)


                    norm_err = err / float(random_perc)
                    norm_abs_err = abs_err / float(random_abs_perc)
                    df[ind]['norm_err'][ind_t] = norm_err
                    df[ind]['norm_abs_err'][ind_t] = norm_abs_err

                # df[ind] = df[ind].copy()

            w_pt_avg_l = []
            w_err_avg_l = []
            w_abs_err_avg_l = []
            w_norm_err_avg_l = []
            w_norm_abs_err_avg_l = []
            w_acc_avg_l = []

            w_pt_std_l = []
            w_err_std_l = []
            w_abs_err_std_l = []
            w_norm_err_std_l = []
            w_norm_abs_err_std_l = []
            w_acc_std_l = []

            w_pt_avg_dict = collections.defaultdict()
            w_err_avg_dict = collections.defaultdict()
            w_abs_err_avg_dict = collections.defaultdict()
            w_norm_err_avg_dict = collections.defaultdict()
            w_norm_abs_err_avg_dict = collections.defaultdict()
            w_acc_avg_dict = collections.defaultdict()

            w_pt_std_dict = collections.defaultdict()
            w_err_std_dict = collections.defaultdict()
            w_abs_err_std_dict = collections.defaultdict()
            w_norm_err_std_dict = collections.defaultdict()
            w_norm_abs_err_std_dict = collections.defaultdict()
            w_acc_std_dict = collections.defaultdict()

            all_w_pt_list  = []
            all_w_err_list = []
            all_w_abs_err_list = []
            all_w_norm_err_list = []
            all_w_norm_abs_err_list  = []
            all_w_acc_list = []

            all_w_cyn_list = []
            all_w_gull_list = []
            w_cyn_avg_l = []
            w_gull_avg_l = []
            w_cyn_std_l= []
            w_gull_std_l = []
            w_cyn_avg_dict =collections.defaultdict()
            w_gull_avg_dict =collections.defaultdict()
            w_cyn_std_dict =collections.defaultdict()
            w_gull_std_dict = collections.defaultdict()
            for ind in ind_l:

                df_m = df[ind].copy()
                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]


                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    # w_abs_err_list = list(df_tmp['abs_err'])
                    w_abs_err_list = list(df_tmp['susc'])
                    w_norm_err_list = list(df_tmp['norm_err'])
                    w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    df_cyn = df_tmp[df_tmp['cyn']>0]
                    df_gull = df_tmp[df_tmp['gull']>0]

                    w_cyn_list = list(df_cyn['cyn'])
                    w_gull_list = list(df_gull['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])
                    w_acc_list = []
                    # w_ind_acc_list
                    acc_c = 0
                    nacc_c = 0




                    # w_acc_avg_l.append(w_ind_acc_list)

                    w_pt_std_l.append(np.std(w_pt_list))
                    w_err_std_l.append(np.std(w_err_list))
                    w_abs_err_std_l.append(np.std(w_abs_err_list))
                    w_norm_err_std_l.append(np.std(w_norm_err_list))
                    w_norm_abs_err_std_l.append(np.std(w_norm_abs_err_list))
                    w_cyn_std_l.append(np.std(w_cyn_list))
                    w_gull_std_l.append(np.std(w_gull_list))
                    # w_acc_std_l.append(np.std(w_ind_acc_list))


                    w_pt_avg_dict[t_id] = np.mean(w_pt_list)
                    w_err_avg_dict[t_id] = np.mean(w_err_list)
                    w_abs_err_avg_dict[t_id] = np.mean(w_abs_err_list)
                    w_norm_err_avg_dict[t_id] = np.mean(w_norm_err_list)
                    w_norm_abs_err_avg_dict[t_id] = np.mean(w_norm_abs_err_list)
                    w_cyn_avg_dict[t_id] = np.mean(w_cyn_list)
                    w_gull_avg_dict[t_id] = np.mean(w_gull_list)
                    # w_acc_avg_dict[t_id] = w_ind_acc_list


                # ind_
            ##################################################
            #
            # tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            # gt_l = []
            # pt_l = []
            # disputability_l = []
            # perc_l = []
            # abs_perc_l=[]
            # abs_perc_rnd_l = []
            # perc_rnd_l = []
            # tweet_skew_ll = []
            # for t_id in tweet_l_sort:
            #     gt_l.append(tweet_gt_var[t_id])
            #     pt_l.append(tweet_avg[t_id])
            #     disputability_l.append(tweet_var[t_id])
            #     perc_l.append(tweet_dev_avg[t_id])
            #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #     perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
            #     abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
            #     tweet_skew_ll.append(tweet_skew[t_id])
            # value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l,tweet_skew_ll]
            # value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
            #               'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'skewness']
            #
            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(8):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(8):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')
            #
            #
            # exit()

            # fig_f = True
            fig_f = False
            # fig_f_1 = True
            fig_f_1 = False
            fig_f_together = True
            if fig_f==True:

                df_tmp = pd.DataFrame({'val' : all_w_acc_list})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), weights=weights, color='g')
                # mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 1.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Readers accuracy', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_acc_list),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_acc_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_acc_density| alt text| width = 500px}}')



                # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
                df_tmp = pd.DataFrame({'val' : w_acc_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                # mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers accuracy', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_acc_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_acc_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_acc_pt| alt text| width = 500px}}')




                df_tmp = pd.DataFrame({'val' : all_w_pt_list})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), weights=weights, color='c')
                # mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 1.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Readers percevied truth value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_pt_list),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_pt_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_pt_density| alt text| width = 500px}}')




                # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
                df_tmp = pd.DataFrame({'val' : w_pt_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                mplpl.hist(list(df_tmp['val']), normed=1, color='c')


                # mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 3.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers percevied truth value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_pt_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_pt_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_pt_density| alt text| width = 500px}}')




                df_tmp = pd.DataFrame({'val' : w_err_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='y')


                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 3.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers perception bias value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_err_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_pb_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_pb_density| alt text| width = 500px}}')



                df_tmp = pd.DataFrame({'val' : w_abs_err_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='y')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers absolute perception bias value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_abs_err_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_apb_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_apb_density| alt text| width = 500px}}||')



                df_tmp = pd.DataFrame({'val' : w_gull_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='m')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 10])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers gullibility value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_gull_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_gull_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_gull_density| alt text| width = 500px}}||')




                df_tmp = pd.DataFrame({'val' : w_cyn_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='k')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 10])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers cynicallity value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_cyn_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_cyn_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_cyn_density| alt text| width = 500px}}||\n\n')



        ########################

                tweet_l_sort = sorted(w_pt_avg_dict, key=w_pt_avg_dict.get, reverse=False)
                pt_l = []
                for t_id in tweet_l_sort:
                    pt_l.append(w_pt_avg_dict[t_id])

                mplpl.scatter(range(len(pt_l)), pt_l,  s=40,color='c',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ranked readers according PTL', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_pt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_pt_pt| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_acc_avg_dict, key=w_acc_avg_dict.get, reverse=False)
                acc_l = []
                for t_id in tweet_l_sort:
                    acc_l.append(w_acc_avg_dict[t_id])


                mplpl.scatter(range(len(acc_l)), acc_l ,  s=40,color='g',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Accuracy', fontsize=18)
                mplpl.xlabel('Ranked readers according accuracy', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(acc_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_acc_acc'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_acc_acc| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
                err_l = []
                for t_id in tweet_l_sort:
                    err_l.append(w_err_avg_dict[t_id])

                mplpl.scatter(range(len(err_l)), err_l,  s=40,color='y',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias (PB)', fontsize=18)
                mplpl.xlabel('Ranked readers according PB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(err_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_err_err'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_err_err| alt text| width = 500px}}')



                tweet_l_sort = sorted(w_abs_err_avg_dict, key=w_abs_err_avg_dict.get, reverse=False)
                abs_err_l = []
                for t_id in tweet_l_sort:
                    abs_err_l.append(w_abs_err_avg_dict[t_id])

                mplpl.scatter(range(len(abs_err_l)), abs_err_l,  s=40,color='y',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according APB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_err_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_abs-err_abs-err'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_abs-err_abs-err| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_gull_avg_dict, key=w_gull_avg_dict.get, reverse=False)
                gull_l = []
                for t_id in tweet_l_sort:
                    gull_l.append(w_gull_avg_dict[t_id])

                mplpl.scatter(range(len(gull_l)), gull_l,  s=40,color='m',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Gullibility', fontsize=18)
                mplpl.xlabel('Ranked readers according gullibility', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(gull_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_gull_gull'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_gull_gull| alt text| width = 500px}}')



                tweet_l_sort = sorted(w_cyn_avg_dict, key=w_cyn_avg_dict.get, reverse=False)
                cyn_l = []
                for t_id in tweet_l_sort:
                    cyn_l.append(w_cyn_avg_dict[t_id])

                mplpl.scatter(range(len(cyn_l)), cyn_l,  s=40,color='k',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Cynicality', fontsize=18)
                mplpl.xlabel('Ranked readers according cynicality', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(cyn_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_cyn_cyn'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_cyn_cyn| alt text| width = 500px}}||\n\n')



                # outF.write('|| Table ||\n\n')

                # mplpl.show()
                # exit()
        #####################################################33


                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_pt_cdf| alt text| width = 500px}}')


                num_bins = len(acc_l)
                counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Accuracy', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_acc_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_acc_cdf| alt text| width = 500px}}')


                num_bins = len(err_l)
                counts, bin_edges = np.histogram(err_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception bias (PB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_err_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_err_cdf| alt text| width = 500px}}')

                num_bins = len(abs_err_l)
                counts, bin_edges = np.histogram(abs_err_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_abs-err_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_abs-err_cdf| alt text| width = 500px}}')

                # outF.write('|| Table ||\n\n')

                num_bins = len(gull_l)
                counts, bin_edges = np.histogram(gull_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='m', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Gullibility', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_gull_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_gull_cdf| alt text| width = 500px}}')

                num_bins = len(cyn_l)
                counts, bin_edges = np.histogram(cyn_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='k', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Cynicality', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_cyn_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_cyn_cdf| alt text| width = 500px}}||\n')


                # mplpl.show()
                # exit()
                # col_l = ['r', 'b', 'g']
                #
                # i = 0
                # for data_s in gt_l_dict.keys():
                #
                #     mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=400, color=col_l[i], marker='*')
                #     mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([-2, 2])
                #     mplpl.ylabel('Perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i+=1
                #
                # mplpl.legend(loc="upper right")
                #
                # mplpl.grid()
                # # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception_scatter'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                # i = 0
                # mark_l = ['*', 'o', '^']
                # for data_s in gt_l_dict.keys():
                #
                #     # mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([-2, 2])
                #     mplpl.ylabel('Perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i+=1
                #
                # mplpl.legend(loc="upper right")
                #
                # mplpl.grid()
                # # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                # i=0
                # for data_s in gt_l_dict.keys():
                #
                #     mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o')
                #     mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=300, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([0, 2])
                #     mplpl.ylabel('Absolute perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i += 1
                #
                # mplpl.grid()
                # mplpl.legend(loc="upper right")
                #
                # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception_scatter'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                # i=0
                # for data_s in gt_l_dict.keys():
                #
                #     # mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([0, 2])
                #     mplpl.ylabel('Absolute perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i += 1
                #
                # mplpl.grid()
                # mplpl.legend(loc="upper right")
                #
                # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                #





            elif fig_f_together==True:
                # out_dict = w_abs_err_avg_dict
                # out_dict = w_gull_avg_dict
                out_dict = w_cyn_avg_dict
                ####ptl_cdf
                mplpl.rcParams['figure.figsize'] = 4.5, 2.5
                mplpl.rc('xtick', labelsize='large')
                mplpl.rc('ytick', labelsize='large')
                mplpl.rc('legend', fontsize='medium')
                w_err_avg_dict
                # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
                tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
                # tweet_l_sort = [x for x in tweet_l_sort if x >= 0 or x < 0]
                acc_l = []
                for t_id in tweet_l_sort:
                    if out_dict[t_id] >=0 or out_dict[t_id]<0:
                        acc_l.append(out_dict[t_id])

                num_bins = len(acc_l)
                counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)

        legend_properties = {'weight': 'bold'}

        #
        mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        # mplpl.xlabel('Total Perception Bias', fontsize=20, fontweight = 'bold')
        # mplpl.xlabel('False Positive Bias', fontsize=20, fontweight = 'bold')
        mplpl.xlabel('False Negative Bias', fontsize=20, fontweight = 'bold')
        mplpl.legend(loc="lower right", prop=legend_properties, fontsize='medium', ncol=1)
        # mplpl.title(data_name)
        # mplpl.legend(loc="upper left",fontsize = 'large')
        mplpl.xlim([0, 2])
        mplpl.ylim([0, 1])
        mplpl.grid()
        mplpl.subplots_adjust(bottom=0.24)
        mplpl.subplots_adjust(left=0.18)
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/APB_cdf_alldataset_news'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/FPB_cdf_alldataset_new'
        pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/FNB_cdf_alldataset_new'
        mplpl.savefig(pp + '.pdf', format='pdf')
        mplpl.savefig(pp + '.png', format='png')



        # elif fig_f_1==True:
            #     balance_f = 'un_balanced'
            #     # balance_f = 'balanced'
            #
            #     # fig_f = True
            #     # fig_f = False
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #
            #     fig_p = 7
            #
            #     for ind in ind_l:
            #
            #         df_m = df[ind].copy()
            #
            #         groupby_ftr = 'worker_id'
            #         grouped = df_m.groupby(groupby_ftr, sort=False)
            #         grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #         # df_tmp = df_m[df_m['tweet_id'] == t_id]
            #         w_cc=0
            #         for w_id in grouped.groups.keys():
            #             print(w_cc)
            #             w_cc+=1
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             df_tmp = df_m[df_m['worker_id'] == w_id]
            #             ind_t = df_tmp.index.tolist()[0]
            #             weights = []
            #             w_acc_list_tmp = list(df_tmp['acc'])
            #             w_acc_list = []
            #             for el in w_acc_list_tmp:
            #                 if el == 0:
            #                     w_acc_list.append(-1)
            #                 elif el == 1:
            #                     w_acc_list.append(1)
            #                 else:
            #                     w_acc_list.append(0)
            #             df_tt = pd.DataFrame({'val' : w_acc_list})
            #
            #             weights.append(np.ones_like(list(df_tt['val'])) / float(len(list(df_tt['val']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(w_acc_list), np.median(w_acc_list), np.var(w_acc_list)]
            #             # tweet_avg[t_id] = np.mean(w_acc_list)
            #             # tweet_med[t_id] = np.median(w_acc_list)
            #             # tweet_var[t_id] = np.var(w_acc_list)
            #             #
            #             # tweet_avg_l.append(np.mean(w_acc_list))
            #             # tweet_med_l.append(np.median(w_acc_list))
            #             # tweet_var_l.append(np.var(w_acc_list))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==1:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tt['val'].plot(kind='kde', lw=4, color='g', label='Accuracy')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tt['val']), weights=weights, color='g')
            #
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Accuracy')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(w_acc_list), 3))+', Var : '
            #                             + str(np.round(np.var(w_acc_list), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_acc_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_acc_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #
            #             weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(list(df_tmp['rel_v']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['rel_v'])), np.median(list(df_tmp['rel_v'])), np.var(list(df_tmp['rel_v']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['rel_v']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['rel_v']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['rel_v']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['rel_v'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['rel_v'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['rel_v'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==6:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['rel_v'].plot(kind='kde', lw=4, color='c', label='Perceive truth value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['rel_v']), weights=weights, color='c')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perceive truth value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['rel_v'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['rel_v'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pt_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pt_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['err'])) / float(len(list(df_tmp['err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['err'])), np.median(list(df_tmp['err'])),
            #             #                            np.var(list(df_tmp['err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==2:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['err'].plot(kind='kde', lw=4, color='y', label='Perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['abs_err'])) / float(len(list(df_tmp['abs_err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['abs_err'])), np.median(list(df_tmp['abs_err'])),
            #             #                            np.var(list(df_tmp['abs_err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['abs_err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['abs_err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['abs_err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['abs_err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['abs_err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['abs_err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==3:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['abs_err'].plot(kind='kde', lw=4, color='y', label='Absolute perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['abs_err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Absolute perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['abs_err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['abs_err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_apb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_apb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['gull'])) / float(len(list(df_tmp['gull']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['gull'])), np.median(list(df_tmp['gull'])),
            #             #                            np.var(list(df_tmp['gull']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['gull']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['gull']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['gull']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['gull'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['gull'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['gull'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==4:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['gull'].plot(kind='kde', lw=4, color='k', label='Gullibility value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['gull']), weights=weights, color='k')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Gullibility value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['gull'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['gull'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_gull_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_gull_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['cyn'])) / float(len(list(df_tmp['cyn']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['cyn'])), np.median(list(df_tmp['cyn'])),
            #             #                            np.var(list(df_tmp['cyn']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['cyn']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['cyn']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['cyn']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['cyn'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['cyn'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['cyn'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==5:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['cyn'].plot(kind='kde', lw=4, color='m', label='Cynicality value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['cyn']), weights=weights, color='m')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Cynicality value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['cyn'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['cyn'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_cyn_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_cyn_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #     exit()
            # else:
            #
            #     AVG_list = []
            #     print(np.mean(all_acc))
            #     outF = open(remotedir + 'output.txt', 'w')
            #
            #     tweet_all_var = {}
            #     tweet_all_dev_avg = {}
            #     tweet_all_avg = {}
            #     tweet_all_gt_var = {}
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #     tweet_all_avg_l = []
            #     tweet_all_med_l = []
            #     tweet_all_var_l = []
            #     tweet_all_gt_var_l = []
            #     diff_group_disp_l = []
            #     dem_disp_l = []
            #     rep_disp_l = []
            #
            #     tweet_all_dev_avg = {}
            #     tweet_all_dev_med = {}
            #     tweet_all_dev_var = {}
            #
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #
            #     tweet_all_abs_dev_avg = {}
            #     tweet_all_abs_dev_med = {}
            #     tweet_all_abs_dev_var = {}
            #
            #     tweet_all_abs_dev_avg_l = []
            #     tweet_all_abs_dev_med_l = []
            #     tweet_all_abs_dev_var_l = []
            #     tweet_all_dev_avg_rnd = {}
            #     tweet_all_abs_dev_avg_rnd = {}
            #
            #     diff_group_disp_dict = {}
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #         news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #         news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #         news_cat_list = ['rumor', 'non-rumor']
            #         ind_l = [1]
            #
            #     for cat_l in news_cat_list:
            #         outF.write('== ' + str(cat_l) + ' ==\n\n')
            #         print('== ' + str(cat_l) + ' ==')
            #         tweet_dev_avg = {}
            #         tweet_dev_med = {}
            #         tweet_dev_var = {}
            #         tweet_abs_dev_avg = {}
            #         tweet_abs_dev_med = {}
            #         tweet_abs_dev_var = {}
            #
            #         tweet_avg = {}
            #         tweet_med = {}
            #         tweet_var = {}
            #         tweet_gt_var = {}
            #
            #         tweet_dev_avg_rnd = {}
            #         tweet_abs_dev_avg_rnd = {}
            #
            #
            #         tweet_dev_avg_l = []
            #         tweet_dev_med_l = []
            #         tweet_dev_var_l = []
            #         tweet_abs_dev_avg_l = []
            #         tweet_abs_dev_med_l = []
            #         tweet_abs_dev_var_l = []
            #
            #         tweet_avg_l = []
            #         tweet_med_l = []
            #         tweet_var_l = []
            #         tweet_gt_var_l = []
            #         AVG_susc_list = []
            #         AVG_wl_list = []
            #         all_acc = []
            #         AVG_dev_list = []
            #         # for lean in [-1, 0, 1]:
            #
            #             # AVG_susc_list = []
            #             # AVG_wl_list = []
            #             # all_acc = []
            #             # df_m = df_m[df_m['leaning'] == lean]
            #             # if lean == 0:
            #             #     col = 'g'
            #             #     lean_cat = 'neutral'
            #             # elif lean == 1:
            #             #     col = 'b'
            #             #     lean_cat = 'democrat'
            #             # elif lean == -1:
            #             #     col = 'r'
            #             #     lean_cat = 'republican'
            #             # print(lean_cat)
            #         for ind in ind_l:
            #
            #             if balance_f == 'balanced':
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
            #             else:
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'
            #
            #             inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
            #             df[ind] = pd.read_csv(inp1, sep="\t")
            #             df_w[ind] = pd.read_csv(inp1_w, sep="\t")
            #
            #             df_m = df[ind].copy()
            #             df_mm = df_m.copy()
            #
            #             df_m = df_m[df_m['ra_gt'] == cat_l]
            #             # df_mm = df_m[df_m['ra_gt']==cat_l]
            #             # df_m = df_m[df_m['leaning'] == lean]
            #
            #             groupby_ftr = 'tweet_id'
            #             grouped = df_m.groupby(groupby_ftr, sort=False)
            #             grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #             for t_id in grouped.groups.keys():
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #
            #                 df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
            #                 df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
            #                 df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #
            #                 weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
            #                 val_list = list(df_tmp['rel_v'])
            #                 tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_avg[t_id] = np.mean(val_list)
            #                 tweet_med[t_id] = np.median(val_list)
            #                 tweet_var[t_id] = np.var(val_list)
            #                 tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_avg_l.append(np.mean(val_list))
            #                 tweet_med_l.append(np.median(val_list))
            #                 tweet_var_l.append(np.var(val_list))
            #                 tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #
            #                 tweet_all_avg[t_id] = np.mean(val_list)
            #                 tweet_all_var[t_id] = np.var(val_list)
            #                 tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_all_avg_l.append(np.mean(val_list))
            #                 tweet_all_med_l.append(np.median(val_list))
            #                 tweet_all_var_l.append(np.var(val_list))
            #                 tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #                 val_list = list(df_tmp['err'])
            #                 abs_var_err = [np.abs(x) for x in val_list]
            #                 tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_dev_med[t_id] = np.median(val_list)
            #                 tweet_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_dev_avg_l.append(np.mean(val_list))
            #                 tweet_dev_med_l.append(np.median(val_list))
            #                 tweet_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #                 tweet_all_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_all_dev_med[t_id] = np.median(val_list)
            #                 tweet_all_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_all_dev_avg_l.append(np.mean(val_list))
            #                 tweet_all_dev_med_l.append(np.median(val_list))
            #                 tweet_all_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_all_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #
            #                 sum_rnd_abs_perc = 0
            #                 sum_rnd_perc = 0
            #                 for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
            #                     sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
            #                     sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
            #                 random_perc = np.abs(sum_rnd_perc / float(7))
            #                 random_abs_perc = sum_rnd_abs_perc / float(7)
            #
            #                 tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #                 tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #         gt_l = []
            #         pt_l = []
            #         disputability_l = []
            #         perc_l = []
            #         abs_perc_l = []
            #         # for t_id in tweet_l_sort:
            #         #     gt_l.append(tweet_gt_var[t_id])
            #         #     pt_l.append(tweet_avg[t_id])
            #         #     disputability_l.append(tweet_var[t_id])
            #         #     perc_l.append(tweet_dev_avg[t_id])
            #         #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #         tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #
            #
            #         if dataset == 'snopes':
            #             data_addr = 'snopes'
            #         elif dataset == 'politifact':
            #             data_addr = 'politifact/fig'
            #         elif dataset == 'mia':
            #             data_addr = 'mia/fig'
            #
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #
            #         for t_id in tweet_l_sort:
            #             count+=1
            #             if balance_f=='balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
            #                            + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
            #                            # str(np.round(diff_group_disp_dict[t_id], 3)) +
            #                            '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
            #                             + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
            #                             '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #
            #
            #
            #
            #     if dataset == 'snopes':
            #         data_addr = 'snopes'
            #     elif dataset == 'politifact':
            #         data_addr = 'politifact/fig'
            #     elif dataset == 'mia':
            #         data_addr = 'mia/fig'
            #
            #     # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
            #     tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #
            #     # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #
            #     tweet_napb_dict_high_disp = {}
            #     tweet_napb_dict_low_disp = {}
            #     for t_id in tweet_l_sort[:20]:
            #         # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     for t_id in tweet_l_sort[-20:]:
            #         # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     kk = 0
            #
            #     for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
            #         if kk==0:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
            #         else:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)
            #
            #         kk+=1
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         for t_id in tweet_l_sort:
            #             count += 1
            #             # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
            #             if balance_f == 'balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
            #                            str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #                 #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
            #                 #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
            #                            str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #             # +
            #             # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
            #             # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_disp_cdf_toghether":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                #     tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','snopes_nonpol','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                tweet_id = 100010
                publisher_name = 110
                tweet_popularity = {}
                tweet_text_dic = {}
                for input_file in [input_rumor, input_non_rumor]:
                    for line in input_file:
                        line.replace('\n', '')
                        line_splt = line.split('\t')
                        tweet_txt = line_splt[1]
                        tweet_link = line_splt[1]
                        tweet_id += 1
                        publisher_name += 1
                        tweet_popularity[tweet_id] = int(line_splt[2])
                        tweet_text_dic[tweet_id] = tweet_txt



                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                    # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    #     tweet_lable_dict[tweet_id] = 'undecided'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                col = 'r'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'snopes_nonpol':
                claims_list = []
                col = 'r'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'politifact':
                col = 'g'

                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                col = 'purple'

                data_name = 'Snopes'
            if dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                col = 'green'

                data_name = 'Snopes\nnonpolitic'
            elif dataset=='politifact':
                col = 'c'
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                col = 'orange'
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()
                df[ind].loc[:, 'abs_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_abs_err'] = df[ind]['tweet_id'] * 0.0

                groupby_ftr = 'tweet_id'
                grouped = df[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df[ind].groupby(groupby_ftr, sort=False).sum()


                for ind_t in df[ind].index.tolist():
                    t_id = df[ind]['tweet_id'][ind_t]
                    err = df[ind]['err'][ind_t]
                    abs_err = np.abs(err)
                    df[ind]['abs_err'][ind_t] = abs_err
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= (val - df[ind]['rel_gt_v'][ind_t])
                        sum_rnd_abs_perc += np.abs(val - df[ind]['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)


                    norm_err = err / float(random_perc)
                    norm_abs_err = abs_err / float(random_abs_perc)
                    df[ind]['norm_err'][ind_t] = norm_err
                    df[ind]['norm_abs_err'][ind_t] = norm_abs_err

                # df[ind] = df[ind].copy()

            w_pt_avg_l = []
            w_err_avg_l = []
            w_abs_err_avg_l = []
            w_norm_err_avg_l = []
            w_norm_abs_err_avg_l = []
            w_acc_avg_l = []

            w_pt_std_l = []
            w_err_std_l = []
            w_abs_err_std_l = []
            w_norm_err_std_l = []
            w_norm_abs_err_std_l = []
            w_acc_std_l = []

            w_pt_avg_dict = collections.defaultdict()
            w_err_avg_dict = collections.defaultdict()
            w_abs_err_avg_dict = collections.defaultdict()
            w_norm_err_avg_dict = collections.defaultdict()
            w_norm_abs_err_avg_dict = collections.defaultdict()
            w_acc_avg_dict = collections.defaultdict()

            w_pt_std_dict = collections.defaultdict()
            w_err_std_dict = collections.defaultdict()
            w_abs_err_std_dict = collections.defaultdict()
            w_norm_err_std_dict = collections.defaultdict()
            w_norm_abs_err_std_dict = collections.defaultdict()
            w_acc_std_dict = collections.defaultdict()

            all_w_pt_list  = []
            all_w_err_list = []
            all_w_abs_err_list = []
            all_w_norm_err_list = []
            all_w_norm_abs_err_list  = []
            all_w_acc_list = []

            all_w_cyn_list = []
            all_w_gull_list = []
            w_cyn_avg_l = []
            w_gull_avg_l = []
            w_cyn_std_l= []
            w_gull_std_l = []
            w_cyn_avg_dict =collections.defaultdict()
            w_gull_avg_dict =collections.defaultdict()
            w_cyn_std_dict =collections.defaultdict()
            w_gull_std_dict = collections.defaultdict()
            w_pt_var_dict = collections.defaultdict()
            for ind in ind_l:

                df_m = df[ind].copy()
                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]


                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    # w_abs_err_list = list(df_tmp['abs_err'])
                    w_abs_err_list = list(df_tmp['susc'])
                    w_norm_err_list = list(df_tmp['norm_err'])
                    w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    df_cyn = df_tmp[df_tmp['cyn']>0]
                    df_gull = df_tmp[df_tmp['gull']>0]

                    w_cyn_list = list(df_cyn['cyn'])
                    w_gull_list = list(df_gull['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])
                    w_acc_list = []
                    # w_ind_acc_list
                    acc_c = 0
                    nacc_c = 0




                    # w_acc_avg_l.append(w_ind_acc_list)

                    w_pt_std_l.append(np.std(w_pt_list))
                    w_err_std_l.append(np.std(w_err_list))
                    w_abs_err_std_l.append(np.std(w_abs_err_list))
                    w_norm_err_std_l.append(np.std(w_norm_err_list))
                    w_norm_abs_err_std_l.append(np.std(w_norm_abs_err_list))
                    w_cyn_std_l.append(np.std(w_cyn_list))
                    w_gull_std_l.append(np.std(w_gull_list))
                    # w_acc_std_l.append(np.std(w_ind_acc_list))


                    w_pt_avg_dict[t_id] = np.mean(w_pt_list)
                    w_err_avg_dict[t_id] = np.mean(w_err_list)
                    w_abs_err_avg_dict[t_id] = np.mean(w_abs_err_list)
                    w_norm_err_avg_dict[t_id] = np.mean(w_norm_err_list)
                    w_norm_abs_err_avg_dict[t_id] = np.mean(w_norm_abs_err_list)
                    w_cyn_avg_dict[t_id] = np.mean(w_cyn_list)
                    w_gull_avg_dict[t_id] = np.mean(w_gull_list)
                    # w_acc_avg_dict[t_id] = w_ind_acc_list


                    w_pt_var_dict[t_id] = np.var(w_pt_list)
                    w_pt_var_dict[t_id] = np.std(w_pt_list)

                # ind_
            ##################################################
            #
            # tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            # gt_l = []
            # pt_l = []
            # disputability_l = []
            # perc_l = []
            # abs_perc_l=[]
            # abs_perc_rnd_l = []
            # perc_rnd_l = []
            # tweet_skew_ll = []
            # for t_id in tweet_l_sort:
            #     gt_l.append(tweet_gt_var[t_id])
            #     pt_l.append(tweet_avg[t_id])
            #     disputability_l.append(tweet_var[t_id])
            #     perc_l.append(tweet_dev_avg[t_id])
            #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #     perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
            #     abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
            #     tweet_skew_ll.append(tweet_skew[t_id])
            # value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l,tweet_skew_ll]
            # value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
            #               'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'skewness']
            #
            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(8):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(8):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')
            #
            #
            # exit()

            # fig_f = True
            fig_f = False
            # fig_f_1 = True
            fig_f_1 = False
            fig_f_together = True
            if fig_f==True:

                df_tmp = pd.DataFrame({'val' : all_w_acc_list})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), weights=weights, color='g')
                # mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 1.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Readers accuracy', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_acc_list),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_acc_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_acc_density| alt text| width = 500px}}')



                # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
                df_tmp = pd.DataFrame({'val' : w_acc_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                # mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers accuracy', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_acc_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_acc_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_acc_pt| alt text| width = 500px}}')




                df_tmp = pd.DataFrame({'val' : all_w_pt_list})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), weights=weights, color='c')
                # mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 1.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Readers percevied truth value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_pt_list),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_pt_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_pt_density| alt text| width = 500px}}')




                # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
                df_tmp = pd.DataFrame({'val' : w_pt_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                mplpl.hist(list(df_tmp['val']), normed=1, color='c')


                # mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 3.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers percevied truth value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_pt_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_pt_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_pt_density| alt text| width = 500px}}')




                df_tmp = pd.DataFrame({'val' : w_err_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='y')


                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 3.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers perception bias value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_err_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_pb_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_pb_density| alt text| width = 500px}}')



                df_tmp = pd.DataFrame({'val' : w_abs_err_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='y')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers absolute perception bias value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_abs_err_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_apb_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_apb_density| alt text| width = 500px}}||')



                df_tmp = pd.DataFrame({'val' : w_gull_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='m')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 10])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers gullibility value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_gull_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_gull_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_gull_density| alt text| width = 500px}}||')




                df_tmp = pd.DataFrame({'val' : w_cyn_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='k')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 10])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers cynicallity value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_cyn_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_cyn_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_cyn_density| alt text| width = 500px}}||\n\n')



        ########################

                tweet_l_sort = sorted(w_pt_avg_dict, key=w_pt_avg_dict.get, reverse=False)
                pt_l = []
                for t_id in tweet_l_sort:
                    pt_l.append(w_pt_avg_dict[t_id])

                mplpl.scatter(range(len(pt_l)), pt_l,  s=40,color='c',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ranked readers according PTL', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_pt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_pt_pt| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_acc_avg_dict, key=w_acc_avg_dict.get, reverse=False)
                acc_l = []
                for t_id in tweet_l_sort:
                    acc_l.append(w_acc_avg_dict[t_id])


                mplpl.scatter(range(len(acc_l)), acc_l ,  s=40,color='g',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Accuracy', fontsize=18)
                mplpl.xlabel('Ranked readers according accuracy', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(acc_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_acc_acc'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_acc_acc| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
                err_l = []
                for t_id in tweet_l_sort:
                    err_l.append(w_err_avg_dict[t_id])

                mplpl.scatter(range(len(err_l)), err_l,  s=40,color='y',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias (PB)', fontsize=18)
                mplpl.xlabel('Ranked readers according PB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(err_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_err_err'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_err_err| alt text| width = 500px}}')



                tweet_l_sort = sorted(w_abs_err_avg_dict, key=w_abs_err_avg_dict.get, reverse=False)
                abs_err_l = []
                for t_id in tweet_l_sort:
                    abs_err_l.append(w_abs_err_avg_dict[t_id])

                mplpl.scatter(range(len(abs_err_l)), abs_err_l,  s=40,color='y',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according APB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_err_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_abs-err_abs-err'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_abs-err_abs-err| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_gull_avg_dict, key=w_gull_avg_dict.get, reverse=False)
                gull_l = []
                for t_id in tweet_l_sort:
                    gull_l.append(w_gull_avg_dict[t_id])

                mplpl.scatter(range(len(gull_l)), gull_l,  s=40,color='m',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Gullibility', fontsize=18)
                mplpl.xlabel('Ranked readers according gullibility', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(gull_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_gull_gull'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_gull_gull| alt text| width = 500px}}')



                tweet_l_sort = sorted(w_cyn_avg_dict, key=w_cyn_avg_dict.get, reverse=False)
                cyn_l = []
                for t_id in tweet_l_sort:
                    cyn_l.append(w_cyn_avg_dict[t_id])

                mplpl.scatter(range(len(cyn_l)), cyn_l,  s=40,color='k',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Cynicality', fontsize=18)
                mplpl.xlabel('Ranked readers according cynicality', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(cyn_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_cyn_cyn'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_cyn_cyn| alt text| width = 500px}}||\n\n')



                # outF.write('|| Table ||\n\n')

                # mplpl.show()
                # exit()
        #####################################################33


                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_pt_cdf| alt text| width = 500px}}')


                num_bins = len(acc_l)
                counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Accuracy', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_acc_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_acc_cdf| alt text| width = 500px}}')


                num_bins = len(err_l)
                counts, bin_edges = np.histogram(err_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception bias (PB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_err_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_err_cdf| alt text| width = 500px}}')

                num_bins = len(abs_err_l)
                counts, bin_edges = np.histogram(abs_err_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_abs-err_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_abs-err_cdf| alt text| width = 500px}}')

                # outF.write('|| Table ||\n\n')

                num_bins = len(gull_l)
                counts, bin_edges = np.histogram(gull_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='m', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Gullibility', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_gull_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_gull_cdf| alt text| width = 500px}}')

                num_bins = len(cyn_l)
                counts, bin_edges = np.histogram(cyn_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='k', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Cynicality', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_cyn_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_cyn_cdf| alt text| width = 500px}}||\n')


                # mplpl.show()
                # exit()
                # col_l = ['r', 'b', 'g']
                #
                # i = 0
                # for data_s in gt_l_dict.keys():
                #
                #     mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=400, color=col_l[i], marker='*')
                #     mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([-2, 2])
                #     mplpl.ylabel('Perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i+=1
                #
                # mplpl.legend(loc="upper right")
                #
                # mplpl.grid()
                # # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception_scatter'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                # i = 0
                # mark_l = ['*', 'o', '^']
                # for data_s in gt_l_dict.keys():
                #
                #     # mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([-2, 2])
                #     mplpl.ylabel('Perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i+=1
                #
                # mplpl.legend(loc="upper right")
                #
                # mplpl.grid()
                # # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                # i=0
                # for data_s in gt_l_dict.keys():
                #
                #     mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o')
                #     mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=300, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([0, 2])
                #     mplpl.ylabel('Absolute perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i += 1
                #
                # mplpl.grid()
                # mplpl.legend(loc="upper right")
                #
                # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception_scatter'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                # i=0
                # for data_s in gt_l_dict.keys():
                #
                #     # mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([0, 2])
                #     mplpl.ylabel('Absolute perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i += 1
                #
                # mplpl.grid()
                # mplpl.legend(loc="upper right")
                #
                # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                #





            elif fig_f_together==True:

                out_dict = w_pt_var_dict
                ####ptl_cdf
                mplpl.rcParams['figure.figsize'] = 4.5, 2.5
                mplpl.rc('xtick', labelsize='large')
                mplpl.rc('ytick', labelsize='large')
                mplpl.rc('legend', fontsize='small')
                w_err_avg_dict
                # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
                tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
                # tweet_l_sort = [x for x in tweet_l_sort if x >= 0 or x < 0]
                acc_l = []
                for t_id in tweet_l_sort:
                    if out_dict[t_id] >=0 or out_dict[t_id]<0:
                        acc_l.append(out_dict[t_id])

                num_bins = len(acc_l)
                counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)

        legend_properties = {'weight': 'bold'}

        #

        mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        mplpl.xlabel('Disputability', fontsize=20, fontweight = 'bold')
        mplpl.legend(loc="lower right", prop=legend_properties, fontsize='small', ncol=1)
        # mplpl.title(data_name)
        # mplpl.legend(loc="upper left",fontsize = 'large')
        mplpl.xlim([0, 1])
        mplpl.ylim([0, 1])
        mplpl.grid()
        mplpl.subplots_adjust(bottom=0.24)
        mplpl.subplots_adjust(left=0.18)
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/Disputability_cdf_alldataset_new'
        mplpl.savefig(pp + '.pdf', format='pdf')
        mplpl.savefig(pp + '.png', format='png')



        # elif fig_f_1==True:
            #     balance_f = 'un_balanced'
            #     # balance_f = 'balanced'
            #
            #     # fig_f = True
            #     # fig_f = False
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #
            #     fig_p = 7
            #
            #     for ind in ind_l:
            #
            #         df_m = df[ind].copy()
            #
            #         groupby_ftr = 'worker_id'
            #         grouped = df_m.groupby(groupby_ftr, sort=False)
            #         grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #         # df_tmp = df_m[df_m['tweet_id'] == t_id]
            #         w_cc=0
            #         for w_id in grouped.groups.keys():
            #             print(w_cc)
            #             w_cc+=1
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             df_tmp = df_m[df_m['worker_id'] == w_id]
            #             ind_t = df_tmp.index.tolist()[0]
            #             weights = []
            #             w_acc_list_tmp = list(df_tmp['acc'])
            #             w_acc_list = []
            #             for el in w_acc_list_tmp:
            #                 if el == 0:
            #                     w_acc_list.append(-1)
            #                 elif el == 1:
            #                     w_acc_list.append(1)
            #                 else:
            #                     w_acc_list.append(0)
            #             df_tt = pd.DataFrame({'val' : w_acc_list})
            #
            #             weights.append(np.ones_like(list(df_tt['val'])) / float(len(list(df_tt['val']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(w_acc_list), np.median(w_acc_list), np.var(w_acc_list)]
            #             # tweet_avg[t_id] = np.mean(w_acc_list)
            #             # tweet_med[t_id] = np.median(w_acc_list)
            #             # tweet_var[t_id] = np.var(w_acc_list)
            #             #
            #             # tweet_avg_l.append(np.mean(w_acc_list))
            #             # tweet_med_l.append(np.median(w_acc_list))
            #             # tweet_var_l.append(np.var(w_acc_list))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==1:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tt['val'].plot(kind='kde', lw=4, color='g', label='Accuracy')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tt['val']), weights=weights, color='g')
            #
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Accuracy')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(w_acc_list), 3))+', Var : '
            #                             + str(np.round(np.var(w_acc_list), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_acc_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_acc_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #
            #             weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(list(df_tmp['rel_v']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['rel_v'])), np.median(list(df_tmp['rel_v'])), np.var(list(df_tmp['rel_v']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['rel_v']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['rel_v']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['rel_v']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['rel_v'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['rel_v'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['rel_v'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==6:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['rel_v'].plot(kind='kde', lw=4, color='c', label='Perceive truth value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['rel_v']), weights=weights, color='c')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perceive truth value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['rel_v'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['rel_v'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pt_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pt_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['err'])) / float(len(list(df_tmp['err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['err'])), np.median(list(df_tmp['err'])),
            #             #                            np.var(list(df_tmp['err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==2:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['err'].plot(kind='kde', lw=4, color='y', label='Perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['abs_err'])) / float(len(list(df_tmp['abs_err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['abs_err'])), np.median(list(df_tmp['abs_err'])),
            #             #                            np.var(list(df_tmp['abs_err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['abs_err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['abs_err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['abs_err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['abs_err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['abs_err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['abs_err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==3:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['abs_err'].plot(kind='kde', lw=4, color='y', label='Absolute perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['abs_err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Absolute perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['abs_err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['abs_err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_apb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_apb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['gull'])) / float(len(list(df_tmp['gull']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['gull'])), np.median(list(df_tmp['gull'])),
            #             #                            np.var(list(df_tmp['gull']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['gull']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['gull']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['gull']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['gull'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['gull'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['gull'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==4:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['gull'].plot(kind='kde', lw=4, color='k', label='Gullibility value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['gull']), weights=weights, color='k')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Gullibility value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['gull'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['gull'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_gull_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_gull_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['cyn'])) / float(len(list(df_tmp['cyn']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['cyn'])), np.median(list(df_tmp['cyn'])),
            #             #                            np.var(list(df_tmp['cyn']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['cyn']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['cyn']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['cyn']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['cyn'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['cyn'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['cyn'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==5:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['cyn'].plot(kind='kde', lw=4, color='m', label='Cynicality value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['cyn']), weights=weights, color='m')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Cynicality value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['cyn'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['cyn'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_cyn_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_cyn_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #     exit()
            # else:
            #
            #     AVG_list = []
            #     print(np.mean(all_acc))
            #     outF = open(remotedir + 'output.txt', 'w')
            #
            #     tweet_all_var = {}
            #     tweet_all_dev_avg = {}
            #     tweet_all_avg = {}
            #     tweet_all_gt_var = {}
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #     tweet_all_avg_l = []
            #     tweet_all_med_l = []
            #     tweet_all_var_l = []
            #     tweet_all_gt_var_l = []
            #     diff_group_disp_l = []
            #     dem_disp_l = []
            #     rep_disp_l = []
            #
            #     tweet_all_dev_avg = {}
            #     tweet_all_dev_med = {}
            #     tweet_all_dev_var = {}
            #
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #
            #     tweet_all_abs_dev_avg = {}
            #     tweet_all_abs_dev_med = {}
            #     tweet_all_abs_dev_var = {}
            #
            #     tweet_all_abs_dev_avg_l = []
            #     tweet_all_abs_dev_med_l = []
            #     tweet_all_abs_dev_var_l = []
            #     tweet_all_dev_avg_rnd = {}
            #     tweet_all_abs_dev_avg_rnd = {}
            #
            #     diff_group_disp_dict = {}
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #         news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #         news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #         news_cat_list = ['rumor', 'non-rumor']
            #         ind_l = [1]
            #
            #     for cat_l in news_cat_list:
            #         outF.write('== ' + str(cat_l) + ' ==\n\n')
            #         print('== ' + str(cat_l) + ' ==')
            #         tweet_dev_avg = {}
            #         tweet_dev_med = {}
            #         tweet_dev_var = {}
            #         tweet_abs_dev_avg = {}
            #         tweet_abs_dev_med = {}
            #         tweet_abs_dev_var = {}
            #
            #         tweet_avg = {}
            #         tweet_med = {}
            #         tweet_var = {}
            #         tweet_gt_var = {}
            #
            #         tweet_dev_avg_rnd = {}
            #         tweet_abs_dev_avg_rnd = {}
            #
            #
            #         tweet_dev_avg_l = []
            #         tweet_dev_med_l = []
            #         tweet_dev_var_l = []
            #         tweet_abs_dev_avg_l = []
            #         tweet_abs_dev_med_l = []
            #         tweet_abs_dev_var_l = []
            #
            #         tweet_avg_l = []
            #         tweet_med_l = []
            #         tweet_var_l = []
            #         tweet_gt_var_l = []
            #         AVG_susc_list = []
            #         AVG_wl_list = []
            #         all_acc = []
            #         AVG_dev_list = []
            #         # for lean in [-1, 0, 1]:
            #
            #             # AVG_susc_list = []
            #             # AVG_wl_list = []
            #             # all_acc = []
            #             # df_m = df_m[df_m['leaning'] == lean]
            #             # if lean == 0:
            #             #     col = 'g'
            #             #     lean_cat = 'neutral'
            #             # elif lean == 1:
            #             #     col = 'b'
            #             #     lean_cat = 'democrat'
            #             # elif lean == -1:
            #             #     col = 'r'
            #             #     lean_cat = 'republican'
            #             # print(lean_cat)
            #         for ind in ind_l:
            #
            #             if balance_f == 'balanced':
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
            #             else:
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'
            #
            #             inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
            #             df[ind] = pd.read_csv(inp1, sep="\t")
            #             df_w[ind] = pd.read_csv(inp1_w, sep="\t")
            #
            #             df_m = df[ind].copy()
            #             df_mm = df_m.copy()
            #
            #             df_m = df_m[df_m['ra_gt'] == cat_l]
            #             # df_mm = df_m[df_m['ra_gt']==cat_l]
            #             # df_m = df_m[df_m['leaning'] == lean]
            #
            #             groupby_ftr = 'tweet_id'
            #             grouped = df_m.groupby(groupby_ftr, sort=False)
            #             grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #             for t_id in grouped.groups.keys():
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #
            #                 df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
            #                 df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
            #                 df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #
            #                 weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
            #                 val_list = list(df_tmp['rel_v'])
            #                 tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_avg[t_id] = np.mean(val_list)
            #                 tweet_med[t_id] = np.median(val_list)
            #                 tweet_var[t_id] = np.var(val_list)
            #                 tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_avg_l.append(np.mean(val_list))
            #                 tweet_med_l.append(np.median(val_list))
            #                 tweet_var_l.append(np.var(val_list))
            #                 tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #
            #                 tweet_all_avg[t_id] = np.mean(val_list)
            #                 tweet_all_var[t_id] = np.var(val_list)
            #                 tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_all_avg_l.append(np.mean(val_list))
            #                 tweet_all_med_l.append(np.median(val_list))
            #                 tweet_all_var_l.append(np.var(val_list))
            #                 tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #                 val_list = list(df_tmp['err'])
            #                 abs_var_err = [np.abs(x) for x in val_list]
            #                 tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_dev_med[t_id] = np.median(val_list)
            #                 tweet_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_dev_avg_l.append(np.mean(val_list))
            #                 tweet_dev_med_l.append(np.median(val_list))
            #                 tweet_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #                 tweet_all_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_all_dev_med[t_id] = np.median(val_list)
            #                 tweet_all_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_all_dev_avg_l.append(np.mean(val_list))
            #                 tweet_all_dev_med_l.append(np.median(val_list))
            #                 tweet_all_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_all_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #
            #                 sum_rnd_abs_perc = 0
            #                 sum_rnd_perc = 0
            #                 for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
            #                     sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
            #                     sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
            #                 random_perc = np.abs(sum_rnd_perc / float(7))
            #                 random_abs_perc = sum_rnd_abs_perc / float(7)
            #
            #                 tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #                 tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #         gt_l = []
            #         pt_l = []
            #         disputability_l = []
            #         perc_l = []
            #         abs_perc_l = []
            #         # for t_id in tweet_l_sort:
            #         #     gt_l.append(tweet_gt_var[t_id])
            #         #     pt_l.append(tweet_avg[t_id])
            #         #     disputability_l.append(tweet_var[t_id])
            #         #     perc_l.append(tweet_dev_avg[t_id])
            #         #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #         tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #
            #
            #         if dataset == 'snopes':
            #             data_addr = 'snopes'
            #         elif dataset == 'politifact':
            #             data_addr = 'politifact/fig'
            #         elif dataset == 'mia':
            #             data_addr = 'mia/fig'
            #
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #
            #         for t_id in tweet_l_sort:
            #             count+=1
            #             if balance_f=='balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
            #                            + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
            #                            # str(np.round(diff_group_disp_dict[t_id], 3)) +
            #                            '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
            #                             + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
            #                             '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #
            #
            #
            #
            #     if dataset == 'snopes':
            #         data_addr = 'snopes'
            #     elif dataset == 'politifact':
            #         data_addr = 'politifact/fig'
            #     elif dataset == 'mia':
            #         data_addr = 'mia/fig'
            #
            #     # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
            #     tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #
            #     # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #
            #     tweet_napb_dict_high_disp = {}
            #     tweet_napb_dict_low_disp = {}
            #     for t_id in tweet_l_sort[:20]:
            #         # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     for t_id in tweet_l_sort[-20:]:
            #         # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     kk = 0
            #
            #     for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
            #         if kk==0:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
            #         else:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)
            #
            #         kk+=1
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         for t_id in tweet_l_sort:
            #             count += 1
            #             # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
            #             if balance_f == 'balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
            #                            str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #                 #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
            #                 #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
            #                            str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #             # +
            #             # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
            #             # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_time_cdf_toghether":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                #     tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['mia']:#,'snopes_nonpol','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                tweet_id = 100010
                publisher_name = 110
                tweet_popularity = {}
                tweet_text_dic = {}
                for input_file in [input_rumor, input_non_rumor]:
                    for line in input_file:
                        line.replace('\n', '')
                        line_splt = line.split('\t')
                        tweet_txt = line_splt[1]
                        tweet_link = line_splt[1]
                        tweet_id += 1
                        publisher_name += 1
                        tweet_popularity[tweet_id] = int(line_splt[2])
                        tweet_text_dic[tweet_id] = tweet_txt



                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                    # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    #     tweet_lable_dict[tweet_id] = 'undecided'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                col = 'purple'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'snopes_nonpol':
                claims_list = []
                col = 'r'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'politifact':
                col = 'g'

                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                col = 'purple'

                data_name = 'Snopes'
            if dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                col = 'green'

                data_name = 'Snopes\nnonpolitic'
            elif dataset=='politifact':
                col = 'c'
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                col = 'orange'
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            cat_time_dict = collections.defaultdict(dict)
            time_dict = collections.defaultdict()
            cat_time_list = collections.defaultdict(list)
            cat_time_dict_var = collections.defaultdict(dict)
            time_dict_var = collections.defaultdict()
            cat_time_list_var = collections.defaultdict(list)



            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()
                df[ind].loc[:, 'abs_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_abs_err'] = df[ind]['tweet_id'] * 0.0

                groupby_ftr = 'tweet_id'
                grouped = df[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df[ind].groupby(groupby_ftr, sort=False).sum()


                for ind_t in df[ind].index.tolist():
                    t_id = df[ind]['tweet_id'][ind_t]
                    err = df[ind]['err'][ind_t]
                    abs_err = np.abs(err)
                    df[ind]['abs_err'][ind_t] = abs_err
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= (val - df[ind]['rel_gt_v'][ind_t])
                        sum_rnd_abs_perc += np.abs(val - df[ind]['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)


                    norm_err = err / float(random_perc)
                    norm_abs_err = abs_err / float(random_abs_perc)
                    df[ind]['norm_err'][ind_t] = norm_err
                    df[ind]['norm_abs_err'][ind_t] = norm_abs_err

                # df[ind] = df[ind].copy()

            w_pt_avg_l = []
            w_err_avg_l = []
            w_abs_err_avg_l = []
            w_norm_err_avg_l = []
            w_norm_abs_err_avg_l = []
            w_acc_avg_l = []

            w_pt_std_l = []
            w_err_std_l = []
            w_abs_err_std_l = []
            w_norm_err_std_l = []
            w_norm_abs_err_std_l = []
            w_acc_std_l = []

            w_pt_avg_dict = collections.defaultdict()
            w_err_avg_dict = collections.defaultdict()
            w_abs_err_avg_dict = collections.defaultdict()
            w_norm_err_avg_dict = collections.defaultdict()
            w_norm_abs_err_avg_dict = collections.defaultdict()
            w_acc_avg_dict = collections.defaultdict()

            w_pt_std_dict = collections.defaultdict()
            w_err_std_dict = collections.defaultdict()
            w_abs_err_std_dict = collections.defaultdict()
            w_norm_err_std_dict = collections.defaultdict()
            w_norm_abs_err_std_dict = collections.defaultdict()
            w_acc_std_dict = collections.defaultdict()

            all_w_pt_list  = []
            all_w_err_list = []
            all_w_abs_err_list = []
            all_w_norm_err_list = []
            all_w_norm_abs_err_list  = []
            all_w_acc_list = []

            all_w_cyn_list = []
            all_w_gull_list = []
            w_cyn_avg_l = []
            w_gull_avg_l = []
            w_cyn_std_l= []
            w_gull_std_l = []
            w_cyn_avg_dict =collections.defaultdict()
            w_gull_avg_dict =collections.defaultdict()
            w_cyn_std_dict =collections.defaultdict()
            w_gull_std_dict = collections.defaultdict()
            w_pt_var_dict = collections.defaultdict()
            for ind in ind_l:

                df_m = df[ind].copy()
                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]


                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    # w_abs_err_list = list(df_tmp['abs_err'])
                    w_abs_err_list = list(df_tmp['susc'])
                    w_norm_err_list = list(df_tmp['norm_err'])
                    w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    df_cyn = df_tmp[df_tmp['cyn']>0]
                    df_gull = df_tmp[df_tmp['gull']>0]

                    w_cyn_list = list(df_cyn['cyn'])
                    w_gull_list = list(df_gull['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])
                    w_acc_list = []
                    # w_ind_acc_list
                    acc_c = 0
                    nacc_c = 0

                    cat_time_dict[tweet_lable_dict[t_id]][t_id] = np.mean(df_tmp['delta_time'])
                    cat_time_list[tweet_lable_dict[t_id]].append(np.mean(df_tmp['delta_time']))
                    time_dict[t_id] = np.mean(df_tmp['delta_time'])

                    cat_time_dict_var[tweet_lable_dict[t_id]][t_id] = np.std(df_tmp['delta_time'])
                    cat_time_list_var[tweet_lable_dict[t_id]].append(np.std(df_tmp['delta_time']))
                    time_dict_var[t_id] = np.std(df_tmp['delta_time'])

                    # w_acc_avg_l.append(w_ind_acc_list)

                    w_pt_std_l.append(np.std(w_pt_list))
                    w_err_std_l.append(np.std(w_err_list))
                    w_abs_err_std_l.append(np.std(w_abs_err_list))
                    w_norm_err_std_l.append(np.std(w_norm_err_list))
                    w_norm_abs_err_std_l.append(np.std(w_norm_abs_err_list))
                    w_cyn_std_l.append(np.std(w_cyn_list))
                    w_gull_std_l.append(np.std(w_gull_list))
                    # w_acc_std_l.append(np.std(w_ind_acc_list))


                    w_pt_avg_dict[t_id] = np.mean(w_pt_list)
                    w_err_avg_dict[t_id] = np.mean(w_err_list)
                    w_abs_err_avg_dict[t_id] = np.mean(w_abs_err_list)
                    w_norm_err_avg_dict[t_id] = np.mean(w_norm_err_list)
                    w_norm_abs_err_avg_dict[t_id] = np.mean(w_norm_abs_err_list)
                    w_cyn_avg_dict[t_id] = np.mean(w_cyn_list)
                    w_gull_avg_dict[t_id] = np.mean(w_gull_list)
                    # w_acc_avg_dict[t_id] = w_ind_acc_list


                    w_pt_var_dict[t_id] = np.var(w_pt_list)
                    w_pt_var_dict[t_id] = np.std(w_pt_list)

                # ind_
            ##################################################
            #
            # tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            # gt_l = []
            # pt_l = []
            # disputability_l = []
            # perc_l = []
            # abs_perc_l=[]
            # abs_perc_rnd_l = []
            # perc_rnd_l = []
            # tweet_skew_ll = []
            # for t_id in tweet_l_sort:
            #     gt_l.append(tweet_gt_var[t_id])
            #     pt_l.append(tweet_avg[t_id])
            #     disputability_l.append(tweet_var[t_id])
            #     perc_l.append(tweet_dev_avg[t_id])
            #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #     perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
            #     abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
            #     tweet_skew_ll.append(tweet_skew[t_id])
            # value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l,tweet_skew_ll]
            # value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
            #               'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'skewness']
            #
            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(8):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(8):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')
            #
            #
            # exit()

            # fig_f = True
            fig_f = False
            # fig_f_1 = True
            fig_f_1 = False
            fig_f_together = True
            if fig_f==True:

                df_tmp = pd.DataFrame({'val' : all_w_acc_list})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), weights=weights, color='g')
                # mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 1.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Readers accuracy', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_acc_list),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_acc_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_acc_density| alt text| width = 500px}}')



                # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
                df_tmp = pd.DataFrame({'val' : w_acc_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                # mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers accuracy', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_acc_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_acc_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_acc_pt| alt text| width = 500px}}')




                df_tmp = pd.DataFrame({'val' : all_w_pt_list})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), weights=weights, color='c')
                # mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 1.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Readers percevied truth value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_pt_list),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_pt_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_pt_density| alt text| width = 500px}}')




                # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
                df_tmp = pd.DataFrame({'val' : w_pt_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                mplpl.hist(list(df_tmp['val']), normed=1, color='c')


                # mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 3.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers percevied truth value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_pt_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_pt_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_pt_density| alt text| width = 500px}}')




                df_tmp = pd.DataFrame({'val' : w_err_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='y')


                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 3.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers perception bias value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_err_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_pb_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_pb_density| alt text| width = 500px}}')



                df_tmp = pd.DataFrame({'val' : w_abs_err_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='y')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers absolute perception bias value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_abs_err_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_apb_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_apb_density| alt text| width = 500px}}||')



                df_tmp = pd.DataFrame({'val' : w_gull_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='m')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 10])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers gullibility value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_gull_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_gull_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_gull_density| alt text| width = 500px}}||')




                df_tmp = pd.DataFrame({'val' : w_cyn_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='k')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 10])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers cynicallity value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_cyn_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_cyn_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_cyn_density| alt text| width = 500px}}||\n\n')



        ########################

                tweet_l_sort = sorted(w_pt_avg_dict, key=w_pt_avg_dict.get, reverse=False)
                pt_l = []
                for t_id in tweet_l_sort:
                    pt_l.append(w_pt_avg_dict[t_id])

                mplpl.scatter(range(len(pt_l)), pt_l,  s=40,color='c',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ranked readers according PTL', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_pt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_pt_pt| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_acc_avg_dict, key=w_acc_avg_dict.get, reverse=False)
                acc_l = []
                for t_id in tweet_l_sort:
                    acc_l.append(w_acc_avg_dict[t_id])


                mplpl.scatter(range(len(acc_l)), acc_l ,  s=40,color='g',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Accuracy', fontsize=18)
                mplpl.xlabel('Ranked readers according accuracy', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(acc_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_acc_acc'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_acc_acc| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
                err_l = []
                for t_id in tweet_l_sort:
                    err_l.append(w_err_avg_dict[t_id])

                mplpl.scatter(range(len(err_l)), err_l,  s=40,color='y',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias (PB)', fontsize=18)
                mplpl.xlabel('Ranked readers according PB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(err_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_err_err'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_err_err| alt text| width = 500px}}')



                tweet_l_sort = sorted(w_abs_err_avg_dict, key=w_abs_err_avg_dict.get, reverse=False)
                abs_err_l = []
                for t_id in tweet_l_sort:
                    abs_err_l.append(w_abs_err_avg_dict[t_id])

                mplpl.scatter(range(len(abs_err_l)), abs_err_l,  s=40,color='y',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according APB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_err_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_abs-err_abs-err'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_abs-err_abs-err| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_gull_avg_dict, key=w_gull_avg_dict.get, reverse=False)
                gull_l = []
                for t_id in tweet_l_sort:
                    gull_l.append(w_gull_avg_dict[t_id])

                mplpl.scatter(range(len(gull_l)), gull_l,  s=40,color='m',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Gullibility', fontsize=18)
                mplpl.xlabel('Ranked readers according gullibility', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(gull_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_gull_gull'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_gull_gull| alt text| width = 500px}}')



                tweet_l_sort = sorted(w_cyn_avg_dict, key=w_cyn_avg_dict.get, reverse=False)
                cyn_l = []
                for t_id in tweet_l_sort:
                    cyn_l.append(w_cyn_avg_dict[t_id])

                mplpl.scatter(range(len(cyn_l)), cyn_l,  s=40,color='k',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Cynicality', fontsize=18)
                mplpl.xlabel('Ranked readers according cynicality', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(cyn_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_cyn_cyn'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_cyn_cyn| alt text| width = 500px}}||\n\n')



                # outF.write('|| Table ||\n\n')

                # mplpl.show()
                # exit()
        #####################################################33


                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_pt_cdf| alt text| width = 500px}}')


                num_bins = len(acc_l)
                counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Accuracy', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_acc_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_acc_cdf| alt text| width = 500px}}')


                num_bins = len(err_l)
                counts, bin_edges = np.histogram(err_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception bias (PB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_err_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_err_cdf| alt text| width = 500px}}')

                num_bins = len(abs_err_l)
                counts, bin_edges = np.histogram(abs_err_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_abs-err_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_abs-err_cdf| alt text| width = 500px}}')

                # outF.write('|| Table ||\n\n')

                num_bins = len(gull_l)
                counts, bin_edges = np.histogram(gull_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='m', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Gullibility', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_gull_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_gull_cdf| alt text| width = 500px}}')

                num_bins = len(cyn_l)
                counts, bin_edges = np.histogram(cyn_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='k', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Cynicality', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_cyn_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_cyn_cdf| alt text| width = 500px}}||\n')


                # mplpl.show()
                # exit()
                # col_l = ['r', 'b', 'g']
                #
                # i = 0
                # for data_s in gt_l_dict.keys():
                #
                #     mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=400, color=col_l[i], marker='*')
                #     mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([-2, 2])
                #     mplpl.ylabel('Perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i+=1
                #
                # mplpl.legend(loc="upper right")
                #
                # mplpl.grid()
                # # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception_scatter'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                # i = 0
                # mark_l = ['*', 'o', '^']
                # for data_s in gt_l_dict.keys():
                #
                #     # mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([-2, 2])
                #     mplpl.ylabel('Perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i+=1
                #
                # mplpl.legend(loc="upper right")
                #
                # mplpl.grid()
                # # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                # i=0
                # for data_s in gt_l_dict.keys():
                #
                #     mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o')
                #     mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=300, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([0, 2])
                #     mplpl.ylabel('Absolute perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i += 1
                #
                # mplpl.grid()
                # mplpl.legend(loc="upper right")
                #
                # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception_scatter'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                # i=0
                # for data_s in gt_l_dict.keys():
                #
                #     # mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([0, 2])
                #     mplpl.ylabel('Absolute perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i += 1
                #
                # mplpl.grid()
                # mplpl.legend(loc="upper right")
                #
                # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                #





            elif fig_f_together==True:

                out_dict = time_dict
                ####ptl_cdf
                mplpl.rcParams['figure.figsize'] = 4.5, 2.5
                mplpl.rc('xtick', labelsize='large')
                mplpl.rc('ytick', labelsize='large')
                mplpl.rc('legend', fontsize='small')
                w_err_avg_l = []
                # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
                tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
                # tweet_l_sort = [x for x in tweet_l_sort if x >= 0 or x < 0]
                acc_l = []
                for t_id in tweet_l_sort:
                    if out_dict[t_id] >=0 or out_dict[t_id]<0:
                        acc_l.append(out_dict[t_id])
                        w_err_avg_l.append(w_abs_err_avg_dict[t_id])
                num_bins = len(acc_l)
                counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)
                mplpl.scatter(acc_l, w_err_avg_l, color=col, lw=5, label=data_name)

        legend_properties = {'weight': 'bold'}

        #

        # mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        mplpl.ylabel('Total Perception Bias', fontsize=12, fontweight = 'bold')
        mplpl.xlabel('Time', fontsize=18, fontweight = 'bold')
        mplpl.legend(loc="lower right", prop=legend_properties, fontsize='small', ncol=1)
        # mplpl.title(data_name)
        # mplpl.legend(loc="upper left",fontsize = 'large')
        # mplpl.xlim([0, 1])
        # mplpl.ylim([0, 1])
        mplpl.grid()
        mplpl.subplots_adjust(bottom=0.24)
        mplpl.subplots_adjust(left=0.18)
        print('time-cdf-figure')
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/time_cdf_alldataset_new'
        pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/'+data_n+'time_tpb_alldataset_new'
        mplpl.savefig(pp + '.pdf', format='pdf')
        mplpl.savefig(pp + '.png', format='png')



        # elif fig_f_1==True:
            #     balance_f = 'un_balanced'
            #     # balance_f = 'balanced'
            #
            #     # fig_f = True
            #     # fig_f = False
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #
            #     fig_p = 7
            #
            #     for ind in ind_l:
            #
            #         df_m = df[ind].copy()
            #
            #         groupby_ftr = 'worker_id'
            #         grouped = df_m.groupby(groupby_ftr, sort=False)
            #         grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #         # df_tmp = df_m[df_m['tweet_id'] == t_id]
            #         w_cc=0
            #         for w_id in grouped.groups.keys():
            #             print(w_cc)
            #             w_cc+=1
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             df_tmp = df_m[df_m['worker_id'] == w_id]
            #             ind_t = df_tmp.index.tolist()[0]
            #             weights = []
            #             w_acc_list_tmp = list(df_tmp['acc'])
            #             w_acc_list = []
            #             for el in w_acc_list_tmp:
            #                 if el == 0:
            #                     w_acc_list.append(-1)
            #                 elif el == 1:
            #                     w_acc_list.append(1)
            #                 else:
            #                     w_acc_list.append(0)
            #             df_tt = pd.DataFrame({'val' : w_acc_list})
            #
            #             weights.append(np.ones_like(list(df_tt['val'])) / float(len(list(df_tt['val']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(w_acc_list), np.median(w_acc_list), np.var(w_acc_list)]
            #             # tweet_avg[t_id] = np.mean(w_acc_list)
            #             # tweet_med[t_id] = np.median(w_acc_list)
            #             # tweet_var[t_id] = np.var(w_acc_list)
            #             #
            #             # tweet_avg_l.append(np.mean(w_acc_list))
            #             # tweet_med_l.append(np.median(w_acc_list))
            #             # tweet_var_l.append(np.var(w_acc_list))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==1:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tt['val'].plot(kind='kde', lw=4, color='g', label='Accuracy')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tt['val']), weights=weights, color='g')
            #
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Accuracy')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(w_acc_list), 3))+', Var : '
            #                             + str(np.round(np.var(w_acc_list), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_acc_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_acc_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #
            #             weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(list(df_tmp['rel_v']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['rel_v'])), np.median(list(df_tmp['rel_v'])), np.var(list(df_tmp['rel_v']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['rel_v']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['rel_v']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['rel_v']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['rel_v'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['rel_v'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['rel_v'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==6:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['rel_v'].plot(kind='kde', lw=4, color='c', label='Perceive truth value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['rel_v']), weights=weights, color='c')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perceive truth value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['rel_v'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['rel_v'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pt_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pt_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['err'])) / float(len(list(df_tmp['err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['err'])), np.median(list(df_tmp['err'])),
            #             #                            np.var(list(df_tmp['err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==2:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['err'].plot(kind='kde', lw=4, color='y', label='Perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['abs_err'])) / float(len(list(df_tmp['abs_err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['abs_err'])), np.median(list(df_tmp['abs_err'])),
            #             #                            np.var(list(df_tmp['abs_err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['abs_err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['abs_err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['abs_err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['abs_err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['abs_err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['abs_err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==3:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['abs_err'].plot(kind='kde', lw=4, color='y', label='Absolute perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['abs_err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Absolute perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['abs_err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['abs_err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_apb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_apb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['gull'])) / float(len(list(df_tmp['gull']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['gull'])), np.median(list(df_tmp['gull'])),
            #             #                            np.var(list(df_tmp['gull']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['gull']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['gull']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['gull']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['gull'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['gull'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['gull'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==4:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['gull'].plot(kind='kde', lw=4, color='k', label='Gullibility value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['gull']), weights=weights, color='k')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Gullibility value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['gull'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['gull'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_gull_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_gull_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['cyn'])) / float(len(list(df_tmp['cyn']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['cyn'])), np.median(list(df_tmp['cyn'])),
            #             #                            np.var(list(df_tmp['cyn']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['cyn']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['cyn']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['cyn']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['cyn'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['cyn'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['cyn'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==5:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['cyn'].plot(kind='kde', lw=4, color='m', label='Cynicality value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['cyn']), weights=weights, color='m')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Cynicality value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['cyn'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['cyn'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_cyn_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_cyn_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #     exit()
            # else:
            #
            #     AVG_list = []
            #     print(np.mean(all_acc))
            #     outF = open(remotedir + 'output.txt', 'w')
            #
            #     tweet_all_var = {}
            #     tweet_all_dev_avg = {}
            #     tweet_all_avg = {}
            #     tweet_all_gt_var = {}
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #     tweet_all_avg_l = []
            #     tweet_all_med_l = []
            #     tweet_all_var_l = []
            #     tweet_all_gt_var_l = []
            #     diff_group_disp_l = []
            #     dem_disp_l = []
            #     rep_disp_l = []
            #
            #     tweet_all_dev_avg = {}
            #     tweet_all_dev_med = {}
            #     tweet_all_dev_var = {}
            #
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #
            #     tweet_all_abs_dev_avg = {}
            #     tweet_all_abs_dev_med = {}
            #     tweet_all_abs_dev_var = {}
            #
            #     tweet_all_abs_dev_avg_l = []
            #     tweet_all_abs_dev_med_l = []
            #     tweet_all_abs_dev_var_l = []
            #     tweet_all_dev_avg_rnd = {}
            #     tweet_all_abs_dev_avg_rnd = {}
            #
            #     diff_group_disp_dict = {}
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #         news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #         news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #         news_cat_list = ['rumor', 'non-rumor']
            #         ind_l = [1]
            #
            #     for cat_l in news_cat_list:
            #         outF.write('== ' + str(cat_l) + ' ==\n\n')
            #         print('== ' + str(cat_l) + ' ==')
            #         tweet_dev_avg = {}
            #         tweet_dev_med = {}
            #         tweet_dev_var = {}
            #         tweet_abs_dev_avg = {}
            #         tweet_abs_dev_med = {}
            #         tweet_abs_dev_var = {}
            #
            #         tweet_avg = {}
            #         tweet_med = {}
            #         tweet_var = {}
            #         tweet_gt_var = {}
            #
            #         tweet_dev_avg_rnd = {}
            #         tweet_abs_dev_avg_rnd = {}
            #
            #
            #         tweet_dev_avg_l = []
            #         tweet_dev_med_l = []
            #         tweet_dev_var_l = []
            #         tweet_abs_dev_avg_l = []
            #         tweet_abs_dev_med_l = []
            #         tweet_abs_dev_var_l = []
            #
            #         tweet_avg_l = []
            #         tweet_med_l = []
            #         tweet_var_l = []
            #         tweet_gt_var_l = []
            #         AVG_susc_list = []
            #         AVG_wl_list = []
            #         all_acc = []
            #         AVG_dev_list = []
            #         # for lean in [-1, 0, 1]:
            #
            #             # AVG_susc_list = []
            #             # AVG_wl_list = []
            #             # all_acc = []
            #             # df_m = df_m[df_m['leaning'] == lean]
            #             # if lean == 0:
            #             #     col = 'g'
            #             #     lean_cat = 'neutral'
            #             # elif lean == 1:
            #             #     col = 'b'
            #             #     lean_cat = 'democrat'
            #             # elif lean == -1:
            #             #     col = 'r'
            #             #     lean_cat = 'republican'
            #             # print(lean_cat)
            #         for ind in ind_l:
            #
            #             if balance_f == 'balanced':
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
            #             else:
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'
            #
            #             inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
            #             df[ind] = pd.read_csv(inp1, sep="\t")
            #             df_w[ind] = pd.read_csv(inp1_w, sep="\t")
            #
            #             df_m = df[ind].copy()
            #             df_mm = df_m.copy()
            #
            #             df_m = df_m[df_m['ra_gt'] == cat_l]
            #             # df_mm = df_m[df_m['ra_gt']==cat_l]
            #             # df_m = df_m[df_m['leaning'] == lean]
            #
            #             groupby_ftr = 'tweet_id'
            #             grouped = df_m.groupby(groupby_ftr, sort=False)
            #             grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #             for t_id in grouped.groups.keys():
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #
            #                 df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
            #                 df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
            #                 df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #
            #                 weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
            #                 val_list = list(df_tmp['rel_v'])
            #                 tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_avg[t_id] = np.mean(val_list)
            #                 tweet_med[t_id] = np.median(val_list)
            #                 tweet_var[t_id] = np.var(val_list)
            #                 tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_avg_l.append(np.mean(val_list))
            #                 tweet_med_l.append(np.median(val_list))
            #                 tweet_var_l.append(np.var(val_list))
            #                 tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #
            #                 tweet_all_avg[t_id] = np.mean(val_list)
            #                 tweet_all_var[t_id] = np.var(val_list)
            #                 tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_all_avg_l.append(np.mean(val_list))
            #                 tweet_all_med_l.append(np.median(val_list))
            #                 tweet_all_var_l.append(np.var(val_list))
            #                 tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #                 val_list = list(df_tmp['err'])
            #                 abs_var_err = [np.abs(x) for x in val_list]
            #                 tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_dev_med[t_id] = np.median(val_list)
            #                 tweet_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_dev_avg_l.append(np.mean(val_list))
            #                 tweet_dev_med_l.append(np.median(val_list))
            #                 tweet_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #                 tweet_all_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_all_dev_med[t_id] = np.median(val_list)
            #                 tweet_all_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_all_dev_avg_l.append(np.mean(val_list))
            #                 tweet_all_dev_med_l.append(np.median(val_list))
            #                 tweet_all_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_all_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #
            #                 sum_rnd_abs_perc = 0
            #                 sum_rnd_perc = 0
            #                 for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
            #                     sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
            #                     sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
            #                 random_perc = np.abs(sum_rnd_perc / float(7))
            #                 random_abs_perc = sum_rnd_abs_perc / float(7)
            #
            #                 tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #                 tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #         gt_l = []
            #         pt_l = []
            #         disputability_l = []
            #         perc_l = []
            #         abs_perc_l = []
            #         # for t_id in tweet_l_sort:
            #         #     gt_l.append(tweet_gt_var[t_id])
            #         #     pt_l.append(tweet_avg[t_id])
            #         #     disputability_l.append(tweet_var[t_id])
            #         #     perc_l.append(tweet_dev_avg[t_id])
            #         #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #         tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #
            #
            #         if dataset == 'snopes':
            #             data_addr = 'snopes'
            #         elif dataset == 'politifact':
            #             data_addr = 'politifact/fig'
            #         elif dataset == 'mia':
            #             data_addr = 'mia/fig'
            #
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #
            #         for t_id in tweet_l_sort:
            #             count+=1
            #             if balance_f=='balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
            #                            + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
            #                            # str(np.round(diff_group_disp_dict[t_id], 3)) +
            #                            '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
            #                             + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
            #                             '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #
            #
            #
            #
            #     if dataset == 'snopes':
            #         data_addr = 'snopes'
            #     elif dataset == 'politifact':
            #         data_addr = 'politifact/fig'
            #     elif dataset == 'mia':
            #         data_addr = 'mia/fig'
            #
            #     # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
            #     tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #
            #     # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #
            #     tweet_napb_dict_high_disp = {}
            #     tweet_napb_dict_low_disp = {}
            #     for t_id in tweet_l_sort[:20]:
            #         # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     for t_id in tweet_l_sort[-20:]:
            #         # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     kk = 0
            #
            #     for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
            #         if kk==0:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
            #         else:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)
            #
            #         kk+=1
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         for t_id in tweet_l_sort:
            #             count += 1
            #             # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
            #             if balance_f == 'balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
            #                            str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #                 #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
            #                 #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
            #                            str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #             # +
            #             # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
            #             # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_ideological_mpb_cdf_toghether":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','snopes_nonpol','politifact','mia']:#['snopes']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                tweet_id = 100010
                publisher_name = 110
                tweet_popularity = {}
                tweet_text_dic = {}
                for input_file in [input_rumor, input_non_rumor]:
                    for line in input_file:
                        line.replace('\n', '')
                        line_splt = line.split('\t')
                        tweet_txt = line_splt[1]
                        tweet_link = line_splt[1]
                        tweet_id += 1
                        publisher_name += 1
                        tweet_popularity[tweet_id] = int(line_splt[2])
                        tweet_text_dic[tweet_id] = tweet_txt



                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                    # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    #     tweet_lable_dict[tweet_id] = 'undecided'
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable


            if dataset == 'snopes':
                claims_list = []
                col = 'r'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                col = 'g'

                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                col = 'purple'

                data_name = 'Snopes'
            elif dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                col = 'green'

                data_name = 'Snopes_nonpolt'
            elif dataset=='politifact':
                col = 'c'
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                col = 'orange'
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()
                df[ind].loc[:, 'abs_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_abs_err'] = df[ind]['tweet_id'] * 0.0

                groupby_ftr = 'tweet_id'
                grouped = df[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df[ind].groupby(groupby_ftr, sort=False).sum()


                for ind_t in df[ind].index.tolist():
                    t_id = df[ind]['tweet_id'][ind_t]
                    err = df[ind]['err'][ind_t]
                    abs_err = np.abs(err)
                    df[ind]['abs_err'][ind_t] = abs_err
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= (val - df[ind]['rel_gt_v'][ind_t])
                        sum_rnd_abs_perc += np.abs(val - df[ind]['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)


                    norm_err = err / float(random_perc)
                    norm_abs_err = abs_err / float(random_abs_perc)
                    df[ind]['norm_err'][ind_t] = norm_err
                    df[ind]['norm_abs_err'][ind_t] = norm_abs_err

                # df[ind] = df[ind].copy()

            w_pt_avg_l = []
            w_err_avg_l = []
            w_abs_err_avg_l = []
            w_norm_err_avg_l = []
            w_norm_abs_err_avg_l = []
            w_acc_avg_l = []

            w_pt_std_l = []
            w_err_std_l = []
            w_abs_err_std_l = []
            w_norm_err_std_l = []
            w_norm_abs_err_std_l = []
            w_acc_std_l = []

            w_pt_avg_dict = collections.defaultdict()
            w_err_avg_dict = collections.defaultdict()
            w_abs_err_avg_dict = collections.defaultdict()
            w_norm_err_avg_dict = collections.defaultdict()
            w_norm_abs_err_avg_dict = collections.defaultdict()
            w_acc_avg_dict = collections.defaultdict()

            w_pt_std_dict = collections.defaultdict()
            w_err_std_dict = collections.defaultdict()
            w_abs_err_std_dict = collections.defaultdict()
            w_norm_err_std_dict = collections.defaultdict()
            w_norm_abs_err_std_dict = collections.defaultdict()
            w_acc_std_dict = collections.defaultdict()

            all_w_pt_list  = []
            all_w_err_list = []
            all_w_abs_err_list = []
            all_w_norm_err_list = []
            all_w_norm_abs_err_list  = []
            all_w_acc_list = []

            all_w_cyn_list = []
            all_w_gull_list = []
            w_cyn_avg_l = []
            w_gull_avg_l = []
            w_cyn_std_l= []
            w_gull_std_l = []
            w_cyn_avg_dict =collections.defaultdict()
            w_gull_avg_dict =collections.defaultdict()
            w_cyn_std_dict =collections.defaultdict()
            w_gull_std_dict = collections.defaultdict()
            w_err_avg_dict_dem = collections.defaultdict()
            w_err_avg_dict_rep = collections.defaultdict()
            w_err_avg_dict_neut = collections.defaultdict()
            for ind in ind_l:

                df_m = df[ind].copy()
                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]

                    df_dem = df_tmp[df_tmp['leaning']==1]
                    df_rep = df_tmp[df_tmp['leaning']==-1]
                    df_neut = df_tmp[df_tmp['leaning']==0]


                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    w_abs_err_list = list(df_tmp['abs_err'])
                    w_norm_err_list = list(df_tmp['norm_err'])
                    w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    w_cyn_list = list(df_tmp['cyn'])
                    w_gull_list = list(df_tmp['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])
                    w_acc_list = []


                    w_err_list_dem = list(df_dem['err'])
                    w_err_list_rep = list(df_rep['err'])
                    w_err_list_neut = list(df_neut['err'])

                    # w_ind_acc_list
                    acc_c = 0
                    nacc_c = 0




                    all_w_pt_list += list(df_tmp['rel_v'])
                    all_w_err_list += list(df_tmp['err'])
                    all_w_abs_err_list += list(df_tmp['abs_err'])
                    all_w_norm_err_list += list(df_tmp['norm_err'])
                    all_w_norm_abs_err_list += list(df_tmp['norm_abs_err'])
                    all_w_cyn_list += list(df_tmp['cyn'])
                    all_w_gull_list += list(df_tmp['gull'])
                    all_w_acc_list += list(w_acc_list)


                    w_pt_avg_l.append(np.mean(w_pt_list))
                    w_err_avg_l.append(np.mean(w_err_list))
                    w_abs_err_avg_l.append(np.mean(w_abs_err_list))
                    w_norm_err_avg_l.append(np.mean(w_norm_err_list))
                    w_norm_abs_err_avg_l.append(np.mean(w_norm_abs_err_list))
                    w_cyn_avg_l.append(np.mean(w_cyn_list))
                    w_gull_avg_l.append(np.mean(w_gull_list))
                    # w_acc_avg_l.append(w_ind_acc_list)

                    w_pt_std_l.append(np.std(w_pt_list))
                    w_err_std_l.append(np.std(w_err_list))
                    w_abs_err_std_l.append(np.std(w_abs_err_list))
                    w_norm_err_std_l.append(np.std(w_norm_err_list))
                    w_norm_abs_err_std_l.append(np.std(w_norm_abs_err_list))
                    w_cyn_std_l.append(np.std(w_cyn_list))
                    w_gull_std_l.append(np.std(w_gull_list))
                    # w_acc_std_l.append(np.std(w_ind_acc_list))


                    w_pt_avg_dict[t_id] = np.mean(w_pt_list)
                    w_err_avg_dict[t_id] = np.mean(w_err_list)
                    w_abs_err_avg_dict[t_id] = np.mean(w_abs_err_list)
                    w_norm_err_avg_dict[t_id] = np.mean(w_norm_err_list)
                    w_norm_abs_err_avg_dict[t_id] = np.mean(w_norm_abs_err_list)
                    w_cyn_avg_dict[t_id] = np.mean(w_cyn_list)
                    w_gull_avg_dict[t_id] = np.mean(w_gull_list)
                    # w_acc_avg_dict[t_id] = w_ind_acc_list


                    w_err_avg_dict_dem[t_id] = np.mean(w_err_list_dem)
                    w_err_avg_dict_rep[t_id] = np.mean(w_err_list_rep)

                    w_err_avg_dict_neut[t_id] = np.mean(w_err_list_neut)


                    w_pt_std_dict[t_id] = np.std(w_pt_list)
                    w_err_std_dict[t_id] = np.std(w_err_list)
                    w_abs_err_std_dict[t_id] = np.std(w_abs_err_list)
                    w_norm_err_std_dict[t_id] = np.std(w_norm_err_list)
                    w_norm_abs_err_std_dict[t_id] = np.std(w_norm_abs_err_list)
                    w_cyn_std_dict[t_id] = np.std(w_cyn_list)
                    w_gull_std_dict[t_id] = np.std(w_gull_list)
                    w_acc_std_dict[t_id] = np.std(w_acc_list)
                # ind_
            ##################################################
            #
            # tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            # gt_l = []
            # pt_l = []
            # disputability_l = []
            # perc_l = []
            # abs_perc_l=[]
            # abs_perc_rnd_l = []
            # perc_rnd_l = []
            # tweet_skew_ll = []
            # for t_id in tweet_l_sort:
            #     gt_l.append(tweet_gt_var[t_id])
            #     pt_l.append(tweet_avg[t_id])
            #     disputability_l.append(tweet_var[t_id])
            #     perc_l.append(tweet_dev_avg[t_id])
            #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #     perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
            #     abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
            #     tweet_skew_ll.append(tweet_skew[t_id])
            # value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l,tweet_skew_ll]
            # value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
            #               'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'skewness']
            #
            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(8):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(8):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')
            #
            #
            # exit()

            # fig_f = True
            fig_f = False
            # fig_f_1 = True
            fig_f_1 = False
            fig_f_together = True


            if fig_f_together==True:

                ####dem
                mplpl.rcParams['figure.figsize'] = 4.5, 2.5
                mplpl.rc('xtick', labelsize='large')
                mplpl.rc('ytick', labelsize='large')
                mplpl.rc('legend', fontsize='small')
                # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
                tweet_l_sort = sorted(w_err_avg_dict_dem, key=w_err_avg_dict_dem.get, reverse=False)
                acc_l_rep = []
                acc_l_dem = []
                acc_l_neut = []
                acc_l_all = []
                acc_l_diff = []
                for t_id in tweet_l_sort:
                    acc_l_dem.append(w_err_avg_dict_dem[t_id])
                    acc_l_rep.append(w_err_avg_dict_rep[t_id])
                    acc_l_neut.append(w_err_avg_dict_neut[t_id])
                    acc_l_all.append(w_err_avg_dict[t_id])


                    acc_l_dem = sorted(acc_l_dem)
                    acc_l_rep = sorted(acc_l_rep)
                    acc_l_neut = sorted(acc_l_neut)
                    acc_l_all = sorted(acc_l_all)
                    acc_l_diff.append(w_err_avg_dict_dem[t_id] - w_err_avg_dict_rep[t_id])

                    acc_l_diff = sorted(acc_l_diff)


                # num_bins = len(acc_l_all)
                # counts, bin_edges = np.histogram(acc_l_all, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c='k', lw=4, label='All users')
                #
                # num_bins = len(acc_l_dem)
                # counts, bin_edges = np.histogram(acc_l_dem, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c='b', lw=4, label='Democrats')
                #
                #
                # num_bins = len(acc_l_neut)
                # counts, bin_edges = np.histogram(acc_l_neut, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c='g', lw=4, label='Neutrals')
                #
                #
                # num_bins = len(acc_l_rep)
                # counts, bin_edges = np.histogram(acc_l_rep, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c='red', lw=4, label='Republicans')





                #
                acc_l_diff = sorted(acc_l_diff)


                num_bins = len(acc_l_diff)
                counts, bin_edges = np.histogram(acc_l_diff, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=4, label=data_name)




                # mplpl.plot(range(len(tweet_l_sort)), acc_l_dem, c=col, lw=5, label=data_name + ' Democrats')
                # mplpl.plot(range(len(tweet_l_sort)), acc_l_rep, c=col, lw=5,linestyle='--', label=data_name + ' Republicans')

                # mplpl.plot(range(len(tweet_l_sort)), acc_l_dem, c=col, lw=5, label=data_name )#+ ' Democrats')
                # mplpl.plot(range(len(tweet_l_sort)), acc_l_rep, c='r', lw=5,linestyle='--', label=data_name + ' Republicans')
                #rep
                # tweet_l_sort = sorted(w_err_avg_dict_rep, key=w_err_avg_dict_rep.get, reverse=False)
                # acc_l = []
                # for t_id in tweet_l_sort:
                #     acc_l.append(w_err_avg_dict_rep[t_id])
                #
                # num_bins = len(acc_l)
                # counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, linestyle='--',label=data_name + ' Republicans')

        #
        legend_properties = {'weight': 'bold'}
        mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        mplpl.xlabel(r'$MPB_{Dem} - MPB_{Rep}$', fontsize=20, fontweight = 'bold')
        # mplpl.xlabel('Mean Perception Bias', fontsize=18, fontweight = 'bold')
        # mplpl.title(data_name,fontsize='xx-large')
        mplpl.legend(loc="lower right",prop=legend_properties,fontsize = 'small',ncol=1)
        mplpl.xlim([-1.5, 1.5])
        mplpl.xlim([-1, 1])
        # mplpl.ylim([0, 1])
        mplpl.grid()
        mplpl.subplots_adjust(bottom=0.24)
        mplpl.subplots_adjust(left=0.18)
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_MPB_alldataset.pdf'
        pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_MPB_all_diff_cdf'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_MPB_snopes_diff_cdf'
        mplpl.savefig(pp+'.pdf', format='pdf')
        mplpl.savefig(pp+'.png', format='png')












        #         mplpl.rcParams['figure.figsize'] = 5.4, 3.2
        #         mplpl.rc('xtick', labelsize='x-large')
        #         mplpl.rc('ytick', labelsize='x-large')
        #         mplpl.rc('legend', fontsize='medium')
        #         w_err_avg_dict
        #         # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
        #         tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
        #         acc_l = []
        #         for t_id in tweet_l_sort:
        #             acc_l.append(w_err_avg_dict[t_id])
        #
        #         num_bins = len(acc_l)
        #         counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         cdf = np.cumsum(counts)
        #         scale = 1.0 / cdf[-1]
        #         ncdf = scale * cdf
        #         mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)
        #
        # legend_properties = {'weight': 'bold'}
        #
        # mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        # mplpl.xlabel('Mean perception bias', fontsize=20, fontweight = 'bold')
        # mplpl.legend(loc="upper left", prop=legend_properties, fontsize='medium', ncol=1)
        # # mplpl.title(data_name)
        # # mplpl.legend(loc="upper left",fontsize = 'large')
        # mplpl.xlim([-1.5, 1.5])
        # mplpl.ylim([0, 1])
        # mplpl.grid()
        # mplpl.subplots_adjust(bottom=0.24)
        # mplpl.subplots_adjust(left=0.18)
        # # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/MPB_cdf_alldataset'
        # mplpl.savefig(pp + '.pdf', format='pdf')
        # mplpl.savefig(pp + '.png', format='png')
        #

        # elif fig_f_1==True:
            #     balance_f = 'un_balanced'
            #     # balance_f = 'balanced'
            #
            #     # fig_f = True
            #     # fig_f = False
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #
            #     fig_p = 7
            #
            #     for ind in ind_l:
            #
            #         df_m = df[ind].copy()
            #
            #         groupby_ftr = 'worker_id'
            #         grouped = df_m.groupby(groupby_ftr, sort=False)
            #         grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #         # df_tmp = df_m[df_m['tweet_id'] == t_id]
            #         w_cc=0
            #         for w_id in grouped.groups.keys():
            #             print(w_cc)
            #             w_cc+=1
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             df_tmp = df_m[df_m['worker_id'] == w_id]
            #             ind_t = df_tmp.index.tolist()[0]
            #             weights = []
            #             w_acc_list_tmp = list(df_tmp['acc'])
            #             w_acc_list = []
            #             for el in w_acc_list_tmp:
            #                 if el == 0:
            #                     w_acc_list.append(-1)
            #                 elif el == 1:
            #                     w_acc_list.append(1)
            #                 else:
            #                     w_acc_list.append(0)
            #             df_tt = pd.DataFrame({'val' : w_acc_list})
            #
            #             weights.append(np.ones_like(list(df_tt['val'])) / float(len(list(df_tt['val']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(w_acc_list), np.median(w_acc_list), np.var(w_acc_list)]
            #             # tweet_avg[t_id] = np.mean(w_acc_list)
            #             # tweet_med[t_id] = np.median(w_acc_list)
            #             # tweet_var[t_id] = np.var(w_acc_list)
            #             #
            #             # tweet_avg_l.append(np.mean(w_acc_list))
            #             # tweet_med_l.append(np.median(w_acc_list))
            #             # tweet_var_l.append(np.var(w_acc_list))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==1:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tt['val'].plot(kind='kde', lw=4, color='g', label='Accuracy')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tt['val']), weights=weights, color='g')
            #
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Accuracy')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(w_acc_list), 3))+', Var : '
            #                             + str(np.round(np.var(w_acc_list), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_acc_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_acc_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #
            #             weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(list(df_tmp['rel_v']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['rel_v'])), np.median(list(df_tmp['rel_v'])), np.var(list(df_tmp['rel_v']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['rel_v']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['rel_v']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['rel_v']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['rel_v'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['rel_v'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['rel_v'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==6:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['rel_v'].plot(kind='kde', lw=4, color='c', label='Perceive truth value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['rel_v']), weights=weights, color='c')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perceive truth value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['rel_v'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['rel_v'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pt_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pt_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['err'])) / float(len(list(df_tmp['err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['err'])), np.median(list(df_tmp['err'])),
            #             #                            np.var(list(df_tmp['err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==2:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['err'].plot(kind='kde', lw=4, color='y', label='Perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['abs_err'])) / float(len(list(df_tmp['abs_err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['abs_err'])), np.median(list(df_tmp['abs_err'])),
            #             #                            np.var(list(df_tmp['abs_err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['abs_err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['abs_err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['abs_err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['abs_err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['abs_err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['abs_err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==3:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['abs_err'].plot(kind='kde', lw=4, color='y', label='Absolute perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['abs_err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Absolute perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['abs_err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['abs_err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_apb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_apb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['gull'])) / float(len(list(df_tmp['gull']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['gull'])), np.median(list(df_tmp['gull'])),
            #             #                            np.var(list(df_tmp['gull']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['gull']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['gull']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['gull']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['gull'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['gull'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['gull'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==4:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['gull'].plot(kind='kde', lw=4, color='k', label='Gullibility value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['gull']), weights=weights, color='k')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Gullibility value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['gull'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['gull'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_gull_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_gull_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['cyn'])) / float(len(list(df_tmp['cyn']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['cyn'])), np.median(list(df_tmp['cyn'])),
            #             #                            np.var(list(df_tmp['cyn']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['cyn']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['cyn']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['cyn']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['cyn'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['cyn'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['cyn'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==5:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['cyn'].plot(kind='kde', lw=4, color='m', label='Cynicality value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['cyn']), weights=weights, color='m')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Cynicality value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['cyn'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['cyn'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_cyn_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_cyn_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #     exit()
            # else:
            #
            #     AVG_list = []
            #     print(np.mean(all_acc))
            #     outF = open(remotedir + 'output.txt', 'w')
            #
            #     tweet_all_var = {}
            #     tweet_all_dev_avg = {}
            #     tweet_all_avg = {}
            #     tweet_all_gt_var = {}
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #     tweet_all_avg_l = []
            #     tweet_all_med_l = []
            #     tweet_all_var_l = []
            #     tweet_all_gt_var_l = []
            #     diff_group_disp_l = []
            #     dem_disp_l = []
            #     rep_disp_l = []
            #
            #     tweet_all_dev_avg = {}
            #     tweet_all_dev_med = {}
            #     tweet_all_dev_var = {}
            #
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #
            #     tweet_all_abs_dev_avg = {}
            #     tweet_all_abs_dev_med = {}
            #     tweet_all_abs_dev_var = {}
            #
            #     tweet_all_abs_dev_avg_l = []
            #     tweet_all_abs_dev_med_l = []
            #     tweet_all_abs_dev_var_l = []
            #     tweet_all_dev_avg_rnd = {}
            #     tweet_all_abs_dev_avg_rnd = {}
            #
            #     diff_group_disp_dict = {}
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #         news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #         news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #         news_cat_list = ['rumor', 'non-rumor']
            #         ind_l = [1]
            #
            #     for cat_l in news_cat_list:
            #         outF.write('== ' + str(cat_l) + ' ==\n\n')
            #         print('== ' + str(cat_l) + ' ==')
            #         tweet_dev_avg = {}
            #         tweet_dev_med = {}
            #         tweet_dev_var = {}
            #         tweet_abs_dev_avg = {}
            #         tweet_abs_dev_med = {}
            #         tweet_abs_dev_var = {}
            #
            #         tweet_avg = {}
            #         tweet_med = {}
            #         tweet_var = {}
            #         tweet_gt_var = {}
            #
            #         tweet_dev_avg_rnd = {}
            #         tweet_abs_dev_avg_rnd = {}
            #
            #
            #         tweet_dev_avg_l = []
            #         tweet_dev_med_l = []
            #         tweet_dev_var_l = []
            #         tweet_abs_dev_avg_l = []
            #         tweet_abs_dev_med_l = []
            #         tweet_abs_dev_var_l = []
            #
            #         tweet_avg_l = []
            #         tweet_med_l = []
            #         tweet_var_l = []
            #         tweet_gt_var_l = []
            #         AVG_susc_list = []
            #         AVG_wl_list = []
            #         all_acc = []
            #         AVG_dev_list = []
            #         # for lean in [-1, 0, 1]:
            #
            #             # AVG_susc_list = []
            #             # AVG_wl_list = []
            #             # all_acc = []
            #             # df_m = df_m[df_m['leaning'] == lean]
            #             # if lean == 0:
            #             #     col = 'g'
            #             #     lean_cat = 'neutral'
            #             # elif lean == 1:
            #             #     col = 'b'
            #             #     lean_cat = 'democrat'
            #             # elif lean == -1:
            #             #     col = 'r'
            #             #     lean_cat = 'republican'
            #             # print(lean_cat)
            #         for ind in ind_l:
            #
            #             if balance_f == 'balanced':
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
            #             else:
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'
            #
            #             inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
            #             df[ind] = pd.read_csv(inp1, sep="\t")
            #             df_w[ind] = pd.read_csv(inp1_w, sep="\t")
            #
            #             df_m = df[ind].copy()
            #             df_mm = df_m.copy()
            #
            #             df_m = df_m[df_m['ra_gt'] == cat_l]
            #             # df_mm = df_m[df_m['ra_gt']==cat_l]
            #             # df_m = df_m[df_m['leaning'] == lean]
            #
            #             groupby_ftr = 'tweet_id'
            #             grouped = df_m.groupby(groupby_ftr, sort=False)
            #             grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #             for t_id in grouped.groups.keys():
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #
            #                 df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
            #                 df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
            #                 df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #
            #                 weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
            #                 val_list = list(df_tmp['rel_v'])
            #                 tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_avg[t_id] = np.mean(val_list)
            #                 tweet_med[t_id] = np.median(val_list)
            #                 tweet_var[t_id] = np.var(val_list)
            #                 tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_avg_l.append(np.mean(val_list))
            #                 tweet_med_l.append(np.median(val_list))
            #                 tweet_var_l.append(np.var(val_list))
            #                 tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #
            #                 tweet_all_avg[t_id] = np.mean(val_list)
            #                 tweet_all_var[t_id] = np.var(val_list)
            #                 tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_all_avg_l.append(np.mean(val_list))
            #                 tweet_all_med_l.append(np.median(val_list))
            #                 tweet_all_var_l.append(np.var(val_list))
            #                 tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #                 val_list = list(df_tmp['err'])
            #                 abs_var_err = [np.abs(x) for x in val_list]
            #                 tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_dev_med[t_id] = np.median(val_list)
            #                 tweet_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_dev_avg_l.append(np.mean(val_list))
            #                 tweet_dev_med_l.append(np.median(val_list))
            #                 tweet_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #                 tweet_all_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_all_dev_med[t_id] = np.median(val_list)
            #                 tweet_all_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_all_dev_avg_l.append(np.mean(val_list))
            #                 tweet_all_dev_med_l.append(np.median(val_list))
            #                 tweet_all_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_all_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #
            #                 sum_rnd_abs_perc = 0
            #                 sum_rnd_perc = 0
            #                 for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
            #                     sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
            #                     sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
            #                 random_perc = np.abs(sum_rnd_perc / float(7))
            #                 random_abs_perc = sum_rnd_abs_perc / float(7)
            #
            #                 tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #                 tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #         gt_l = []
            #         pt_l = []
            #         disputability_l = []
            #         perc_l = []
            #         abs_perc_l = []
            #         # for t_id in tweet_l_sort:
            #         #     gt_l.append(tweet_gt_var[t_id])
            #         #     pt_l.append(tweet_avg[t_id])
            #         #     disputability_l.append(tweet_var[t_id])
            #         #     perc_l.append(tweet_dev_avg[t_id])
            #         #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #         tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #
            #
            #         if dataset == 'snopes':
            #             data_addr = 'snopes'
            #         elif dataset == 'politifact':
            #             data_addr = 'politifact/fig'
            #         elif dataset == 'mia':
            #             data_addr = 'mia/fig'
            #
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #
            #         for t_id in tweet_l_sort:
            #             count+=1
            #             if balance_f=='balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
            #                            + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
            #                            # str(np.round(diff_group_disp_dict[t_id], 3)) +
            #                            '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
            #                             + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
            #                             '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #
            #
            #
            #
            #     if dataset == 'snopes':
            #         data_addr = 'snopes'
            #     elif dataset == 'politifact':
            #         data_addr = 'politifact/fig'
            #     elif dataset == 'mia':
            #         data_addr = 'mia/fig'
            #
            #     # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
            #     tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #
            #     # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #
            #     tweet_napb_dict_high_disp = {}
            #     tweet_napb_dict_low_disp = {}
            #     for t_id in tweet_l_sort[:20]:
            #         # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     for t_id in tweet_l_sort[-20:]:
            #         # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     kk = 0
            #
            #     for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
            #         if kk==0:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
            #         else:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)
            #
            #         kk+=1
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         for t_id in tweet_l_sort:
            #             count += 1
            #             # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
            #             if balance_f == 'balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
            #                            str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #                 #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
            #                 #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
            #                            str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #             # +
            #             # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
            #             # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_ideological_apb_cdf_toghether":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','politifact','mia']:#['snopes']:#'['snopes','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                tweet_id = 100010
                publisher_name = 110
                tweet_popularity = {}
                tweet_text_dic = {}
                for input_file in [input_rumor, input_non_rumor]:
                    for line in input_file:
                        line.replace('\n', '')
                        line_splt = line.split('\t')
                        tweet_txt = line_splt[1]
                        tweet_link = line_splt[1]
                        tweet_id += 1
                        publisher_name += 1
                        tweet_popularity[tweet_id] = int(line_splt[2])
                        tweet_text_dic[tweet_id] = tweet_txt



                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                    # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    #     tweet_lable_dict[tweet_id] = 'undecided'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                col = 'r'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                col = 'g'

                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                col = 'purple'

                data_name = 'Snopes'
            elif dataset=='politifact':
                col = 'c'
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                col = 'orange'
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")
            #
            #     df_m = df[ind].copy()
            #     df[ind].loc[:, 'abs_err'] = df[ind]['tweet_id'] * 0.0
            #     df[ind].loc[:, 'norm_err'] = df[ind]['tweet_id'] * 0.0
            #     df[ind].loc[:, 'norm_abs_err'] = df[ind]['tweet_id'] * 0.0
            #
            #     groupby_ftr = 'tweet_id'
            #     grouped = df[ind].groupby(groupby_ftr, sort=False)
            #     grouped_sum = df[ind].groupby(groupby_ftr, sort=False).sum()
            #
            #
            #     for ind_t in df[ind].index.tolist():
            #         t_id = df[ind]['tweet_id'][ind_t]
            #         err = df[ind]['err'][ind_t]
            #         abs_err = np.abs(err)
            #         df[ind]['abs_err'][ind_t] = abs_err
            #         sum_rnd_abs_perc = 0
            #         sum_rnd_perc = 0
            #         for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
            #             sum_rnd_perc+= (val - df[ind]['rel_gt_v'][ind_t])
            #             sum_rnd_abs_perc += np.abs(val - df[ind]['rel_gt_v'][ind_t])
            #         random_perc = np.abs(sum_rnd_perc / float(7))
            #         random_abs_perc = sum_rnd_abs_perc / float(7)
            #
            #
            #         norm_err = err / float(random_perc)
            #         norm_abs_err = abs_err / float(random_abs_perc)
            #         df[ind]['norm_err'][ind_t] = norm_err
            #         df[ind]['norm_abs_err'][ind_t] = norm_abs_err
            #
            #     # df[ind] = df[ind].copy()

            w_pt_avg_l = []
            w_err_avg_l = []
            w_abs_err_avg_l = []
            w_norm_err_avg_l = []
            w_norm_abs_err_avg_l = []
            w_acc_avg_l = []

            w_pt_std_l = []
            w_err_std_l = []
            w_abs_err_std_l = []
            w_norm_err_std_l = []
            w_norm_abs_err_std_l = []
            w_acc_std_l = []

            w_pt_avg_dict = collections.defaultdict()
            w_err_avg_dict = collections.defaultdict()
            w_abs_err_avg_dict = collections.defaultdict()
            w_norm_err_avg_dict = collections.defaultdict()
            w_norm_abs_err_avg_dict = collections.defaultdict()
            w_acc_avg_dict = collections.defaultdict()

            w_pt_std_dict = collections.defaultdict()
            w_err_std_dict = collections.defaultdict()
            w_abs_err_std_dict = collections.defaultdict()
            w_norm_err_std_dict = collections.defaultdict()
            w_norm_abs_err_std_dict = collections.defaultdict()
            w_acc_std_dict = collections.defaultdict()

            all_w_pt_list  = []
            all_w_err_list = []
            all_w_abs_err_list = []
            all_w_norm_err_list = []
            all_w_norm_abs_err_list  = []
            all_w_acc_list = []

            all_w_cyn_list = []
            all_w_gull_list = []
            w_cyn_avg_l = []
            w_gull_avg_l = []
            w_cyn_std_l= []
            w_gull_std_l = []
            w_cyn_avg_dict =collections.defaultdict()
            w_gull_avg_dict =collections.defaultdict()
            w_cyn_std_dict =collections.defaultdict()
            w_gull_std_dict = collections.defaultdict()
            w_err_avg_dict_dem = collections.defaultdict()
            w_err_avg_dict_rep = collections.defaultdict()
            w_err_avg_dict_neut = collections.defaultdict()
            w_susc_avg_dict = collections.defaultdict()

            w_cyn_avg_dict = collections.defaultdict()
            w_gull_avg_dict = collections.defaultdict()
            w_susc_avg_dict = collections.defaultdict()

            w_cyn_avg_dict_dem = collections.defaultdict()
            w_gull_avg_dict_dem = collections.defaultdict()
            w_susc_avg_dict_dem = collections.defaultdict()

            w_cyn_avg_dict_rep = collections.defaultdict()
            w_gull_avg_dict_rep = collections.defaultdict()
            w_susc_avg_dict_rep = collections.defaultdict()

            w_cyn_avg_dict_neut = collections.defaultdict()
            w_gull_avg_dict_neut = collections.defaultdict()
            w_susc_avg_dict_neut = collections.defaultdict()
            w_err_avg_dict_dem= collections.defaultdict()
            w_err_avg_dict_rep= collections.defaultdict()

            for ind in ind_l:

                df_m = df[ind].copy()
                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]

                    df_dem = df_tmp[df_tmp['leaning']==1]
                    df_rep = df_tmp[df_tmp['leaning']==-1]
                    df_neut = df_tmp[df_tmp['leaning']==0]


                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    w_susc_list = list(df_tmp['susc'])

                    w_cyn_all = list(df_tmp['cyn'])
                    w_gull_all = list(df_tmp['gull'])
                    w_susc_all = list(df_tmp['susc'])

                    w_cyn_dem = list(df_dem['cyn'])
                    w_gull_dem = list(df_dem['gull'])
                    w_susc_dem = list(df_dem['susc'])


                    w_cyn_rep = list(df_rep['cyn'])
                    w_gull_rep = list(df_rep['gull'])
                    w_susc_rep = list(df_rep['susc'])

                    w_cyn_neut = list(df_neut['cyn'])
                    w_gull_neut = list(df_neut['gull'])
                    w_susc_neut = list(df_neut['susc'])

                    w_acc_list_tmp = list(df_tmp['acc'])
                    w_acc_list = []

                    w_err_list_dem = list(df_dem['err'])
                    w_err_list_rep = list(df_rep['err'])
                    w_err_list_neut = list(df_neut['err'])

                    acc_c = 0
                    nacc_c = 0


                    w_err_avg_dict_dem[t_id] = np.mean(w_err_list_dem)
                    w_err_avg_dict_rep[t_id] = np.mean(w_err_list_rep)


                    w_cyn_avg_dict[t_id] = np.mean(w_cyn_all)
                    w_gull_avg_dict[t_id] = np.mean(w_gull_all)
                    w_susc_avg_dict[t_id] = np.mean(w_susc_all)

                    w_cyn_avg_dict_dem[t_id] = np.mean(w_cyn_dem)
                    w_gull_avg_dict_dem[t_id] = np.mean(w_gull_dem)
                    w_susc_avg_dict_dem[t_id] = np.mean(w_susc_dem)

                    w_cyn_avg_dict_rep[t_id] = np.mean(w_cyn_rep)
                    w_gull_avg_dict_rep[t_id] = np.mean(w_gull_rep)
                    w_susc_avg_dict_rep[t_id] = np.mean(w_susc_rep)

                    w_cyn_avg_dict_neut[t_id] = np.mean(w_cyn_neut)
                    w_gull_avg_dict_neut[t_id] = np.mean(w_gull_neut)
                    w_susc_avg_dict_neut[t_id] = np.mean(w_susc_neut)

            # ind_
            ##################################################
            #
            # tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            # gt_l = []
            # pt_l = []
            # disputability_l = []
            # perc_l = []
            # abs_perc_l=[]
            # abs_perc_rnd_l = []
            # perc_rnd_l = []
            # tweet_skew_ll = []
            # for t_id in tweet_l_sort:
            #     gt_l.append(tweet_gt_var[t_id])
            #     pt_l.append(tweet_avg[t_id])
            #     disputability_l.append(tweet_var[t_id])
            #     perc_l.append(tweet_dev_avg[t_id])
            #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #     perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
            #     abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
            #     tweet_skew_ll.append(tweet_skew[t_id])
            # value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l,tweet_skew_ll]
            # value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
            #               'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'skewness']
            #
            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(8):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(8):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')
            #
            #
            # exit()

            # fig_f = True
            fig_f = False
            # fig_f_1 = True
            fig_f_1 = False
            fig_f_together = True


            if fig_f_together==True:

                ####dem
                out_dict = w_susc_avg_dict
                # out_dict = w_gull_avg_dict
                # out_dict = w_cyn_avg_dict

                mplpl.rcParams['figure.figsize'] = 5.4, 3
                mplpl.rc('xtick', labelsize='large')
                mplpl.rc('ytick', labelsize='large')
                mplpl.rc('legend', fontsize='medium')
                # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
                tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
                acc_l_rep = []
                acc_l_dem = []
                acc_l_neut = []
                acc_l_all = []
                acc_l_diff = []
                for t_id in tweet_l_sort:



                    # acc_l_dem.append(w_susc_avg_dict_dem[t_id])
                    # acc_l_rep.append(w_susc_avg_dict_rep[t_id])
                    # acc_l_neut.append(w_susc_avg_dict_neut[t_id])
                    # acc_l_all.append(w_susc_avg_dict[t_id])

                    # acc_l_dem.append(w_gull_avg_dict_dem[t_id])
                    # acc_l_rep.append(w_gull_avg_dict_rep[t_id])
                    # acc_l_neut.append(w_gull_avg_dict_neut[t_id])
                    # acc_l_all.append(w_gull_avg_dict[t_id])
                    #
                    # acc_l_dem.append(w_cyn_avg_dict_dem[t_id])
                    # acc_l_rep.append(w_cyn_avg_dict_rep[t_id])
                    # acc_l_neut.append(w_cyn_avg_dict_neut[t_id])
                    # acc_l_all.append(w_cyn_avg_dict[t_id])

                    # acc_l_dem = sorted(acc_l_dem)
                    # acc_l_rep = sorted(acc_l_rep)
                    # acc_l_neut = sorted(acc_l_neut)
                    # acc_l_all = sorted(acc_l_all)


                    # acc_l_diff.append(w_susc_avg_dict_dem[t_id] - w_susc_avg_dict_rep[t_id])
                    acc_l_diff.append(w_err_avg_dict_dem[t_id] - w_err_avg_dict_rep[t_id])

                    # acc_l_diff.append(w_gull_avg_dict_dem[t_id] - w_gull_avg_dict_rep[t_id])
                    # acc_l_diff.append(w_cyn_avg_dict_dem[t_id] - w_cyn_avg_dict_rep[t_id])

                    # acc_l_diff.append(w_err_avg_dict_dem[t_id] - w_err_avg_dict_rep[t_id])

                    # acc_l_diff = sorted(acc_l_diff)
                # tweet_l_sort = sorted(w_err_avg_dict_rep, key=w_err_avg_dict_rep.get, reverse=False)
                # acc_l_rep = []
                # # acc_l_dem = []
                # for t_id in tweet_l_sort:
                #     # acc_l_dem.append(w_err_avg_dict_dem[t_id])
                #     acc_l_rep.append(w_err_avg_dict_rep[t_id])

                acc_l_diff = sorted(acc_l_diff)


                num_bins = len(acc_l_diff)
                counts, bin_edges = np.histogram(acc_l_diff, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=4, label=data_name)


                # num_bins = len(acc_l_all)
                # counts, bin_edges = np.histogram(acc_l_all, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c='k', lw=4, label='All users')
                #
                # num_bins = len(acc_l_dem)
                # counts, bin_edges = np.histogram(acc_l_dem, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c='b', lw=4, label='Democrats')
                #
                #
                # num_bins = len(acc_l_neut)
                # counts, bin_edges = np.histogram(acc_l_neut, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c='g', lw=4, label='Neutrals')
                #
                #
                # num_bins = len(acc_l_rep)
                # counts, bin_edges = np.histogram(acc_l_rep, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c='red', lw=4, label='Republicans')





        #
        legend_properties = {'weight': 'bold'}
        mplpl.ylabel('CDF', fontsize=24, fontweight = 'bold')
        # mplpl.xlabel('TPB of Democrats - TPB of Repuplicans', fontsize=14, fontweight = 'bold')
        mplpl.xlabel('MPB of Democrats - MPB of Repuplicans', fontsize=14, fontweight = 'bold')
        # mplpl.xlabel('FPB of Democrats - FPB of Repuplicans', fontsize=14, fontweight = 'bold')
        # mplpl.xlabel('FNB of Democrats - FNB of Repuplicans', fontsize=14, fontweight = 'bold')
        # mplpl.xlabel('Absolute Perception Bias', fontsize=18, fontweight = 'bold')
        # mplpl.xlabel('False Positive Bias', fontsize=18, fontweight = 'bold')
        # mplpl.xlabel('False Negative Bias', fontsize=18, fontweight = 'bold')
        # mplpl.title(data_name,fontsize='xx-large')
        mplpl.legend(loc="lower right",prop=legend_properties,fontsize = 'medium',ncol=1)
        # mplpl.xlim([0, 1.5])
        # mplpl.ylim([0, 1])
        mplpl.xlim([-1, 1])
        mplpl.grid()
        mplpl.subplots_adjust(bottom=0.24)
        mplpl.subplots_adjust(left=0.18)
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_MPB_alldataset.pdf'
        pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_MPB_all_diff_cdf'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/ITPB_diff_cdf'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_FPB_snopes_diff_cdf'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_FNB_snopes_diff_cdf'


        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_APB_snopes_cdf'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_FPB_snopes_cdf'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_FNB_snopes_cdf'
        mplpl.savefig(pp+'.pdf', format='pdf')
        mplpl.savefig(pp+'.png', format='png')












        #         mplpl.rcParams['figure.figsize'] = 5.4, 3.2
        #         mplpl.rc('xtick', labelsize='x-large')
        #         mplpl.rc('ytick', labelsize='x-large')
        #         mplpl.rc('legend', fontsize='medium')
        #         w_err_avg_dict
        #         # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
        #         tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
        #         acc_l = []
        #         for t_id in tweet_l_sort:
        #             acc_l.append(w_err_avg_dict[t_id])
        #
        #         num_bins = len(acc_l)
        #         counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         cdf = np.cumsum(counts)
        #         scale = 1.0 / cdf[-1]
        #         ncdf = scale * cdf
        #         mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)
        #
        # legend_properties = {'weight': 'bold'}
        #
        # mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        # mplpl.xlabel('Mean perception bias', fontsize=20, fontweight = 'bold')
        # mplpl.legend(loc="upper left", prop=legend_properties, fontsize='medium', ncol=1)
        # # mplpl.title(data_name)
        # # mplpl.legend(loc="upper left",fontsize = 'large')
        # mplpl.xlim([-1.5, 1.5])
        # mplpl.ylim([0, 1])
        # mplpl.grid()
        # mplpl.subplots_adjust(bottom=0.24)
        # mplpl.subplots_adjust(left=0.18)
        # # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/MPB_cdf_alldataset'
        # mplpl.savefig(pp + '.pdf', format='pdf')
        # mplpl.savefig(pp + '.png', format='png')
        #

        # elif fig_f_1==True:
            #     balance_f = 'un_balanced'
            #     # balance_f = 'balanced'
            #
            #     # fig_f = True
            #     # fig_f = False
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #
            #     fig_p = 7
            #
            #     for ind in ind_l:
            #
            #         df_m = df[ind].copy()
            #
            #         groupby_ftr = 'worker_id'
            #         grouped = df_m.groupby(groupby_ftr, sort=False)
            #         grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #         # df_tmp = df_m[df_m['tweet_id'] == t_id]
            #         w_cc=0
            #         for w_id in grouped.groups.keys():
            #             print(w_cc)
            #             w_cc+=1
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             df_tmp = df_m[df_m['worker_id'] == w_id]
            #             ind_t = df_tmp.index.tolist()[0]
            #             weights = []
            #             w_acc_list_tmp = list(df_tmp['acc'])
            #             w_acc_list = []
            #             for el in w_acc_list_tmp:
            #                 if el == 0:
            #                     w_acc_list.append(-1)
            #                 elif el == 1:
            #                     w_acc_list.append(1)
            #                 else:
            #                     w_acc_list.append(0)
            #             df_tt = pd.DataFrame({'val' : w_acc_list})
            #
            #             weights.append(np.ones_like(list(df_tt['val'])) / float(len(list(df_tt['val']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(w_acc_list), np.median(w_acc_list), np.var(w_acc_list)]
            #             # tweet_avg[t_id] = np.mean(w_acc_list)
            #             # tweet_med[t_id] = np.median(w_acc_list)
            #             # tweet_var[t_id] = np.var(w_acc_list)
            #             #
            #             # tweet_avg_l.append(np.mean(w_acc_list))
            #             # tweet_med_l.append(np.median(w_acc_list))
            #             # tweet_var_l.append(np.var(w_acc_list))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==1:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tt['val'].plot(kind='kde', lw=4, color='g', label='Accuracy')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tt['val']), weights=weights, color='g')
            #
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Accuracy')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(w_acc_list), 3))+', Var : '
            #                             + str(np.round(np.var(w_acc_list), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_acc_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_acc_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #
            #             weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(list(df_tmp['rel_v']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['rel_v'])), np.median(list(df_tmp['rel_v'])), np.var(list(df_tmp['rel_v']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['rel_v']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['rel_v']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['rel_v']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['rel_v'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['rel_v'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['rel_v'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==6:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['rel_v'].plot(kind='kde', lw=4, color='c', label='Perceive truth value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['rel_v']), weights=weights, color='c')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perceive truth value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['rel_v'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['rel_v'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pt_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pt_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['err'])) / float(len(list(df_tmp['err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['err'])), np.median(list(df_tmp['err'])),
            #             #                            np.var(list(df_tmp['err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==2:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['err'].plot(kind='kde', lw=4, color='y', label='Perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['abs_err'])) / float(len(list(df_tmp['abs_err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['abs_err'])), np.median(list(df_tmp['abs_err'])),
            #             #                            np.var(list(df_tmp['abs_err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['abs_err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['abs_err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['abs_err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['abs_err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['abs_err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['abs_err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==3:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['abs_err'].plot(kind='kde', lw=4, color='y', label='Absolute perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['abs_err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Absolute perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['abs_err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['abs_err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_apb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_apb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['gull'])) / float(len(list(df_tmp['gull']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['gull'])), np.median(list(df_tmp['gull'])),
            #             #                            np.var(list(df_tmp['gull']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['gull']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['gull']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['gull']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['gull'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['gull'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['gull'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==4:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['gull'].plot(kind='kde', lw=4, color='k', label='Gullibility value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['gull']), weights=weights, color='k')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Gullibility value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['gull'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['gull'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_gull_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_gull_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['cyn'])) / float(len(list(df_tmp['cyn']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['cyn'])), np.median(list(df_tmp['cyn'])),
            #             #                            np.var(list(df_tmp['cyn']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['cyn']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['cyn']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['cyn']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['cyn'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['cyn'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['cyn'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==5:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['cyn'].plot(kind='kde', lw=4, color='m', label='Cynicality value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['cyn']), weights=weights, color='m')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Cynicality value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['cyn'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['cyn'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_cyn_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_cyn_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #     exit()
            # else:
            #
            #     AVG_list = []
            #     print(np.mean(all_acc))
            #     outF = open(remotedir + 'output.txt', 'w')
            #
            #     tweet_all_var = {}
            #     tweet_all_dev_avg = {}
            #     tweet_all_avg = {}
            #     tweet_all_gt_var = {}
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #     tweet_all_avg_l = []
            #     tweet_all_med_l = []
            #     tweet_all_var_l = []
            #     tweet_all_gt_var_l = []
            #     diff_group_disp_l = []
            #     dem_disp_l = []
            #     rep_disp_l = []
            #
            #     tweet_all_dev_avg = {}
            #     tweet_all_dev_med = {}
            #     tweet_all_dev_var = {}
            #
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #
            #     tweet_all_abs_dev_avg = {}
            #     tweet_all_abs_dev_med = {}
            #     tweet_all_abs_dev_var = {}
            #
            #     tweet_all_abs_dev_avg_l = []
            #     tweet_all_abs_dev_med_l = []
            #     tweet_all_abs_dev_var_l = []
            #     tweet_all_dev_avg_rnd = {}
            #     tweet_all_abs_dev_avg_rnd = {}
            #
            #     diff_group_disp_dict = {}
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #         news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #         news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #         news_cat_list = ['rumor', 'non-rumor']
            #         ind_l = [1]
            #
            #     for cat_l in news_cat_list:
            #         outF.write('== ' + str(cat_l) + ' ==\n\n')
            #         print('== ' + str(cat_l) + ' ==')
            #         tweet_dev_avg = {}
            #         tweet_dev_med = {}
            #         tweet_dev_var = {}
            #         tweet_abs_dev_avg = {}
            #         tweet_abs_dev_med = {}
            #         tweet_abs_dev_var = {}
            #
            #         tweet_avg = {}
            #         tweet_med = {}
            #         tweet_var = {}
            #         tweet_gt_var = {}
            #
            #         tweet_dev_avg_rnd = {}
            #         tweet_abs_dev_avg_rnd = {}
            #
            #
            #         tweet_dev_avg_l = []
            #         tweet_dev_med_l = []
            #         tweet_dev_var_l = []
            #         tweet_abs_dev_avg_l = []
            #         tweet_abs_dev_med_l = []
            #         tweet_abs_dev_var_l = []
            #
            #         tweet_avg_l = []
            #         tweet_med_l = []
            #         tweet_var_l = []
            #         tweet_gt_var_l = []
            #         AVG_susc_list = []
            #         AVG_wl_list = []
            #         all_acc = []
            #         AVG_dev_list = []
            #         # for lean in [-1, 0, 1]:
            #
            #             # AVG_susc_list = []
            #             # AVG_wl_list = []
            #             # all_acc = []
            #             # df_m = df_m[df_m['leaning'] == lean]
            #             # if lean == 0:
            #             #     col = 'g'
            #             #     lean_cat = 'neutral'
            #             # elif lean == 1:
            #             #     col = 'b'
            #             #     lean_cat = 'democrat'
            #             # elif lean == -1:
            #             #     col = 'r'
            #             #     lean_cat = 'republican'
            #             # print(lean_cat)
            #         for ind in ind_l:
            #
            #             if balance_f == 'balanced':
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
            #             else:
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'
            #
            #             inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
            #             df[ind] = pd.read_csv(inp1, sep="\t")
            #             df_w[ind] = pd.read_csv(inp1_w, sep="\t")
            #
            #             df_m = df[ind].copy()
            #             df_mm = df_m.copy()
            #
            #             df_m = df_m[df_m['ra_gt'] == cat_l]
            #             # df_mm = df_m[df_m['ra_gt']==cat_l]
            #             # df_m = df_m[df_m['leaning'] == lean]
            #
            #             groupby_ftr = 'tweet_id'
            #             grouped = df_m.groupby(groupby_ftr, sort=False)
            #             grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #             for t_id in grouped.groups.keys():
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #
            #                 df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
            #                 df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
            #                 df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #
            #                 weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
            #                 val_list = list(df_tmp['rel_v'])
            #                 tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_avg[t_id] = np.mean(val_list)
            #                 tweet_med[t_id] = np.median(val_list)
            #                 tweet_var[t_id] = np.var(val_list)
            #                 tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_avg_l.append(np.mean(val_list))
            #                 tweet_med_l.append(np.median(val_list))
            #                 tweet_var_l.append(np.var(val_list))
            #                 tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #
            #                 tweet_all_avg[t_id] = np.mean(val_list)
            #                 tweet_all_var[t_id] = np.var(val_list)
            #                 tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_all_avg_l.append(np.mean(val_list))
            #                 tweet_all_med_l.append(np.median(val_list))
            #                 tweet_all_var_l.append(np.var(val_list))
            #                 tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #                 val_list = list(df_tmp['err'])
            #                 abs_var_err = [np.abs(x) for x in val_list]
            #                 tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_dev_med[t_id] = np.median(val_list)
            #                 tweet_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_dev_avg_l.append(np.mean(val_list))
            #                 tweet_dev_med_l.append(np.median(val_list))
            #                 tweet_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #                 tweet_all_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_all_dev_med[t_id] = np.median(val_list)
            #                 tweet_all_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_all_dev_avg_l.append(np.mean(val_list))
            #                 tweet_all_dev_med_l.append(np.median(val_list))
            #                 tweet_all_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_all_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #
            #                 sum_rnd_abs_perc = 0
            #                 sum_rnd_perc = 0
            #                 for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
            #                     sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
            #                     sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
            #                 random_perc = np.abs(sum_rnd_perc / float(7))
            #                 random_abs_perc = sum_rnd_abs_perc / float(7)
            #
            #                 tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #                 tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #         gt_l = []
            #         pt_l = []
            #         disputability_l = []
            #         perc_l = []
            #         abs_perc_l = []
            #         # for t_id in tweet_l_sort:
            #         #     gt_l.append(tweet_gt_var[t_id])
            #         #     pt_l.append(tweet_avg[t_id])
            #         #     disputability_l.append(tweet_var[t_id])
            #         #     perc_l.append(tweet_dev_avg[t_id])
            #         #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #         tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #
            #
            #         if dataset == 'snopes':
            #             data_addr = 'snopes'
            #         elif dataset == 'politifact':
            #             data_addr = 'politifact/fig'
            #         elif dataset == 'mia':
            #             data_addr = 'mia/fig'
            #
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #
            #         for t_id in tweet_l_sort:
            #             count+=1
            #             if balance_f=='balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
            #                            + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
            #                            # str(np.round(diff_group_disp_dict[t_id], 3)) +
            #                            '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
            #                             + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
            #                             '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #
            #
            #
            #
            #     if dataset == 'snopes':
            #         data_addr = 'snopes'
            #     elif dataset == 'politifact':
            #         data_addr = 'politifact/fig'
            #     elif dataset == 'mia':
            #         data_addr = 'mia/fig'
            #
            #     # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
            #     tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #
            #     # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #
            #     tweet_napb_dict_high_disp = {}
            #     tweet_napb_dict_low_disp = {}
            #     for t_id in tweet_l_sort[:20]:
            #         # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     for t_id in tweet_l_sort[-20:]:
            #         # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     kk = 0
            #
            #     for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
            #         if kk==0:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
            #         else:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)
            #
            #         kk+=1
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         for t_id in tweet_l_sort:
            #             count += 1
            #             # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
            #             if balance_f == 'balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
            #                            str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #                 #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
            #                 #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
            #                            str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #             # +
            #             # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
            #             # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_ideological_disp_cdf_toghether":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','politifact','mia']:#['snopes']:#'['snopes','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                tweet_id = 100010
                publisher_name = 110
                tweet_popularity = {}
                tweet_text_dic = {}
                for input_file in [input_rumor, input_non_rumor]:
                    for line in input_file:
                        line.replace('\n', '')
                        line_splt = line.split('\t')
                        tweet_txt = line_splt[1]
                        tweet_link = line_splt[1]
                        tweet_id += 1
                        publisher_name += 1
                        tweet_popularity[tweet_id] = int(line_splt[2])
                        tweet_text_dic[tweet_id] = tweet_txt



                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                    # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    #     tweet_lable_dict[tweet_id] = 'undecided'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                col = 'r'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                col = 'g'

                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                col = 'purple'

                data_name = 'Snopes'
            elif dataset=='politifact':
                col = 'c'
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                col = 'orange'
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")
            #
            #     df_m = df[ind].copy()
            #     df[ind].loc[:, 'abs_err'] = df[ind]['tweet_id'] * 0.0
            #     df[ind].loc[:, 'norm_err'] = df[ind]['tweet_id'] * 0.0
            #     df[ind].loc[:, 'norm_abs_err'] = df[ind]['tweet_id'] * 0.0
            #
            #     groupby_ftr = 'tweet_id'
            #     grouped = df[ind].groupby(groupby_ftr, sort=False)
            #     grouped_sum = df[ind].groupby(groupby_ftr, sort=False).sum()
            #
            #
            #     for ind_t in df[ind].index.tolist():
            #         t_id = df[ind]['tweet_id'][ind_t]
            #         err = df[ind]['err'][ind_t]
            #         abs_err = np.abs(err)
            #         df[ind]['abs_err'][ind_t] = abs_err
            #         sum_rnd_abs_perc = 0
            #         sum_rnd_perc = 0
            #         for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
            #             sum_rnd_perc+= (val - df[ind]['rel_gt_v'][ind_t])
            #             sum_rnd_abs_perc += np.abs(val - df[ind]['rel_gt_v'][ind_t])
            #         random_perc = np.abs(sum_rnd_perc / float(7))
            #         random_abs_perc = sum_rnd_abs_perc / float(7)
            #
            #
            #         norm_err = err / float(random_perc)
            #         norm_abs_err = abs_err / float(random_abs_perc)
            #         df[ind]['norm_err'][ind_t] = norm_err
            #         df[ind]['norm_abs_err'][ind_t] = norm_abs_err
            #
            #     # df[ind] = df[ind].copy()

            w_pt_avg_l = []
            w_err_avg_l = []
            w_abs_err_avg_l = []
            w_norm_err_avg_l = []
            w_norm_abs_err_avg_l = []
            w_acc_avg_l = []

            w_pt_std_l = []
            w_err_std_l = []
            w_abs_err_std_l = []
            w_norm_err_std_l = []
            w_norm_abs_err_std_l = []
            w_acc_std_l = []

            w_pt_avg_dict = collections.defaultdict()
            w_err_avg_dict = collections.defaultdict()
            w_abs_err_avg_dict = collections.defaultdict()
            w_norm_err_avg_dict = collections.defaultdict()
            w_norm_abs_err_avg_dict = collections.defaultdict()
            w_acc_avg_dict = collections.defaultdict()

            w_pt_std_dict = collections.defaultdict()
            w_err_std_dict = collections.defaultdict()
            w_abs_err_std_dict = collections.defaultdict()
            w_norm_err_std_dict = collections.defaultdict()
            w_norm_abs_err_std_dict = collections.defaultdict()
            w_acc_std_dict = collections.defaultdict()

            all_w_pt_list  = []
            all_w_err_list = []
            all_w_abs_err_list = []
            all_w_norm_err_list = []
            all_w_norm_abs_err_list  = []
            all_w_acc_list = []

            all_w_cyn_list = []
            all_w_gull_list = []
            w_cyn_avg_l = []
            w_gull_avg_l = []
            w_cyn_std_l= []
            w_gull_std_l = []
            w_cyn_avg_dict =collections.defaultdict()
            w_gull_avg_dict =collections.defaultdict()
            w_cyn_std_dict =collections.defaultdict()
            w_gull_std_dict = collections.defaultdict()
            w_err_avg_dict_dem = collections.defaultdict()
            w_err_avg_dict_rep = collections.defaultdict()
            w_err_avg_dict_neut = collections.defaultdict()
            w_susc_avg_dict = collections.defaultdict()

            w_cyn_avg_dict = collections.defaultdict()
            w_gull_avg_dict = collections.defaultdict()
            w_susc_avg_dict = collections.defaultdict()

            w_cyn_avg_dict_dem = collections.defaultdict()
            w_gull_avg_dict_dem = collections.defaultdict()
            w_susc_avg_dict_dem = collections.defaultdict()

            w_cyn_avg_dict_rep = collections.defaultdict()
            w_gull_avg_dict_rep = collections.defaultdict()
            w_susc_avg_dict_rep = collections.defaultdict()

            w_cyn_avg_dict_neut = collections.defaultdict()
            w_gull_avg_dict_neut = collections.defaultdict()
            w_susc_avg_dict_neut = collections.defaultdict()
            for ind in ind_l:

                df_m = df[ind].copy()
                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]

                    df_dem = df_tmp[df_tmp['leaning']==1]
                    df_rep = df_tmp[df_tmp['leaning']==-1]
                    df_neut = df_tmp[df_tmp['leaning']==0]


                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    w_susc_list = list(df_tmp['susc'])

                    w_cyn_all = list(df_tmp['cyn'])
                    w_gull_all = list(df_tmp['gull'])
                    w_susc_all = list(df_tmp['susc'])

                    w_cyn_dem = list(df_dem['cyn'])
                    w_gull_dem = list(df_dem['gull'])
                    w_susc_dem = list(df_dem['susc'])


                    w_cyn_rep = list(df_rep['cyn'])
                    w_gull_rep = list(df_rep['gull'])
                    w_susc_rep = list(df_rep['susc'])

                    w_cyn_neut = list(df_neut['cyn'])
                    w_gull_neut = list(df_neut['gull'])
                    w_susc_neut = list(df_neut['susc'])

                    w_acc_list_tmp = list(df_tmp['acc'])
                    w_acc_list = []


                    w_err_list_dem = list(df_dem['err'])
                    w_err_list_rep = list(df_rep['err'])
                    w_err_list_neut = list(df_neut['err'])

                    acc_c = 0
                    nacc_c = 0



                    w_cyn_avg_dict[t_id] = np.mean(w_cyn_all)
                    w_gull_avg_dict[t_id] = np.mean(w_gull_all)
                    w_susc_avg_dict[t_id] = np.mean(w_susc_all)

                    w_cyn_avg_dict_dem[t_id] = np.mean(w_cyn_dem)
                    w_gull_avg_dict_dem[t_id] = np.mean(w_gull_dem)
                    w_susc_avg_dict_dem[t_id] = np.mean(w_susc_dem)

                    w_cyn_avg_dict_rep[t_id] = np.mean(w_cyn_rep)
                    w_gull_avg_dict_rep[t_id] = np.mean(w_gull_rep)
                    w_susc_avg_dict_rep[t_id] = np.mean(w_susc_rep)

                    w_cyn_avg_dict_neut[t_id] = np.mean(w_cyn_neut)
                    w_gull_avg_dict_neut[t_id] = np.mean(w_gull_neut)
                    w_susc_avg_dict_neut[t_id] = np.mean(w_susc_neut)

            # ind_
            ##################################################
            #
            # tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            # gt_l = []
            # pt_l = []
            # disputability_l = []
            # perc_l = []
            # abs_perc_l=[]
            # abs_perc_rnd_l = []
            # perc_rnd_l = []
            # tweet_skew_ll = []
            # for t_id in tweet_l_sort:
            #     gt_l.append(tweet_gt_var[t_id])
            #     pt_l.append(tweet_avg[t_id])
            #     disputability_l.append(tweet_var[t_id])
            #     perc_l.append(tweet_dev_avg[t_id])
            #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #     perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
            #     abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
            #     tweet_skew_ll.append(tweet_skew[t_id])
            # value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l,tweet_skew_ll]
            # value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
            #               'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'skewness']
            #
            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(8):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(8):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')
            #
            #
            # exit()

            # fig_f = True
            fig_f = False
            # fig_f_1 = True
            fig_f_1 = False
            fig_f_together = True


            if fig_f_together==True:

                ####dem
                # out_dict = w_susc_avg_dict
                # out_dict = w_gull_avg_dict
                out_dict = w_cyn_avg_dict

                mplpl.rcParams['figure.figsize'] = 4.5, 2.5
                mplpl.rc('xtick', labelsize='large')
                mplpl.rc('ytick', labelsize='large')
                mplpl.rc('legend', fontsize='medium')
                # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
                tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
                acc_l_rep = []
                acc_l_dem = []
                acc_l_neut = []
                acc_l_all = []
                acc_l_diff = []
                for t_id in tweet_l_sort:



                    # acc_l_dem.append(w_susc_avg_dict_dem[t_id])
                    # acc_l_rep.append(w_susc_avg_dict_rep[t_id])
                    # acc_l_neut.append(w_susc_avg_dict_neut[t_id])
                    # acc_l_all.append(w_susc_avg_dict[t_id])

                    # acc_l_dem.append(w_gull_avg_dict_dem[t_id])
                    # acc_l_rep.append(w_gull_avg_dict_rep[t_id])
                    # acc_l_neut.append(w_gull_avg_dict_neut[t_id])
                    # acc_l_all.append(w_gull_avg_dict[t_id])
                    #
                    # acc_l_dem.append(w_cyn_avg_dict_dem[t_id])
                    # acc_l_rep.append(w_cyn_avg_dict_rep[t_id])
                    # acc_l_neut.append(w_cyn_avg_dict_neut[t_id])
                    # acc_l_all.append(w_cyn_avg_dict[t_id])

                    # acc_l_dem = sorted(acc_l_dem)
                    # acc_l_rep = sorted(acc_l_rep)
                    # acc_l_neut = sorted(acc_l_neut)
                    # acc_l_all = sorted(acc_l_all)


                    # acc_l_diff.append(w_susc_avg_dict_dem[t_id] - w_susc_avg_dict_rep[t_id])

                    # acc_l_diff.append(w_gull_avg_dict_dem[t_id] - w_gull_avg_dict_rep[t_id])
                    # acc_l_diff.append(w_cyn_avg_dict_dem[t_id] - w_cyn_avg_dict_rep[t_id])

                    acc_l_diff.append(w_err_avg_dict_dem[t_id] - w_err_avg_dict_rep[t_id])

                    # acc_l_diff = sorted(acc_l_diff)
                # tweet_l_sort = sorted(w_err_avg_dict_rep, key=w_err_avg_dict_rep.get, reverse=False)
                # acc_l_rep = []
                # # acc_l_dem = []
                # for t_id in tweet_l_sort:
                #     # acc_l_dem.append(w_err_avg_dict_dem[t_id])
                #     acc_l_rep.append(w_err_avg_dict_rep[t_id])

                acc_l_diff = sorted(acc_l_diff)


                num_bins = len(acc_l_diff)
                counts, bin_edges = np.histogram(acc_l_diff, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=4, label=data_name)


                # num_bins = len(acc_l_all)
                # counts, bin_edges = np.histogram(acc_l_all, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c='k', lw=4, label='All users')
                #
                # num_bins = len(acc_l_dem)
                # counts, bin_edges = np.histogram(acc_l_dem, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c='b', lw=4, label='Democrats')
                #
                #
                # num_bins = len(acc_l_neut)
                # counts, bin_edges = np.histogram(acc_l_neut, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c='g', lw=4, label='Neutrals')
                #
                #
                # num_bins = len(acc_l_rep)
                # counts, bin_edges = np.histogram(acc_l_rep, bins=num_bins, normed=True)
                # cdf = np.cumsum(counts)
                # scale = 1.0 / cdf[-1]
                # ncdf = scale * cdf
                # mplpl.plot(bin_edges[1:], ncdf, c='red', lw=4, label='Republicans')

        # mplpl.rcParams['figure.figsize'] = 5.4, 3
        # mplpl.rc('xtick', labelsize='large')
        # mplpl.rc('ytick', labelsize='large')
        # mplpl.rc('legend', fontsize='medium')

        #
        legend_properties = {'weight': 'bold'}
        mplpl.ylabel('CDF', fontsize=16, fontweight = 'bold')
        # mplpl.xlabel('APB of Democrats - APB of Repuplicans', fontsize=14, fontweight = 'bold')
        mplpl.xlabel('r$|MPB_{Dem} - MPB_{Rep}|$', fontsize=16, fontweight = 'bold')
        # mplpl.xlabel('FNB of Democrats - FNB of Repuplicans', fontsize=14, fontweight = 'bold')
        # mplpl.xlabel('Absolute Perception Bias', fontsize=18, fontweight = 'bold')
        # mplpl.xlabel('False Positive Bias', fontsize=18, fontweight = 'bold')
        # mplpl.xlabel('False Negative Bias', fontsize=18, fontweight = 'bold')
        # mplpl.title(data_name,fontsize='xx-large')
        mplpl.legend(loc="lower right",prop=legend_properties,fontsize = 'medium',ncol=1)
        # mplpl.xlim([0, 1.5])
        # mplpl.ylim([0, 1])
        mplpl.xlim([-1, 1])
        mplpl.grid()
        mplpl.subplots_adjust(bottom=0.24)
        mplpl.subplots_adjust(left=0.18)
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_MPB_alldataset.pdf'
        pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_MPB_all_diff_cdf'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_APB_snopes_diff_cdf'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_FPB_snopes_diff_cdf'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_FNB_snopes_diff_cdf'


        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_APB_snopes_cdf'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_FPB_snopes_cdf'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/idealogical_FNB_snopes_cdf'
        mplpl.savefig(pp+'.pdf', format='pdf')
        mplpl.savefig(pp+'.png', format='png')












        #         mplpl.rcParams['figure.figsize'] = 5.4, 3.2
        #         mplpl.rc('xtick', labelsize='x-large')
        #         mplpl.rc('ytick', labelsize='x-large')
        #         mplpl.rc('legend', fontsize='medium')
        #         w_err_avg_dict
        #         # tweet_l_sort = sorted(w_norm_abs_err_avg_dict, key=w_norm_abs_err_avg_dict.get, reverse=False)
        #         tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
        #         acc_l = []
        #         for t_id in tweet_l_sort:
        #             acc_l.append(w_err_avg_dict[t_id])
        #
        #         num_bins = len(acc_l)
        #         counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         cdf = np.cumsum(counts)
        #         scale = 1.0 / cdf[-1]
        #         ncdf = scale * cdf
        #         mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)
        #
        # legend_properties = {'weight': 'bold'}
        #
        # mplpl.ylabel('CDF', fontsize=20, fontweight = 'bold')
        # mplpl.xlabel('Mean perception bias', fontsize=20, fontweight = 'bold')
        # mplpl.legend(loc="upper left", prop=legend_properties, fontsize='medium', ncol=1)
        # # mplpl.title(data_name)
        # # mplpl.legend(loc="upper left",fontsize = 'large')
        # mplpl.xlim([-1.5, 1.5])
        # mplpl.ylim([0, 1])
        # mplpl.grid()
        # mplpl.subplots_adjust(bottom=0.24)
        # mplpl.subplots_adjust(left=0.18)
        # # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/MPB_cdf_alldataset'
        # mplpl.savefig(pp + '.pdf', format='pdf')
        # mplpl.savefig(pp + '.png', format='png')
        #

        # elif fig_f_1==True:
            #     balance_f = 'un_balanced'
            #     # balance_f = 'balanced'
            #
            #     # fig_f = True
            #     # fig_f = False
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #
            #     fig_p = 7
            #
            #     for ind in ind_l:
            #
            #         df_m = df[ind].copy()
            #
            #         groupby_ftr = 'worker_id'
            #         grouped = df_m.groupby(groupby_ftr, sort=False)
            #         grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #         # df_tmp = df_m[df_m['tweet_id'] == t_id]
            #         w_cc=0
            #         for w_id in grouped.groups.keys():
            #             print(w_cc)
            #             w_cc+=1
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             df_tmp = df_m[df_m['worker_id'] == w_id]
            #             ind_t = df_tmp.index.tolist()[0]
            #             weights = []
            #             w_acc_list_tmp = list(df_tmp['acc'])
            #             w_acc_list = []
            #             for el in w_acc_list_tmp:
            #                 if el == 0:
            #                     w_acc_list.append(-1)
            #                 elif el == 1:
            #                     w_acc_list.append(1)
            #                 else:
            #                     w_acc_list.append(0)
            #             df_tt = pd.DataFrame({'val' : w_acc_list})
            #
            #             weights.append(np.ones_like(list(df_tt['val'])) / float(len(list(df_tt['val']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(w_acc_list), np.median(w_acc_list), np.var(w_acc_list)]
            #             # tweet_avg[t_id] = np.mean(w_acc_list)
            #             # tweet_med[t_id] = np.median(w_acc_list)
            #             # tweet_var[t_id] = np.var(w_acc_list)
            #             #
            #             # tweet_avg_l.append(np.mean(w_acc_list))
            #             # tweet_med_l.append(np.median(w_acc_list))
            #             # tweet_var_l.append(np.var(w_acc_list))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==1:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tt['val'].plot(kind='kde', lw=4, color='g', label='Accuracy')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tt['val']), weights=weights, color='g')
            #
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Accuracy')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(w_acc_list), 3))+', Var : '
            #                             + str(np.round(np.var(w_acc_list), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_acc_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_acc_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #
            #             weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(list(df_tmp['rel_v']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['rel_v'])), np.median(list(df_tmp['rel_v'])), np.var(list(df_tmp['rel_v']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['rel_v']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['rel_v']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['rel_v']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['rel_v'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['rel_v'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['rel_v'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==6:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['rel_v'].plot(kind='kde', lw=4, color='c', label='Perceive truth value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['rel_v']), weights=weights, color='c')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perceive truth value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['rel_v'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['rel_v'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pt_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pt_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['err'])) / float(len(list(df_tmp['err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['err'])), np.median(list(df_tmp['err'])),
            #             #                            np.var(list(df_tmp['err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #
            #             if fig_p==2:
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['err'].plot(kind='kde', lw=4, color='y', label='Perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_pb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_pb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['abs_err'])) / float(len(list(df_tmp['abs_err']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['abs_err'])), np.median(list(df_tmp['abs_err'])),
            #             #                            np.var(list(df_tmp['abs_err']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['abs_err']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['abs_err']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['abs_err']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['abs_err'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['abs_err'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['abs_err'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==3:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['abs_err'].plot(kind='kde', lw=4, color='y', label='Absolute perception bias value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['abs_err']), weights=weights, color='y')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Absolute perception bias value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['abs_err'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['abs_err'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_apb_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_apb_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['gull'])) / float(len(list(df_tmp['gull']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['gull'])), np.median(list(df_tmp['gull'])),
            #             #                            np.var(list(df_tmp['gull']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['gull']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['gull']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['gull']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['gull'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['gull'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['gull'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==4:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['gull'].plot(kind='kde', lw=4, color='k', label='Gullibility value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['gull']), weights=weights, color='k')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Gullibility value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['gull'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['gull'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_gull_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_gull_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #             weights = []
            #
            #             w_pt_list = []
            #
            #             weights.append(np.ones_like(list(df_tmp['cyn'])) / float(len(list(df_tmp['cyn']))))
            #
            #             # tweet_avg_med_var[t_id] = [np.mean(list(df_tmp['cyn'])), np.median(list(df_tmp['cyn'])),
            #             #                            np.var(list(df_tmp['cyn']))]
            #             # tweet_avg[t_id] = np.mean(list(df_tmp['cyn']))
            #             # tweet_med[t_id] = np.median(list(df_tmp['cyn']))
            #             # tweet_var[t_id] = np.var(list(df_tmp['cyn']))
            #             #
            #             # tweet_avg_l.append(np.mean(list(df_tmp['cyn'])))
            #             # tweet_med_l.append(np.median(list(df_tmp['cyn'])))
            #             # tweet_var_l.append(np.var(list(df_tmp['cyn'])))
            #             accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
            #             if fig_p==5:
            #
            #                 all_acc.append(accuracy)
            #                 try:
            #                     df_tmp['cyn'].plot(kind='kde', lw=4, color='m', label='Cynicality value')
            #                 except:
            #                     print('hmm')
            #
            #                 mplpl.hist(list(df_tmp['cyn']), weights=weights, color='m')
            #
            #                 mplpl.ylabel('Frequency')
            #                 mplpl.xlabel('Cynicality value')
            #                 mplpl.title(' Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : '
            #                             + str(np.round(np.mean(list(df_tmp['cyn'])), 3)) + ', Var : '
            #                             + str(np.round(np.var(list(df_tmp['cyn'])), 3)))
            #                 mplpl.legend(loc="upper right")
            #                 mplpl.xlim([-2, 2])
            #                 mplpl.ylim([0, 1])
            #                 if balance_f == 'balanced':
            #                     pp = remotedir + '/fig/fig_exp1/news_based/balanced/ind_users/' + str(w_id) + '_cyn_dist'
            #                 else:
            #                     pp = remotedir + '/fig/fig_exp1/user_based/ind_users/' + str(w_id) + '_cyn_dist'
            #                 mplpl.savefig(pp, format='png')
            #                 mplpl.figure()
            #
            #
            #
            #
            #
            #     exit()
            # else:
            #
            #     AVG_list = []
            #     print(np.mean(all_acc))
            #     outF = open(remotedir + 'output.txt', 'w')
            #
            #     tweet_all_var = {}
            #     tweet_all_dev_avg = {}
            #     tweet_all_avg = {}
            #     tweet_all_gt_var = {}
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #     tweet_all_avg_l = []
            #     tweet_all_med_l = []
            #     tweet_all_var_l = []
            #     tweet_all_gt_var_l = []
            #     diff_group_disp_l = []
            #     dem_disp_l = []
            #     rep_disp_l = []
            #
            #     tweet_all_dev_avg = {}
            #     tweet_all_dev_med = {}
            #     tweet_all_dev_var = {}
            #
            #     tweet_all_dev_avg_l = []
            #     tweet_all_dev_med_l = []
            #     tweet_all_dev_var_l = []
            #
            #     tweet_all_abs_dev_avg = {}
            #     tweet_all_abs_dev_med = {}
            #     tweet_all_abs_dev_var = {}
            #
            #     tweet_all_abs_dev_avg_l = []
            #     tweet_all_abs_dev_med_l = []
            #     tweet_all_abs_dev_var_l = []
            #     tweet_all_dev_avg_rnd = {}
            #     tweet_all_abs_dev_avg_rnd = {}
            #
            #     diff_group_disp_dict = {}
            #     if dataset == 'snopes':
            #         data_n = 'sp'
            #         news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'politifact':
            #         data_n = 'pf'
            #         news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            #         ind_l = [1, 2, 3]
            #     elif dataset == 'mia':
            #         data_n = 'mia'
            #         news_cat_list = ['rumor', 'non-rumor']
            #         ind_l = [1]
            #
            #     for cat_l in news_cat_list:
            #         outF.write('== ' + str(cat_l) + ' ==\n\n')
            #         print('== ' + str(cat_l) + ' ==')
            #         tweet_dev_avg = {}
            #         tweet_dev_med = {}
            #         tweet_dev_var = {}
            #         tweet_abs_dev_avg = {}
            #         tweet_abs_dev_med = {}
            #         tweet_abs_dev_var = {}
            #
            #         tweet_avg = {}
            #         tweet_med = {}
            #         tweet_var = {}
            #         tweet_gt_var = {}
            #
            #         tweet_dev_avg_rnd = {}
            #         tweet_abs_dev_avg_rnd = {}
            #
            #
            #         tweet_dev_avg_l = []
            #         tweet_dev_med_l = []
            #         tweet_dev_var_l = []
            #         tweet_abs_dev_avg_l = []
            #         tweet_abs_dev_med_l = []
            #         tweet_abs_dev_var_l = []
            #
            #         tweet_avg_l = []
            #         tweet_med_l = []
            #         tweet_var_l = []
            #         tweet_gt_var_l = []
            #         AVG_susc_list = []
            #         AVG_wl_list = []
            #         all_acc = []
            #         AVG_dev_list = []
            #         # for lean in [-1, 0, 1]:
            #
            #             # AVG_susc_list = []
            #             # AVG_wl_list = []
            #             # all_acc = []
            #             # df_m = df_m[df_m['leaning'] == lean]
            #             # if lean == 0:
            #             #     col = 'g'
            #             #     lean_cat = 'neutral'
            #             # elif lean == 1:
            #             #     col = 'b'
            #             #     lean_cat = 'democrat'
            #             # elif lean == -1:
            #             #     col = 'r'
            #             #     lean_cat = 'republican'
            #             # print(lean_cat)
            #         for ind in ind_l:
            #
            #             if balance_f == 'balanced':
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
            #             else:
            #                 inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'
            #
            #             inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
            #             df[ind] = pd.read_csv(inp1, sep="\t")
            #             df_w[ind] = pd.read_csv(inp1_w, sep="\t")
            #
            #             df_m = df[ind].copy()
            #             df_mm = df_m.copy()
            #
            #             df_m = df_m[df_m['ra_gt'] == cat_l]
            #             # df_mm = df_m[df_m['ra_gt']==cat_l]
            #             # df_m = df_m[df_m['leaning'] == lean]
            #
            #             groupby_ftr = 'tweet_id'
            #             grouped = df_m.groupby(groupby_ftr, sort=False)
            #             grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
            #
            #             for t_id in grouped.groups.keys():
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #
            #                 df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
            #                 df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
            #                 df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #                 df_tmp = df_m[df_m['tweet_id'] == t_id]
            #                 ind_t = df_tmp.index.tolist()[0]
            #                 weights = []
            #
            #                 weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
            #                 val_list = list(df_tmp['rel_v'])
            #                 tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_avg[t_id] = np.mean(val_list)
            #                 tweet_med[t_id] = np.median(val_list)
            #                 tweet_var[t_id] = np.var(val_list)
            #                 tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_avg_l.append(np.mean(val_list))
            #                 tweet_med_l.append(np.median(val_list))
            #                 tweet_var_l.append(np.var(val_list))
            #                 tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #
            #                 tweet_all_avg[t_id] = np.mean(val_list)
            #                 tweet_all_var[t_id] = np.var(val_list)
            #                 tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
            #
            #                 tweet_all_avg_l.append(np.mean(val_list))
            #                 tweet_all_med_l.append(np.median(val_list))
            #                 tweet_all_var_l.append(np.var(val_list))
            #                 tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
            #
            #
            #
            #                 val_list = list(df_tmp['err'])
            #                 abs_var_err = [np.abs(x) for x in val_list]
            #                 tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
            #                 tweet_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_dev_med[t_id] = np.median(val_list)
            #                 tweet_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_dev_avg_l.append(np.mean(val_list))
            #                 tweet_dev_med_l.append(np.median(val_list))
            #                 tweet_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #                 tweet_all_dev_avg[t_id] = np.mean(val_list)
            #                 tweet_all_dev_med[t_id] = np.median(val_list)
            #                 tweet_all_dev_var[t_id] = np.var(val_list)
            #
            #                 tweet_all_dev_avg_l.append(np.mean(val_list))
            #                 tweet_all_dev_med_l.append(np.median(val_list))
            #                 tweet_all_dev_var_l.append(np.var(val_list))
            #
            #                 tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
            #                 tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
            #                 tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)
            #
            #                 tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
            #                 tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
            #                 tweet_all_abs_dev_var_l.append(np.var(abs_var_err))
            #
            #
            #
            #                 sum_rnd_abs_perc = 0
            #                 sum_rnd_perc = 0
            #                 for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
            #                     sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
            #                     sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
            #                 random_perc = np.abs(sum_rnd_perc / float(7))
            #                 random_abs_perc = sum_rnd_abs_perc / float(7)
            #
            #                 tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #                 tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
            #                 tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
            #
            #         gt_l = []
            #         pt_l = []
            #         disputability_l = []
            #         perc_l = []
            #         abs_perc_l = []
            #         # for t_id in tweet_l_sort:
            #         #     gt_l.append(tweet_gt_var[t_id])
            #         #     pt_l.append(tweet_avg[t_id])
            #         #     disputability_l.append(tweet_var[t_id])
            #         #     perc_l.append(tweet_dev_avg[t_id])
            #         #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #         tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            #         # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            #
            #         # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
            #
            #
            #         if dataset == 'snopes':
            #             data_addr = 'snopes'
            #         elif dataset == 'politifact':
            #             data_addr = 'politifact/fig'
            #         elif dataset == 'mia':
            #             data_addr = 'mia/fig'
            #
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #
            #         for t_id in tweet_l_sort:
            #             count+=1
            #             if balance_f=='balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
            #                            + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
            #                            # str(np.round(diff_group_disp_dict[t_id], 3)) +
            #                            '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
            #                             + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
            #                             '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #
            #
            #
            #
            #     if dataset == 'snopes':
            #         data_addr = 'snopes'
            #     elif dataset == 'politifact':
            #         data_addr = 'politifact/fig'
            #     elif dataset == 'mia':
            #         data_addr = 'mia/fig'
            #
            #     # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
            #     tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
            #
            #     # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
            #     # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
            #
            #     tweet_napb_dict_high_disp = {}
            #     tweet_napb_dict_low_disp = {}
            #     for t_id in tweet_l_sort[:20]:
            #         # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     for t_id in tweet_l_sort[-20:]:
            #         # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
            #         tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]
            #
            #     kk = 0
            #
            #     for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
            #         if kk==0:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
            #         else:
            #             tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)
            #
            #         kk+=1
            #         count = 0
            #         outF.write(
            #             '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
            #         for t_id in tweet_l_sort:
            #             count += 1
            #             # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
            #             if balance_f == 'balanced':
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
            #                            + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
            #                            str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #                 # +
            #                 #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
            #                 #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')
            #
            #             else:
            #                 outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
            #                            str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
            #                            str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
            #                            str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
            #                            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
            #                            str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
            #             # +
            #             # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
            #             # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')


    if args.t == "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_acc_cdf_toghether":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                #     tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes_nonpol':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable

        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','snopes_nonpol','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                tweet_id = 100010
                publisher_name = 110
                tweet_popularity = {}
                tweet_text_dic = {}
                for input_file in [input_rumor, input_non_rumor]:
                    for line in input_file:
                        line.replace('\n', '')
                        line_splt = line.split('\t')
                        tweet_txt = line_splt[1]
                        tweet_link = line_splt[1]
                        tweet_id += 1
                        publisher_name += 1
                        tweet_popularity[tweet_id] = int(line_splt[2])
                        tweet_text_dic[tweet_id] = tweet_txt



                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                #     if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                #         tweet_lable_dict[tweet_id] = 'undecided'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes_nonpol':
                claims_list = []
                col = 'r'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    # print(cat_lable)
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                col = 'r'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'politifact':
                col = 'g'

                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                col = 'purple'

                data_name = 'Snopes'
            elif dataset=='snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                col = 'green'

                data_name = 'Snopes_nonpol'
            elif dataset=='politifact':
                col = 'c'
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                col = 'orange'
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            if dataset=='snopes' or dataset=='snopes_nonpol':
                news_cat_list_t_f = [['FALSE', 'MOSTLY FALSE'],['MOSTLY TRUE', 'TRUE']]
            if dataset=='politifact':
                news_cat_list_t_f = [['pants-fire', 'false', 'mostly-false'],['mostly-true', 'true']]

            if dataset=='mia':
                news_cat_list_t_f = [['rumor'], ['non-rumor']]



            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()
                df[ind].loc[:, 'abs_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_abs_err'] = df[ind]['tweet_id'] * 0.0

                groupby_ftr = 'tweet_id'
                grouped = df[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df[ind].groupby(groupby_ftr, sort=False).sum()


                for ind_t in df[ind].index.tolist():
                    t_id = df[ind]['tweet_id'][ind_t]
                    err = df[ind]['err'][ind_t]
                    abs_err = np.abs(err)
                    df[ind]['abs_err'][ind_t] = abs_err
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df[ind]['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df[ind]['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)


                    norm_err = err / float(random_perc)
                    norm_abs_err = abs_err / float(random_abs_perc)
                    df[ind]['norm_err'][ind_t] = norm_err
                    df[ind]['norm_abs_err'][ind_t] = norm_abs_err

                # df[ind] = df[ind].copy()

            w_pt_avg_l = []
            w_err_avg_l = []
            w_abs_err_avg_l = []
            w_norm_err_avg_l = []
            w_norm_abs_err_avg_l = []
            w_acc_avg_l = []

            w_pt_std_l = []
            w_err_std_l = []
            w_abs_err_std_l = []
            w_norm_err_std_l = []
            w_norm_abs_err_std_l = []
            w_acc_std_l = []

            w_pt_avg_dict = collections.defaultdict()
            w_err_avg_dict = collections.defaultdict()
            w_abs_err_avg_dict = collections.defaultdict()
            w_norm_err_avg_dict = collections.defaultdict()
            w_norm_abs_err_avg_dict = collections.defaultdict()
            w_acc_avg_dict = collections.defaultdict()

            w_pt_std_dict = collections.defaultdict()
            w_err_std_dict = collections.defaultdict()
            w_abs_err_std_dict = collections.defaultdict()
            w_norm_err_std_dict = collections.defaultdict()
            w_norm_abs_err_std_dict = collections.defaultdict()
            w_acc_std_dict = collections.defaultdict()

            all_w_pt_list  = []
            all_w_err_list = []
            all_w_abs_err_list = []
            all_w_norm_err_list = []
            all_w_norm_abs_err_list  = []
            all_w_acc_list = []

            all_w_cyn_list = []
            all_w_gull_list = []
            w_cyn_avg_l = []
            w_gull_avg_l = []
            w_cyn_std_l= []
            w_gull_std_l = []
            w_cyn_avg_dict =collections.defaultdict()
            w_gull_avg_dict =collections.defaultdict()
            w_cyn_std_dict =collections.defaultdict()
            w_gull_std_dict = collections.defaultdict()

            acc_TT = 0
            acc_FT = 0
            acc_TF = 0
            acc_FF = 0

            for ind in ind_l:

                df_m = df[ind].copy()
                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    t_count = 0; f_count=0
                    for index_tmp in df_tmp.index.tolist():
                        if df_tmp['acc'][index_tmp]==1:
                            t_count+=1
                        elif df_tmp['acc'][index_tmp]==0:
                            f_count+=1

                    if t_count>f_count:
                        if tweet_lable_dict[t_id] in news_cat_list_t_f[0]:
                            acc_FT+=1
                        elif tweet_lable_dict[t_id] in news_cat_list_t_f[1]:
                            acc_TT+=1

                    elif t_count<f_count:
                        if tweet_lable_dict[t_id] in news_cat_list_t_f[0]:
                            acc_FF+=1
                        elif tweet_lable_dict[t_id] in news_cat_list_t_f[1]:
                            acc_TF+=1



            print('dataset : ' + dataset)
            print('True and preceived correctly' + str(acc_TT))
            print('False and preceived correctly' + str(acc_FT))
            print('True but preceived wrongly' + str(acc_TF))
            print('False but preceived wrongly' + str(acc_FF))

            for ind in ind_l:

                df_m = df[ind].copy()
                groupby_ftr = 'worker_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                for w_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['worker_id'] == w_id]


                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    w_abs_err_list = list(df_tmp['abs_err'])
                    w_norm_err_list = list(df_tmp['norm_err'])
                    w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    w_cyn_list = list(df_tmp['cyn'])
                    w_gull_list = list(df_tmp['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])
                    w_acc_list = []
                    w_acc_list_t = []
                    w_acc_list_f = []
                    # w_ind_acc_list
                    acc_c = 0
                    nacc_c = 0



                    for el in w_acc_list_tmp:
                        if el == 0:
                            w_acc_list.append(-1)
                            nacc_c += 1
                        elif el ==1:
                            w_acc_list.append(1)
                            acc_c+=1
                        else:
                            w_acc_list.append(0)


                    w_ind_acc_list = (acc_c / float(acc_c + nacc_c))



                    all_w_pt_list += list(df_tmp['rel_v'])
                    all_w_err_list += list(df_tmp['err'])
                    all_w_abs_err_list += list(df_tmp['abs_err'])
                    all_w_norm_err_list += list(df_tmp['norm_err'])
                    all_w_norm_abs_err_list += list(df_tmp['norm_abs_err'])
                    all_w_cyn_list += list(df_tmp['cyn'])
                    all_w_gull_list += list(df_tmp['gull'])
                    all_w_acc_list += list(w_acc_list)


                    w_pt_avg_l.append(np.mean(w_pt_list))
                    w_err_avg_l.append(np.mean(w_err_list))
                    w_abs_err_avg_l.append(np.mean(w_abs_err_list))
                    w_norm_err_avg_l.append(np.mean(w_norm_err_list))
                    w_norm_abs_err_avg_l.append(np.mean(w_norm_abs_err_list))
                    w_cyn_avg_l.append(np.mean(w_cyn_list))
                    w_gull_avg_l.append(np.mean(w_gull_list))
                    w_acc_avg_l.append(w_ind_acc_list)

                    w_pt_std_l.append(np.std(w_pt_list))
                    w_err_std_l.append(np.std(w_err_list))
                    w_abs_err_std_l.append(np.std(w_abs_err_list))
                    w_norm_err_std_l.append(np.std(w_norm_err_list))
                    w_norm_abs_err_std_l.append(np.std(w_norm_abs_err_list))
                    w_cyn_std_l.append(np.std(w_cyn_list))
                    w_gull_std_l.append(np.std(w_gull_list))
                    # w_acc_std_l.append(np.std(w_ind_acc_list))


                    w_pt_avg_dict[w_id] = np.mean(w_pt_list)
                    w_err_avg_dict[w_id] = np.mean(w_err_list)
                    w_abs_err_avg_dict[w_id] = np.mean(w_abs_err_list)
                    w_norm_err_avg_dict[w_id] = np.mean(w_norm_err_list)
                    w_norm_abs_err_avg_dict[w_id] = np.mean(w_norm_abs_err_list)
                    w_cyn_avg_dict[w_id] = np.mean(w_cyn_list)
                    w_gull_avg_dict[w_id] = np.mean(w_gull_list)
                    w_acc_avg_dict[w_id] = w_ind_acc_list

                    w_pt_std_dict[w_id] = np.std(w_pt_list)
                    w_err_std_dict[w_id] = np.std(w_err_list)
                    w_abs_err_std_dict[w_id] = np.std(w_abs_err_list)
                    w_norm_err_std_dict[w_id] = np.std(w_norm_err_list)
                    w_norm_abs_err_std_dict[w_id] = np.std(w_norm_abs_err_list)
                    w_cyn_std_dict[w_id] = np.std(w_cyn_list)
                    w_gull_std_dict[w_id] = np.std(w_gull_list)
                    w_acc_std_dict[w_id] = np.std(w_acc_list)
                # ind_
            ##################################################
            #
            # tweet_l_sort = sorted(tweet_gt_var, key=tweet_gt_var.get, reverse=True)
            # gt_l = []
            # pt_l = []
            # disputability_l = []
            # perc_l = []
            # abs_perc_l=[]
            # abs_perc_rnd_l = []
            # perc_rnd_l = []
            # tweet_skew_ll = []
            # for t_id in tweet_l_sort:
            #     gt_l.append(tweet_gt_var[t_id])
            #     pt_l.append(tweet_avg[t_id])
            #     disputability_l.append(tweet_var[t_id])
            #     perc_l.append(tweet_dev_avg[t_id])
            #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
            #
            #     perc_rnd_l.append(tweet_dev_avg_rnd[t_id])
            #     abs_perc_rnd_l.append(tweet_abs_dev_avg_rnd[t_id])
            #     tweet_skew_ll.append(tweet_skew[t_id])
            # value_list = [gt_l, pt_l, disputability_l, perc_l, abs_perc_l,perc_rnd_l,abs_perc_rnd_l,tweet_skew_ll]
            # value_name = ['ground truth value', 'perceived truth value', 'disputability', 'perception bias',
            #               'absolute perception bias','perception bias rnd', 'absolute perception bias rnd', 'skewness']
            #
            # outF.write('|| ')
            # for v_name in value_name:
            #     outF.write('||' + v_name)
            # outF.write('||\n')
            #
            # for f_list in range(8):
            #     outF.write('|| ' + value_name[f_list] + '||')
            #     for s_list in range(8):
            #         m_corr = np.round(np.corrcoef(value_list[f_list], value_list[s_list])[1][0],3)
            #         outF.write(str(m_corr) + '||')
            #     outF.write('\n')
            #
            #
            # exit()

            # fig_f = True
            fig_f = False
            # fig_f_1 = True
            fig_f_1 = False
            fig_f_together = True
            if fig_f==True:

                df_tmp = pd.DataFrame({'val' : all_w_acc_list})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), weights=weights, color='g')
                # mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 1.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Readers accuracy', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_acc_list),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_acc_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_acc_density| alt text| width = 500px}}')



                # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
                df_tmp = pd.DataFrame({'val' : w_acc_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                # mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers accuracy', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_acc_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_acc_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_acc_pt| alt text| width = 500px}}')




                df_tmp = pd.DataFrame({'val' : all_w_pt_list})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), weights=weights, color='c')
                # mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                mplpl.xlim([-1.5, 1.5])
                mplpl.ylim([0, 1.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Readers percevied truth value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_pt_list),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_pt_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_pt_density| alt text| width = 500px}}')




                # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
                df_tmp = pd.DataFrame({'val' : w_pt_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                mplpl.hist(list(df_tmp['val']), normed=1, color='c')


                # mplpl.plot(gt_set, pt_mean,  color='k')
                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 3.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers percevied truth value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_pt_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_pt_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_pt_density| alt text| width = 500px}}')




                df_tmp = pd.DataFrame({'val' : w_err_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='y')


                mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 3.5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers perception bias value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_err_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_pb_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_pb_density| alt text| width = 500px}}')



                df_tmp = pd.DataFrame({'val' : w_abs_err_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='y')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 5])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers absolute perception bias value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_abs_err_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_apb_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_apb_density| alt text| width = 500px}}||')



                df_tmp = pd.DataFrame({'val' : w_gull_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='m')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 10])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers gullibility value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_gull_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_gull_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_gull_density| alt text| width = 500px}}||')




                df_tmp = pd.DataFrame({'val' : w_cyn_avg_l})
                weights = []
                weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                col = 'r'
                try:
                    df_tmp['val'].plot(kind='kde', lw=4, color=col)
                except:
                    print('hmm')

                mplpl.hist(list(df_tmp['val']), normed=1, color='k')

                mplpl.xlim([0, 1.2])
                mplpl.ylim([0, 10])
                mplpl.ylabel('Density', fontsize=18)
                mplpl.xlabel('Individual readers cynicallity value', fontsize=18)
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_cyn_avg_l),4)))
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_avg_cyn_density'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_avg_cyn_density| alt text| width = 500px}}||\n\n')



        ########################

                tweet_l_sort = sorted(w_pt_avg_dict, key=w_pt_avg_dict.get, reverse=False)
                pt_l = []
                for t_id in tweet_l_sort:
                    pt_l.append(w_pt_avg_dict[t_id])

                mplpl.scatter(range(len(pt_l)), pt_l,  s=40,color='c',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Perception truth value (PTL)', fontsize=18)
                mplpl.xlabel('Ranked readers according PTL', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(pt_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_pt_pt'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_pt_pt| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_acc_avg_dict, key=w_acc_avg_dict.get, reverse=False)
                acc_l = []
                for t_id in tweet_l_sort:
                    acc_l.append(w_acc_avg_dict[t_id])


                mplpl.scatter(range(len(acc_l)), acc_l ,  s=40,color='g',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-1, 1])
                mplpl.ylabel('Accuracy', fontsize=18)
                mplpl.xlabel('Ranked readers according accuracy', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(acc_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_acc_acc'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_acc_acc| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
                err_l = []
                for t_id in tweet_l_sort:
                    err_l.append(w_err_avg_dict[t_id])

                mplpl.scatter(range(len(err_l)), err_l,  s=40,color='y',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([-2, 2])
                mplpl.ylabel('Perception bias (PB)', fontsize=18)
                mplpl.xlabel('Ranked readers according PB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(err_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_err_err'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_err_err| alt text| width = 500px}}')



                tweet_l_sort = sorted(w_abs_err_avg_dict, key=w_abs_err_avg_dict.get, reverse=False)
                abs_err_l = []
                for t_id in tweet_l_sort:
                    abs_err_l.append(w_abs_err_avg_dict[t_id])

                mplpl.scatter(range(len(abs_err_l)), abs_err_l,  s=40,color='y',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 2])
                mplpl.ylabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.xlabel('Ranked news stories according APB', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_err_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_abs-err_abs-err'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_abs-err_abs-err| alt text| width = 500px}}')


                tweet_l_sort = sorted(w_gull_avg_dict, key=w_gull_avg_dict.get, reverse=False)
                gull_l = []
                for t_id in tweet_l_sort:
                    gull_l.append(w_gull_avg_dict[t_id])

                mplpl.scatter(range(len(gull_l)), gull_l,  s=40,color='m',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Gullibility', fontsize=18)
                mplpl.xlabel('Ranked readers according gullibility', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(gull_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_gull_gull'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_gull_gull| alt text| width = 500px}}')



                tweet_l_sort = sorted(w_cyn_avg_dict, key=w_cyn_avg_dict.get, reverse=False)
                cyn_l = []
                for t_id in tweet_l_sort:
                    cyn_l.append(w_cyn_avg_dict[t_id])

                mplpl.scatter(range(len(cyn_l)), cyn_l,  s=40,color='k',marker='o', label='All users')
                # mplpl.xlim([-1.2, 1.2])
                mplpl.ylim([0, 1])
                mplpl.ylabel('Cynicality', fontsize=18)
                mplpl.xlabel('Ranked readers according cynicality', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(cyn_l),4)))
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_cyn_cyn'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_cyn_cyn| alt text| width = 500px}}||\n\n')



                # outF.write('|| Table ||\n\n')

                # mplpl.show()
                # exit()
        #####################################################33


                num_bins = len(pt_l)
                counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_pt_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_pt_cdf| alt text| width = 500px}}')


                num_bins = len(acc_l)
                counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Accuracy', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_acc_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_acc_cdf| alt text| width = 500px}}')


                num_bins = len(err_l)
                counts, bin_edges = np.histogram(err_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Perception bias (PB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([-1, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_err_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_err_cdf| alt text| width = 500px}}')

                num_bins = len(abs_err_l)
                counts, bin_edges = np.histogram(abs_err_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='All users')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                mplpl.grid()
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_abs-err_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()
                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_abs-err_cdf| alt text| width = 500px}}')

                # outF.write('|| Table ||\n\n')

                num_bins = len(gull_l)
                counts, bin_edges = np.histogram(gull_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='m', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Gullibility', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_gull_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_gull_cdf| alt text| width = 500px}}')

                num_bins = len(cyn_l)
                counts, bin_edges = np.histogram(cyn_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='k', lw=5, label='')
                mplpl.ylabel('CDF', fontsize=18)
                mplpl.xlabel('Cynicality', fontsize=18)
                mplpl.grid()
                mplpl.title(data_name)
                #         mplpl.legend(loc="upper right")
                mplpl.xlim([0, 1])
                mplpl.ylim([0, 1])
                pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_cyn_cdf'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_cyn_cdf| alt text| width = 500px}}||\n')


                # mplpl.show()
                # exit()
                # col_l = ['r', 'b', 'g']
                #
                # i = 0
                # for data_s in gt_l_dict.keys():
                #
                #     mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=400, color=col_l[i], marker='*')
                #     mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([-2, 2])
                #     mplpl.ylabel('Perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i+=1
                #
                # mplpl.legend(loc="upper right")
                #
                # mplpl.grid()
                # # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception_scatter'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                # i = 0
                # mark_l = ['*', 'o', '^']
                # for data_s in gt_l_dict.keys():
                #
                #     # mplpl.scatter(gt_l_dict[data_s], perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([-2, 2])
                #     mplpl.ylabel('Perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i+=1
                #
                # mplpl.legend(loc="upper right")
                #
                # mplpl.grid()
                # # mplpl.title('avg : ' + str(np.round(np.mean(perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data' + '/all_dataset_gt_perception'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                # i=0
                # for data_s in gt_l_dict.keys():
                #
                #     mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o')
                #     mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=300, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([0, 2])
                #     mplpl.ylabel('Absolute perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i += 1
                #
                # mplpl.grid()
                # mplpl.legend(loc="upper right")
                #
                # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception_scatter'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                # i=0
                # for data_s in gt_l_dict.keys():
                #
                #     # mplpl.scatter(gt_l_dict[data_s], abs_perc_l_dict[data_s], s=40, color=col_l[i], marker='o', label=data_s)
                #     mplpl.scatter(gt_set_dict[data_s], abs_perc_mean_dict[data_s], s=40, color=col_l[i], marker=mark_l[i], label=data_s)
                #     mplpl.plot(gt_set_dict[data_s], abs_perc_mean_dict[data_s], color=col_l[i])
                #     mplpl.xlim([-1.2, 1.2])
                #     mplpl.ylim([0, 2])
                #     mplpl.ylabel('Absolute perception bias', fontsize=18)
                #     mplpl.xlabel('Ground truth value (GTL)', fontsize=18)
                #     i += 1
                #
                # mplpl.grid()
                # mplpl.legend(loc="upper right")
                #
                # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(abs_perc_l), 4)))
                # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data'  + '/all_dataset_gt_abs_perception'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #
                #





            elif fig_f_together==True:
                ###accuracy


                mplpl.rcParams['figure.figsize'] = 5.4, 3
                mplpl.rc('xtick', labelsize='x-large')
                mplpl.rc('ytick', labelsize='x-large')
                mplpl.rc('legend', fontsize='medium')
                tweet_l_sort = sorted(w_acc_avg_dict, key=w_acc_avg_dict.get, reverse=False)
                acc_l = []
                for t_id in tweet_l_sort:
                    acc_l.append(w_acc_avg_dict[t_id])

                num_bins = len(acc_l)
                counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c=col, lw=5, label=data_name)




        legend_properties = {'weight': 'bold'}
        mplpl.ylabel('CDF', fontsize=22, fontweight = 'bold')
        mplpl.xlabel('Users\' accuracy', fontsize=22, fontweight = 'bold')
        # mplpl.title(data_name)
        mplpl.legend(loc="upper left",prop=legend_properties,fontsize = 'medium',ncol=1)
        mplpl.xlim([0, 1])
        mplpl.ylim([0, 1])
        mplpl.grid()
        mplpl.subplots_adjust(bottom=0.24)
        mplpl.subplots_adjust(left=0.18)
        pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/user_based/user_acc_cdf_alldataset_new'
        mplpl.savefig(pp + '.pdf', format='pdf')
        mplpl.savefig(pp + '.png', format='png')




    if args.t == "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_visualisation_initial_stastistics_demographics":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'


        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['mia']:#,'mia','snopes','politifact']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                    if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                        tweet_lable_dict[tweet_id] = 'undecided'
                outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'

            # 'nationality': Series(w_nationality_l, index=index_list_m),
            # 'residence': Series(w_residence_l, index=index_list_m),
            # 'gender': Series(w_gender_l, index=index_list_m),
            # 'age': Series(w_age_l, index=index_list_m),
            # 'degree': Series(w_degree_l, index=index_list_m),
            # 'employment': Series(w_employment_l, index=index_list_m),
            # 'income': Series(w_income_l, index=index_list_m),
            # 'political_view': Series(w_political_view_l, index=index_list_m),
            # 'race': Series(w_race_l, index=index_list_m),
            # 'marital_status': Series(w_marital_status_l, index=index_list_m),})


            inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(1) + '_final.csv'
            df_test = pd.read_csv(inp1, sep="\t")

            for feat in ['gender','leaning']:
                feat_cat_l = set(df_test[feat])
                if feat == 'leaning':
                    feat_cat_l = [-1,0,1]
                if feat == 'gender':
                    feat_cat_l = ['male','female']


                for feat_cat in feat_cat_l:
                    if feat == 'leaning':
                        if feat_cat==0:
                            col = 'g'
                        elif feat_cat==-1:
                            col='r'
                        elif feat_cat==1:
                            col='b'

                    if feat == 'gender':
                        if feat_cat == 'male':
                            col = 'c'
                        elif feat_cat == 'female':
                            col = 'y'

                    df = collections.defaultdict()
                    df_w = collections.defaultdict()
                    tweet_avg_med_var = collections.defaultdict(list)
                    tweet_dev_avg_med_var = collections.defaultdict(list)
                    tweet_dev_avg = {}
                    tweet_dev_med = {}
                    tweet_dev_var = {}
                    tweet_avg = {}
                    tweet_med = {}
                    tweet_var = {}
                    tweet_gt_var = {}

                    tweet_dev_avg_l = []
                    tweet_dev_med_l = []
                    tweet_dev_var_l = []
                    tweet_avg_l = []
                    tweet_med_l = []
                    tweet_var_l = []
                    tweet_gt_var_l = []
                    avg_susc = 0
                    avg_gull = 0
                    avg_cyn = 0

                    tweet_abs_dev_avg = {}
                    tweet_abs_dev_med = {}
                    tweet_abs_dev_var = {}

                    tweet_abs_dev_avg_l = []
                    tweet_abs_dev_med_l = []
                    tweet_abs_dev_var_l = []

                    tweet_abs_dev_avg_rnd = {}
                    tweet_dev_avg_rnd = {}

                    tweet_skew = {}
                    tweet_skew_l = []

                    for ind in ind_l:
                        if balance_f == 'balanced':
                            inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                        else:
                            inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                        inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                        df[ind] = pd.read_csv(inp1, sep="\t")
                        df_w[ind] = pd.read_csv(inp1_w, sep="\t")
                        df[ind] = df[ind][df[ind][feat]==feat_cat]
                        df_m = df[ind].copy()
                        df[ind].loc[:, 'abs_err'] = df[ind]['tweet_id'] * 0.0
                        df[ind].loc[:, 'norm_err'] = df[ind]['tweet_id'] * 0.0
                        df[ind].loc[:, 'norm_abs_err'] = df[ind]['tweet_id'] * 0.0

                        groupby_ftr = 'tweet_id'
                        grouped = df[ind].groupby(groupby_ftr, sort=False)
                        grouped_sum = df[ind].groupby(groupby_ftr, sort=False).sum()


                        for ind_t in df[ind].index.tolist():
                            t_id = df[ind]['tweet_id'][ind_t]
                            err = df[ind]['err'][ind_t]
                            abs_err = np.abs(err)
                            df[ind]['abs_err'][ind_t] = abs_err
                            sum_rnd_abs_perc = 0
                            sum_rnd_perc = 0
                            for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                                sum_rnd_perc+= val - df[ind]['rel_gt_v'][ind_t]
                                sum_rnd_abs_perc += np.abs(val - df[ind]['rel_gt_v'][ind_t])
                            random_perc = np.abs(sum_rnd_perc / float(7))
                            random_abs_perc = sum_rnd_abs_perc / float(7)


                            norm_err = err / float(random_perc)
                            norm_abs_err = abs_err / float(random_abs_perc)
                            df[ind]['norm_err'][ind_t] = norm_err
                            df[ind]['norm_abs_err'][ind_t] = norm_abs_err

                        # df[ind] = df[ind].copy()



                    w_pt_avg_l = []
                    w_err_avg_l = []
                    w_abs_err_avg_l = []
                    w_norm_err_avg_l = []
                    w_norm_abs_err_avg_l = []
                    w_acc_avg_l = []

                    w_pt_std_l = []
                    w_err_std_l = []
                    w_abs_err_std_l = []
                    w_norm_err_std_l = []
                    w_norm_abs_err_std_l = []
                    w_acc_std_l = []

                    w_pt_avg_dict = collections.defaultdict()
                    w_err_avg_dict = collections.defaultdict()
                    w_abs_err_avg_dict = collections.defaultdict()
                    w_norm_err_avg_dict = collections.defaultdict()
                    w_norm_abs_err_avg_dict = collections.defaultdict()
                    w_acc_avg_dict = collections.defaultdict()

                    w_pt_std_dict = collections.defaultdict()
                    w_err_std_dict = collections.defaultdict()
                    w_abs_err_std_dict = collections.defaultdict()
                    w_norm_err_std_dict = collections.defaultdict()
                    w_norm_abs_err_std_dict = collections.defaultdict()
                    w_acc_std_dict = collections.defaultdict()

                    all_w_pt_list  = []
                    all_w_err_list = []
                    all_w_abs_err_list = []
                    all_w_norm_err_list = []
                    all_w_norm_abs_err_list  = []
                    all_w_acc_list = []

                    all_w_cyn_list = []
                    all_w_gull_list = []
                    w_cyn_avg_l = []
                    w_gull_avg_l = []
                    w_cyn_std_l= []
                    w_gull_std_l = []
                    w_cyn_avg_dict =collections.defaultdict()
                    w_gull_avg_dict =collections.defaultdict()
                    w_cyn_std_dict =collections.defaultdict()
                    w_gull_std_dict = collections.defaultdict()
                    for ind in ind_l:

                        df_m = df[ind].copy()
                        groupby_ftr = 'worker_id'
                        grouped = df_m.groupby(groupby_ftr, sort=False)
                        grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                        for w_id in grouped.groups.keys():
                            df_tmp = df_m[df_m['worker_id'] == w_id]


                            w_pt_list = list(df_tmp['rel_v'])
                            w_err_list = list(df_tmp['err'])
                            w_abs_err_list = list(df_tmp['abs_err'])
                            w_norm_err_list = list(df_tmp['norm_err'])
                            w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                            w_cyn_list = list(df_tmp['cyn'])
                            w_gull_list = list(df_tmp['gull'])
                            w_acc_list_tmp = list(df_tmp['acc'])
                            w_acc_list = []
                            # w_ind_acc_list
                            acc_c = 0
                            nacc_c = 0
                            for el in w_acc_list_tmp:
                                if el == 0:
                                    w_acc_list.append(-1)
                                    nacc_c += 1
                                elif el == 1:
                                    w_acc_list.append(1)
                                    acc_c += 1
                                else:
                                    w_acc_list.append(0)

                            w_ind_acc_list = (acc_c / float(acc_c + nacc_c))


                            all_w_pt_list += list(df_tmp['rel_v'])
                            all_w_err_list += list(df_tmp['err'])
                            all_w_abs_err_list += list(df_tmp['abs_err'])
                            all_w_norm_err_list += list(df_tmp['norm_err'])
                            all_w_norm_abs_err_list += list(df_tmp['norm_abs_err'])
                            all_w_cyn_list += list(df_tmp['cyn'])
                            all_w_gull_list += list(df_tmp['gull'])
                            all_w_acc_list += list(w_acc_list)



                            w_pt_avg_l.append(np.mean(w_pt_list))
                            w_err_avg_l.append(np.mean(w_err_list))
                            w_abs_err_avg_l.append(np.mean(w_abs_err_list))
                            w_norm_err_avg_l.append(np.mean(w_norm_err_list))
                            w_norm_abs_err_avg_l.append(np.mean(w_norm_abs_err_list))
                            w_cyn_avg_l.append(np.mean(w_cyn_list))
                            w_gull_avg_l.append(np.mean(w_gull_list))
                            w_acc_avg_l.append(w_ind_acc_list)

                            w_pt_std_l.append(np.std(w_pt_list))
                            w_err_std_l.append(np.std(w_err_list))
                            w_abs_err_std_l.append(np.std(w_abs_err_list))
                            w_norm_err_std_l.append(np.std(w_norm_err_list))
                            w_norm_abs_err_std_l.append(np.std(w_norm_abs_err_list))
                            w_cyn_std_l.append(np.std(w_cyn_list))
                            w_gull_std_l.append(np.std(w_gull_list))
                            # w_acc_std_l.append(np.std(w_ind_acc_list))


                            w_pt_avg_dict[w_id] = np.mean(w_pt_list)
                            w_err_avg_dict[w_id] = np.mean(w_err_list)
                            w_abs_err_avg_dict[w_id] = np.mean(w_abs_err_list)
                            w_norm_err_avg_dict[w_id] = np.mean(w_norm_err_list)
                            w_norm_abs_err_avg_dict[w_id] = np.mean(w_norm_abs_err_list)
                            w_cyn_avg_dict[w_id] = np.mean(w_cyn_list)
                            w_gull_avg_dict[w_id] = np.mean(w_gull_list)
                            w_acc_avg_dict[w_id] = w_ind_acc_list

                            w_pt_std_dict[w_id] = np.std(w_pt_list)
                            w_err_std_dict[w_id] = np.std(w_err_list)
                            w_abs_err_std_dict[w_id] = np.std(w_abs_err_list)
                            w_norm_err_std_dict[w_id] = np.std(w_norm_err_list)
                            w_norm_abs_err_std_dict[w_id] = np.std(w_norm_abs_err_list)
                            w_cyn_std_dict[w_id] = np.std(w_cyn_list)
                            w_gull_std_dict[w_id] = np.std(w_gull_list)
                            # w_acc_std_dict[w_id] = np.std(w_acc_list)

                    ##################################################



                    fig_f = True
                    # fig_f = False
                    fig_f_1 = True
                    # fig_f_1 = False
                    if fig_f==True:

                        df_tmp = pd.DataFrame({'val' : all_w_acc_list})
                        weights = []
                        weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                        col_r = 'm'
                        try:
                            df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
                        except:
                            print('hmm')

                        mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                        # mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                        mplpl.xlim([-1.5, 1.5])
                        mplpl.ylim([0, 1.5])
                        mplpl.ylabel('Density', fontsize=18)
                        mplpl.xlabel('Readers accuracy', fontsize=18)
                        mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_acc_list),4)))
                        mplpl.grid()
                        pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) +'_acc_density'
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()

                        outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_acc_density| alt text| width = 500px}}')



                        # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
                        df_tmp = pd.DataFrame({'val' : w_acc_avg_l})
                        weights = []
                        weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                        col_r = 'm'
                        try:
                            df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
                        except:
                            print('hmm')

                        # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                        mplpl.hist(list(df_tmp['val']), normed=1, color=col)


                        # mplpl.plot(gt_set, pt_mean,  color='k')
                        mplpl.xlim([-1.5, 1.5])
                        mplpl.ylim([0, 5])
                        mplpl.ylabel('Density', fontsize=18)
                        mplpl.xlabel('Individual readers accuracy', fontsize=18)
                        mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_acc_avg_l),4)))
                        mplpl.grid()
                        pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_acc_pt'
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()

                        outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_acc_pt| alt text| width = 500px}}')




                        df_tmp = pd.DataFrame({'val' : all_w_pt_list})
                        weights = []
                        weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                        col_r = 'm'
                        try:
                            df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
                        except:
                            print('hmm')

                        mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                        # mplpl.hist(list(df_tmp['val']), normed=1, color='g')


                        mplpl.xlim([-1.5, 1.5])
                        mplpl.ylim([0, 1.5])
                        mplpl.ylabel('Density', fontsize=18)
                        mplpl.xlabel('Readers percevied truth value', fontsize=18)
                        mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_pt_list),4)))
                        mplpl.grid()
                        pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) +'_pt_density'
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()

                        outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_pt_density| alt text| width = 500px}}')




                        # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
                        df_tmp = pd.DataFrame({'val' : w_pt_avg_l})
                        weights = []
                        weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                        col_r = 'm'
                        try:
                            df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
                        except:
                            print('hmm')

                        # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
                        mplpl.hist(list(df_tmp['val']), normed=1, color=col)


                        # mplpl.plot(gt_set, pt_mean,  color='k')
                        mplpl.xlim([-1.2, 1.2])
                        mplpl.ylim([0, 3.5])
                        mplpl.ylabel('Density', fontsize=18)
                        mplpl.xlabel('Individual readers percevied truth value', fontsize=18)
                        mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_pt_avg_l),4)))
                        mplpl.grid()
                        pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_pt_density'
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()

                        outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_pt_density| alt text| width = 500px}}')




                        df_tmp = pd.DataFrame({'val' : w_err_avg_l})
                        weights = []
                        weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                        col_r = 'm'
                        try:
                            df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
                        except:
                            print('hmm')

                        mplpl.hist(list(df_tmp['val']), normed=1, color=col)


                        mplpl.xlim([-1.2, 1.2])
                        mplpl.ylim([0, 3.5])
                        mplpl.ylabel('Density', fontsize=18)
                        mplpl.xlabel('Individual readers perception bias value', fontsize=18)
                        mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_err_avg_l),4)))
                        mplpl.grid()
                        pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_pb_density'
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()

                        outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_pb_density| alt text| width = 500px}}')



                        df_tmp = pd.DataFrame({'val' : w_abs_err_avg_l})
                        weights = []
                        weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                        col_r = 'm'
                        try:
                            df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
                        except:
                            print('hmm')

                        mplpl.hist(list(df_tmp['val']), normed=1, color=col)

                        mplpl.xlim([0, 1.2])
                        mplpl.ylim([0, 5])
                        mplpl.ylabel('Density', fontsize=18)
                        mplpl.xlabel('Individual readers absolute perception bias value', fontsize=18)
                        mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_abs_err_avg_l),4)))
                        mplpl.grid()
                        pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_apb_density'
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()
                        outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_apb_density| alt text| width = 500px}}||')



                        df_tmp = pd.DataFrame({'val' : w_gull_avg_l})
                        weights = []
                        weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                        col_r = 'm'
                        try:
                            df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
                        except:
                            print('hmm')

                        mplpl.hist(list(df_tmp['val']), normed=1, color=col)

                        mplpl.xlim([0, 1.2])
                        mplpl.ylim([0, 10])
                        mplpl.ylabel('Density', fontsize=18)
                        mplpl.xlabel('Individual readers gullibility value', fontsize=18)
                        mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_gull_avg_l),4)))
                        mplpl.grid()
                        pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_gull_density'
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()
                        outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_gull_density| alt text| width = 500px}}||')




                        df_tmp = pd.DataFrame({'val' : w_cyn_avg_l})
                        weights = []
                        weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
                        col_r = 'm'
                        try:
                            df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
                        except:
                            print('hmm')

                        mplpl.hist(list(df_tmp['val']), normed=1, color=col)

                        mplpl.xlim([0, 1.2])
                        mplpl.ylim([0, 10])
                        mplpl.ylabel('Density', fontsize=18)
                        mplpl.xlabel('Individual readers cynicallity value', fontsize=18)
                        mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_cyn_avg_l),4)))
                        mplpl.grid()
                        pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_cyn_density'
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()
                        outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_cyn_density| alt text| width = 500px}}||\n\n')



                ########################

                #####################################################33

                        #
                        # num_bins = len(pt_l)
                        # counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
                        # cdf = np.cumsum(counts)
                        # scale = 1.0 / cdf[-1]
                        # ncdf = scale * cdf
                        # mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='')
                        # mplpl.ylabel('CDF', fontsize=18)
                        # mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
                        # mplpl.grid()
                        # mplpl.title(data_name)
                        # #         mplpl.legend(loc="upper right")
                        # mplpl.xlim([-1, 1])
                        # mplpl.ylim([0, 1])
                        # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_pt_cdf'
                        # mplpl.savefig(pp, format='png')
                        # mplpl.figure()
                        #
                        # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_pt_cdf| alt text| width = 500px}}')
                        #
                        #
                        # num_bins = len(acc_l)
                        # counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
                        # cdf = np.cumsum(counts)
                        # scale = 1.0 / cdf[-1]
                        # ncdf = scale * cdf
                        # mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='')
                        # mplpl.ylabel('CDF', fontsize=18)
                        # mplpl.xlabel('Accuracy', fontsize=18)
                        # mplpl.title(data_name)
                        # #         mplpl.legend(loc="upper right")
                        # mplpl.xlim([-1, 1])
                        # mplpl.ylim([0, 1])
                        # mplpl.grid()
                        # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_acc_cdf'
                        # mplpl.savefig(pp, format='png')
                        # mplpl.figure()
                        # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_acc_cdf| alt text| width = 500px}}')
                        #
                        #
                        # num_bins = len(err_l)
                        # counts, bin_edges = np.histogram(err_l, bins=num_bins, normed=True)
                        # cdf = np.cumsum(counts)
                        # scale = 1.0 / cdf[-1]
                        # ncdf = scale * cdf
                        # mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='')
                        # mplpl.ylabel('CDF', fontsize=18)
                        # mplpl.xlabel('Perception bias (PB)', fontsize=18)
                        # mplpl.title(data_name)
                        # #         mplpl.legend(loc="upper right")
                        # mplpl.xlim([-1, 1])
                        # mplpl.ylim([0, 1])
                        # mplpl.grid()
                        # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_err_cdf'
                        # mplpl.savefig(pp, format='png')
                        # mplpl.figure()
                        #
                        # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_err_cdf| alt text| width = 500px}}')
                        #
                        # num_bins = len(abs_err_l)
                        # counts, bin_edges = np.histogram(abs_err_l, bins=num_bins, normed=True)
                        # cdf = np.cumsum(counts)
                        # scale = 1.0 / cdf[-1]
                        # ncdf = scale * cdf
                        # mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='All users')
                        # mplpl.ylabel('CDF', fontsize=18)
                        # mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
                        # mplpl.title(data_name)
                        # #         mplpl.legend(loc="upper right")
                        # mplpl.xlim([0, 1])
                        # mplpl.ylim([0, 1])
                        # mplpl.grid()
                        # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_abs-err_cdf'
                        # mplpl.savefig(pp, format='png')
                        # mplpl.figure()
                        # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_abs-err_cdf| alt text| width = 500px}}')
                        #
                        # # outF.write('|| Table ||\n\n')
                        #
                        # num_bins = len(gull_l)
                        # counts, bin_edges = np.histogram(gull_l, bins=num_bins, normed=True)
                        # cdf = np.cumsum(counts)
                        # scale = 1.0 / cdf[-1]
                        # ncdf = scale * cdf
                        # mplpl.plot(bin_edges[1:], ncdf, c='m', lw=5, label='')
                        # mplpl.ylabel('CDF', fontsize=18)
                        # mplpl.xlabel('Gullibility', fontsize=18)
                        # mplpl.grid()
                        # mplpl.title(data_name)
                        # #         mplpl.legend(loc="upper right")
                        # mplpl.xlim([0, 1])
                        # mplpl.ylim([0, 1])
                        # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_gull_cdf'
                        # mplpl.savefig(pp, format='png')
                        # mplpl.figure()
                        #
                        # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_gull_cdf| alt text| width = 500px}}')
                        #
                        # num_bins = len(cyn_l)
                        # counts, bin_edges = np.histogram(cyn_l, bins=num_bins, normed=True)
                        # cdf = np.cumsum(counts)
                        # scale = 1.0 / cdf[-1]
                        # ncdf = scale * cdf
                        # mplpl.plot(bin_edges[1:], ncdf, c='k', lw=5, label='')
                        # mplpl.ylabel('CDF', fontsize=18)
                        # mplpl.xlabel('Cynicality', fontsize=18)
                        # mplpl.grid()
                        # mplpl.title(data_name)
                        # #         mplpl.legend(loc="upper right")
                        # mplpl.xlim([0, 1])
                        # mplpl.ylim([0, 1])
                        # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_cyn_cdf'
                        # mplpl.savefig(pp, format='png')
                        # mplpl.figure()
                        #
                        # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_cyn_cdf| alt text| width = 500px}}||\n')
                        #
                        #
                        #
                        #
                        # exit()
                    else:

                        AVG_list = []
                        print(np.mean(all_acc))
                        outF = open(remotedir + 'output.txt', 'w')

                        tweet_all_var = {}
                        tweet_all_dev_avg = {}
                        tweet_all_avg = {}
                        tweet_all_gt_var = {}
                        tweet_all_dev_avg_l = []
                        tweet_all_dev_med_l = []
                        tweet_all_dev_var_l = []
                        tweet_all_avg_l = []
                        tweet_all_med_l = []
                        tweet_all_var_l = []
                        tweet_all_gt_var_l = []
                        diff_group_disp_l = []
                        dem_disp_l = []
                        rep_disp_l = []

                        tweet_all_dev_avg = {}
                        tweet_all_dev_med = {}
                        tweet_all_dev_var = {}

                        tweet_all_dev_avg_l = []
                        tweet_all_dev_med_l = []
                        tweet_all_dev_var_l = []

                        tweet_all_abs_dev_avg = {}
                        tweet_all_abs_dev_med = {}
                        tweet_all_abs_dev_var = {}

                        tweet_all_abs_dev_avg_l = []
                        tweet_all_abs_dev_med_l = []
                        tweet_all_abs_dev_var_l = []
                        tweet_all_dev_avg_rnd = {}
                        tweet_all_abs_dev_avg_rnd = {}

                        diff_group_disp_dict = {}
                        if dataset == 'snopes':
                            data_n = 'sp'
                            news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                            ind_l = [1, 2, 3]
                        elif dataset == 'politifact':
                            data_n = 'pf'
                            news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                            ind_l = [1, 2, 3]
                        elif dataset == 'mia':
                            data_n = 'mia'
                            news_cat_list = ['rumor', 'non-rumor']
                            ind_l = [1]

                        for cat_l in news_cat_list:
                            outF.write('== ' + str(cat_l) + ' ==\n\n')
                            print('== ' + str(cat_l) + ' ==')
                            tweet_dev_avg = {}
                            tweet_dev_med = {}
                            tweet_dev_var = {}
                            tweet_abs_dev_avg = {}
                            tweet_abs_dev_med = {}
                            tweet_abs_dev_var = {}

                            tweet_avg = {}
                            tweet_med = {}
                            tweet_var = {}
                            tweet_gt_var = {}

                            tweet_dev_avg_rnd = {}
                            tweet_abs_dev_avg_rnd = {}


                            tweet_dev_avg_l = []
                            tweet_dev_med_l = []
                            tweet_dev_var_l = []
                            tweet_abs_dev_avg_l = []
                            tweet_abs_dev_med_l = []
                            tweet_abs_dev_var_l = []

                            tweet_avg_l = []
                            tweet_med_l = []
                            tweet_var_l = []
                            tweet_gt_var_l = []
                            AVG_susc_list = []
                            AVG_wl_list = []
                            all_acc = []
                            AVG_dev_list = []
                            # for lean in [-1, 0, 1]:

                                # AVG_susc_list = []
                                # AVG_wl_list = []
                                # all_acc = []
                                # df_m = df_m[df_m['leaning'] == lean]
                                # if lean == 0:
                                #     col = 'g'
                                #     lean_cat = 'neutral'
                                # elif lean == 1:
                                #     col = 'b'
                                #     lean_cat = 'democrat'
                                # elif lean == -1:
                                #     col = 'r'
                                #     lean_cat = 'republican'
                                # print(lean_cat)
                            for ind in ind_l:

                                if balance_f == 'balanced':
                                    inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
                                else:
                                    inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'

                                inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
                                df[ind] = pd.read_csv(inp1, sep="\t")
                                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                                df_m = df[ind].copy()
                                df_mm = df_m.copy()

                                df_m = df_m[df_m['ra_gt'] == cat_l]
                                # df_mm = df_m[df_m['ra_gt']==cat_l]
                                # df_m = df_m[df_m['leaning'] == lean]

                                groupby_ftr = 'tweet_id'
                                grouped = df_m.groupby(groupby_ftr, sort=False)
                                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                                for t_id in grouped.groups.keys():
                                    df_tmp = df_m[df_m['tweet_id'] == t_id]

                                    df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
                                    df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
                                    df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
                                    ind_t = df_tmp.index.tolist()[0]
                                    weights = []
                                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                                    ind_t = df_tmp.index.tolist()[0]
                                    weights = []

                                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                                    val_list = list(df_tmp['rel_v'])
                                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                                    tweet_avg[t_id] = np.mean(val_list)
                                    tweet_med[t_id] = np.median(val_list)
                                    tweet_var[t_id] = np.var(val_list)
                                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                                    tweet_avg_l.append(np.mean(val_list))
                                    tweet_med_l.append(np.median(val_list))
                                    tweet_var_l.append(np.var(val_list))
                                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])




                                    tweet_all_avg[t_id] = np.mean(val_list)
                                    tweet_all_var[t_id] = np.var(val_list)
                                    tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                                    tweet_all_avg_l.append(np.mean(val_list))
                                    tweet_all_med_l.append(np.median(val_list))
                                    tweet_all_var_l.append(np.var(val_list))
                                    tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])



                                    val_list = list(df_tmp['err'])
                                    abs_var_err = [np.abs(x) for x in val_list]
                                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                                    tweet_dev_avg[t_id] = np.mean(val_list)
                                    tweet_dev_med[t_id] = np.median(val_list)
                                    tweet_dev_var[t_id] = np.var(val_list)

                                    tweet_dev_avg_l.append(np.mean(val_list))
                                    tweet_dev_med_l.append(np.median(val_list))
                                    tweet_dev_var_l.append(np.var(val_list))

                                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                                    tweet_abs_dev_var_l.append(np.var(abs_var_err))


                                    tweet_all_dev_avg[t_id] = np.mean(val_list)
                                    tweet_all_dev_med[t_id] = np.median(val_list)
                                    tweet_all_dev_var[t_id] = np.var(val_list)

                                    tweet_all_dev_avg_l.append(np.mean(val_list))
                                    tweet_all_dev_med_l.append(np.median(val_list))
                                    tweet_all_dev_var_l.append(np.var(val_list))

                                    tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
                                    tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
                                    tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)

                                    tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
                                    tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
                                    tweet_all_abs_dev_var_l.append(np.var(abs_var_err))



                                    sum_rnd_abs_perc = 0
                                    sum_rnd_perc = 0
                                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                                    random_perc = np.abs(sum_rnd_perc / float(7))
                                    random_abs_perc = sum_rnd_abs_perc / float(7)

                                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                                    tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                                    tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)

                            gt_l = []
                            pt_l = []
                            disputability_l = []
                            perc_l = []
                            abs_perc_l = []
                            # for t_id in tweet_l_sort:
                            #     gt_l.append(tweet_gt_var[t_id])
                            #     pt_l.append(tweet_avg[t_id])
                            #     disputability_l.append(tweet_var[t_id])
                            #     perc_l.append(tweet_dev_avg[t_id])
                            #     abs_perc_l.append(tweet_abs_dev_avg[t_id])



                            # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
                            tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
                            # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
                            # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
                            # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
                            # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)

                            # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)


                            if dataset == 'snopes':
                                data_addr = 'snopes'
                            elif dataset == 'politifact':
                                data_addr = 'politifact/fig'
                            elif dataset == 'mia':
                                data_addr = 'mia/fig'

                            count = 0
                            outF.write(
                                '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                            # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')

                            for t_id in tweet_l_sort:
                                count+=1
                                if balance_f=='balanced':
                                    outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                               + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
                                               + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                               str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                               str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                               str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                               str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                                    # +

                                else:
                                    outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
                                               # str(np.round(diff_group_disp_dict[t_id], 3)) +
                                               '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
                                                + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
                                                '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                               str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                               str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                               str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                               str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')




                        if dataset == 'snopes':
                            data_addr = 'snopes'
                        elif dataset == 'politifact':
                            data_addr = 'politifact/fig'
                        elif dataset == 'mia':
                            data_addr = 'mia/fig'

                        # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
                        # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
                        tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                        # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
                        # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
                        # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
                        # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)

                        # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
                        # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)

                        tweet_napb_dict_high_disp = {}
                        tweet_napb_dict_low_disp = {}
                        for t_id in tweet_l_sort[:20]:
                            # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                            tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                        for t_id in tweet_l_sort[-20:]:
                            # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
                            tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]

                        kk = 0

                        for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
                            if kk==0:
                                tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
                            else:
                                tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)

                            kk+=1
                            count = 0
                            outF.write(
                                '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
                            for t_id in tweet_l_sort:
                                count += 1
                                # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
                                if balance_f == 'balanced':
                                    outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
                                               + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
                                               str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
                                               str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
                                               str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                               str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                               str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
                                               str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                                    # +
                                    #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
                                    #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

                                else:
                                    outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                                               str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
                                               str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
                                               str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                               str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                               str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
                                               str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
                                # +
                                # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                                # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')


    if args.t == "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_stastistics_top-bottom_accurate_readers_demographics":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []



        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','politifact','mia']:
        # for dataset in ['mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                    if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                        tweet_lable_dict[tweet_id] = 'undecided'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                col = 'purple'
                marker = 'o'
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            elif dataset=='politifact':
                col = 'c'
                marker='>'
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                col = 'orange'
                marker='*'
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'


            inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(1) + '_final.csv'
            df_test = pd.read_csv(inp1, sep="\t")

            # for feat in ['gender','leaning']:
            #     feat_cat_l = set(df_test[feat])
            #     if feat == 'leaning':
            #         feat_cat_l = [-1,0,1]
            #     if feat == 'gender':
            #         feat_cat_l = ['male','female']
            #
            #
            #     for feat_cat in feat_cat_l:
            #         if feat == 'leaning':
            #             if feat_cat==0:
            #                 col = 'g'
            #             elif feat_cat==-1:
            #                 col='r'
            #             elif feat_cat==1:
            #                 col='b'
            #
            #         if feat == 'gender':
            #             if feat_cat == 'male':
            #                 col = 'c'
            #             elif feat_cat == 'female':
            #                 col = 'y'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []
            user_acc_dict = {}
            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                # df[ind] = df[ind][df[ind][feat]==feat_cat]
                df_m = df[ind].copy()
                df[ind].loc[:, 'abs_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_abs_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'user_acc'] = df[ind]['tweet_id'] * 0.0

                groupby_ftr = 'tweet_id'
                grouped = df[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df[ind].groupby(groupby_ftr, sort=False).sum()


                for ind_t in df[ind].index.tolist():
                    t_id = df[ind]['tweet_id'][ind_t]
                    err = df[ind]['err'][ind_t]
                    abs_err = np.abs(err)
                    df[ind]['abs_err'][ind_t] = abs_err
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df[ind]['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df[ind]['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)


                    norm_err = err / float(random_perc)
                    norm_abs_err = abs_err / float(random_abs_perc)
                    df[ind]['norm_err'][ind_t] = norm_err
                    df[ind]['norm_abs_err'][ind_t] = norm_abs_err

                groupby_ftr = 'worker_id'
                grouped = df[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df[ind].groupby(groupby_ftr, sort=False).sum()

                for ind_w in df[ind].index.tolist():
                    w_id = df[ind]['worker_id'][ind_w]
                    df_tmp = df[ind][df[ind]['worker_id']==w_id]
                    w_acc_list_tmp = list(df_tmp['acc'])
                    acc_c = 0
                    nacc_c = 0
                    for el in w_acc_list_tmp:
                        if el == 0:
                            nacc_c += 1
                        elif el == 1:
                            acc_c += 1

                    w_ind_acc = (acc_c / float(acc_c + nacc_c))
                    df[ind]['user_acc'][ind_w] = w_ind_acc
                    user_acc_dict[w_id] = w_ind_acc
                # df[ind] = df[ind].copy()



            w_pt_avg_l = []
            w_err_avg_l = []
            w_abs_err_avg_l = []
            w_norm_err_avg_l = []
            w_norm_abs_err_avg_l = []
            w_acc_avg_l = []

            w_pt_std_l = []
            w_err_std_l = []
            w_abs_err_std_l = []
            w_norm_err_std_l = []
            w_norm_abs_err_std_l = []
            w_acc_std_l = []

            w_pt_avg_dict = collections.defaultdict()
            w_err_avg_dict = collections.defaultdict()
            w_abs_err_avg_dict = collections.defaultdict()
            w_norm_err_avg_dict = collections.defaultdict()
            w_norm_abs_err_avg_dict = collections.defaultdict()
            w_acc_avg_dict = collections.defaultdict()

            w_acc_std_dict = collections.defaultdict()

            all_w_pt_list  = []
            all_w_err_list = []
            all_w_abs_err_list = []
            all_w_norm_err_list = []
            all_w_norm_abs_err_list  = []
            all_w_acc_list = []

            all_w_cyn_list = []
            all_w_gull_list = []
            w_cyn_avg_l = []
            w_gull_avg_l = []
            w_cyn_std_l= []
            w_gull_std_l = []
            w_cyn_avg_dict =collections.defaultdict()
            w_gull_avg_dict =collections.defaultdict()
            w_cyn_std_dict =collections.defaultdict()
            w_gull_std_dict = collections.defaultdict()


            thr=0.1
            user_acc_l_sort = sorted(user_acc_dict, key=user_acc_dict.get, reverse=True)

            # demog_list = ['nationality', 'residence', 'gender', 'age', 'degree', 'employment', 'income',
            #               'political_view', 'race', 'marital_status', 'leaning']
            #
            # output = open(remotedir + 'accurate_user.txt','w')
            #
            # user_dem_att_main = {}
            # df_m = df[1].copy()
            # for dem_att in demog_list:
            #     user_dem_att_main[dem_att] = list(set(df_m[dem_att]))
            #
            # user_dem_att_total = collections.defaultdict()
            #
            # for dem_att in demog_list:
            #
            #     for thr in [1]:
            #         top_acc_users = user_acc_l_sort[:int(thr * len(user_acc_l_sort))]
            #         for w_id in top_acc_users:
            #             for ind_t in ind_l:
            #                 if w_id in list(df[ind_t]['worker_id']):
            #                     ind =ind_t
            #                     break
            #             df_m = df[ind].copy()
            #
            #             if dem_att not in user_dem_att_total:
            #                 user_dem_att_total[dem_att] = collections.defaultdict(int)
            #             w_ind = df_m[df_m['worker_id']==w_id].index.tolist()[0]
            #             feat_list = user_dem_att_main[dem_att]
            #             for feat in list(feat_list):
            #
            #                 if df_m[dem_att][w_ind]==feat:
            #                     user_dem_att_total[dem_att][feat]+=1




            # for dem_att in demog_list:
            #     output.write('== ' + dem_att + ' ==\n')
            #     output.write('|| ||')
            #     for feat in user_dem_att_main[dem_att]:
            #         output.write(str(modify_demographic(str(feat))) + ' ||')
            #     output.write('\n')
            #
            #     for thr in [0.1, 0.2, 1]:
            #         output.write('||' + str(thr * 100) + '% bottom ||')
            #         # top_acc_users = user_acc_l_sort[:int(thr * len(user_acc_l_sort))]
            #         bottom_acc_users = user_acc_l_sort[int((1-thr) * len(user_acc_l_sort)):]
            #         user_dem_att = collections.defaultdict()
            #
            #         # for w_id in user_acc_l_sort:
            #         for w_id in bottom_acc_users:
            #             for ind_t in ind_l:
            #                 if w_id in list(df[ind_t]['worker_id']):
            #                     ind =ind_t
            #                     break
            #             df_m = df[ind].copy()
            #
            #             if dem_att not in user_dem_att:
            #                 user_dem_att[dem_att] = collections.defaultdict(int)
            #             w_ind = df_m[df_m['worker_id']==w_id].index.tolist()[0]
            #             feat_list = user_dem_att_main[dem_att]
            #             for feat in list(feat_list):
            #
            #                 if df_m[dem_att][w_ind]==feat:
            #                     user_dem_att[dem_att][feat]+=1
            #
            #
            #         for feat in user_dem_att_main[dem_att]:
            #             x = user_dem_att[dem_att][feat]/float(len(bottom_acc_users))
            #             y = user_dem_att_total[dem_att][feat] / float(len(user_acc_l_sort))
            #
            #             output.write(str(user_dem_att[dem_att][feat]) + '<<BR>>'
            #                          + str(np.round(x,3)) + '<<BR>>'
            #                          + str(np.round(x/float(y),3))+ '||')
            #
            #         output.write('\n')
            #     output.write('\n')
            #
            # exit()
            thr_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
            acc_top_list = []
            correlation_top_list = []

            for thr in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]:
                w_acc_list = []
                w_pt_avg_l = []
                w_err_avg_l = []
                w_abs_err_avg_l = []
                w_norm_err_avg_l = []
                w_norm_abs_err_avg_l = []
                w_acc_avg_l = []

                w_pt_std_l = []
                w_err_std_l = []
                w_abs_err_std_l = []
                w_norm_err_std_l = []
                w_norm_abs_err_std_l = []
                w_acc_std_l = []

                w_pt_avg_dict = collections.defaultdict()
                w_err_avg_dict = collections.defaultdict()
                w_abs_err_avg_dict = collections.defaultdict()
                w_norm_err_avg_dict = collections.defaultdict()
                w_norm_abs_err_avg_dict = collections.defaultdict()
                w_acc_avg_dict = collections.defaultdict()

                w_acc_std_dict = collections.defaultdict()

                all_w_pt_list = []
                all_w_err_list = []
                all_w_abs_err_list = []
                all_w_norm_err_list = []
                all_w_norm_abs_err_list = []
                all_w_acc_list = []

                all_w_cyn_list = []
                all_w_gull_list = []
                w_cyn_avg_l = []
                w_gull_avg_l = []
                w_cyn_std_l = []
                w_gull_std_l = []
                w_cyn_avg_dict = collections.defaultdict()
                w_gull_avg_dict = collections.defaultdict()
                w_cyn_std_dict = collections.defaultdict()
                w_gull_std_dict = collections.defaultdict()

                top_acc_users = user_acc_l_sort[:int(thr * len(user_acc_l_sort))]
                # bottom_acc_users = user_acc_l_sort[int((1 - thr) * len(user_acc_l_sort)):]
                user_dem_att = collections.defaultdict()
                top_f = True
                # top_f = False
                if top_f == False:
                    top_f='bot'
                else:
                    top_f=''

                PT_l = []
                GT_l = []
                for ind_t in ind_l:
                    df_m = df[ind_t].copy()

                    df_top_acc = df_m[df_m['worker_id'].isin(top_acc_users)]
                    for t_id in list(df[ind_t]['tweet_id']):
                        if tweet_lable_dict[t_id]=='MIXTURE' or tweet_lable_dict[t_id]=='half-true':
                            continue
                        df_tmp = df_top_acc[df_top_acc['tweet_id']==t_id]
                        index_t = df_tmp['tweet_id'].index.tolist()[0]
                        w_acc_list_tmp = list(df_tmp['acc'])
                        acc_c = 0
                        nacc_c = 0
                        for el in w_acc_list_tmp:
                            if el == 0:
                                nacc_c += 1
                            elif el == 1:
                                acc_c += 1

                        # w_ind_acc = (acc_c / float(acc_c + nacc_c))
                        # w_acc_list.appen(w_ind_acc)
                        if acc_c >= nacc_c:
                            w_ind_acc = 1
                        else:
                            w_ind_acc = 0
                        w_acc_list.append(w_ind_acc)
                        gt_v = df_tmp['rel_gt_v'][index_t]
                        GT_l.append(gt_v)
                        val_list = list(df_tmp['rel_v'])
                        PT_l.append(np.mean(val_list))
                acc_top_list.append(np.mean(w_acc_list))
                correlation_top_list.append(np.round(np.corrcoef(GT_l, PT_l)[0][1],3))
                # for w_id in top_acc_users:
                #     for ind_t in ind_l:
                #         if w_id in list(df[ind_t]['worker_id']):
                #             ind = ind_t
                #             break
                #     df_m = df[ind].copy()
                #
                #     df_tmp = df_m[df_m['worker_id'] == w_id]
                #     w_ind = df_m['worker_id'].index.tolist()[0]
                #
                #
                #     w_pt_list = list(df_tmp['rel_v'])
                #     w_err_list = list(df_tmp['err'])
                #     w_abs_err_list = list(df_tmp['abs_err'])
                #     w_norm_err_list = list(df_tmp['norm_err'])
                #     w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                #     w_cyn_list = list(df_tmp['cyn'])
                #     w_gull_list = list(df_tmp['gull'])
                #     w_acc_list_tmp = list(df_tmp['acc'])
                #     w_acc_list = []
                #
                #
                #     all_w_pt_list += list(df_tmp['rel_v'])
                #     all_w_err_list += list(df_tmp['err'])
                #     all_w_abs_err_list += list(df_tmp['abs_err'])
                #     all_w_norm_err_list += list(df_tmp['norm_err'])
                #     all_w_norm_abs_err_list += list(df_tmp['norm_abs_err'])
                #     all_w_cyn_list += list(df_tmp['cyn'])
                #     all_w_gull_list += list(df_tmp['gull'])
                #     all_w_acc_list += list(df_tmp['user_acc'])[0]
                #
                #
                #
                #     w_pt_avg_l.append(np.mean(w_pt_list))
                #     w_err_avg_l.append(np.mean(w_err_list))
                #     w_abs_err_avg_l.append(np.mean(w_abs_err_list))
                #     w_norm_err_avg_l.append(np.mean(w_norm_err_list))
                #     w_norm_abs_err_avg_l.append(np.mean(w_norm_abs_err_list))
                #     w_cyn_avg_l.append(np.mean(w_cyn_list))
                #     w_gull_avg_l.append(np.mean(w_gull_list))
                #     w_acc_avg_l.append(list(df_tmp['user_acc'])[0])
                #
                #     w_pt_std_l.append(np.std(w_pt_list))
                #     w_err_std_l.append(np.std(w_err_list))
                #     w_abs_err_std_l.append(np.std(w_abs_err_list))
                #     w_norm_err_std_l.append(np.std(w_norm_err_list))
                #     w_norm_abs_err_std_l.append(np.std(w_norm_abs_err_list))
                #     w_cyn_std_l.append(np.std(w_cyn_list))
                #     w_gull_std_l.append(np.std(w_gull_list))
                #     # w_acc_std_l.append(np.std(w_ind_acc_list))
                #
                #
                #     w_pt_avg_dict[w_id] = np.mean(w_pt_list)
                #     w_err_avg_dict[w_id] = np.mean(w_err_list)
                #     w_abs_err_avg_dict[w_id] = np.mean(w_abs_err_list)
                #     w_norm_err_avg_dict[w_id] = np.mean(w_norm_err_list)
                #     w_norm_abs_err_avg_dict[w_id] = np.mean(w_norm_abs_err_list)
                #     w_cyn_avg_dict[w_id] = np.mean(w_cyn_list)
                #     w_gull_avg_dict[w_id] = np.mean(w_gull_list)
                #     w_acc_avg_dict[w_id] = list(df_tmp['user_acc'])[0]

            # mplpl.plot(thr_list, acc_top_list)
            mplpl.scatter(thr_list,acc_top_list, s=80,color=col, marker=marker, label=data_name)
            mplpl.plot(thr_list,acc_top_list,linewidth=3.0, color=col)

            # mplpl.plot(thr_list, correlation_top_list)
            # print(correlation_top_list)
            # mplpl.scatter(thr_list, correlation_top_list, s=80, color=col, marker=marker, label=data_name)
            # mplpl.plot(thr_list, correlation_top_list, linewidth=3.0, color=col)

        mplpl.rc('xtick', labelsize='large')
        mplpl.rc('ytick', labelsize='large')
        # mplpl.rc('xtick.major', size=3, pad=3)
        # mplpl.rc('xtick.minor', size=2, pad=3)
        mplpl.rc('legend', fontsize='large')
        mplpl.xlabel('Top k fraction of users ranked by accuracy',fontsize=18)
        mplpl.ylabel('Fraction of stories guess correctly by users',fontsize=18)
        # mplpl.xlabel('Top k fraction of users ranked by accuracy',fontsize=18)
        # mplpl.ylabel('Correleation between GTL and PTL',fontsize=18)
        # mplpl.title('All users, Mean : ' + str(np.mean(dem_gull_list_sort)))
        mplpl.ylim([0,1])
        mplpl.xlim([0,1])
        mplpl.grid()
        #
        mplpl.legend(loc="lower right")
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/user_based/assess_truth_level'
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/user_based/assess_corr_gt_pt_topk_acc'
        # mplpl.savefig(pp + '.png', format='png')

        pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/user_based/assess_truth_level'
        mplpl.savefig(pp + '.png', format='png')

        pp = PdfPages('/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/user_based/assess_truth_level.pdf')
        pp.savefig()
        pp.close()

        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/user_based/assess_corr_gt_pt_topk_acc'
        # mplpl.savefig(pp + '.png', format='png')
        #
        # pp = PdfPages('/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/user_based/assess_corr_gt_pt_topk_acc.pdf')
        # pp.savefig()
        # pp.close()
        #

        # mplpl.figure()






                    # w_pt_std_dict[w_id] = np.std(w_pt_list)
                    # w_err_std_dict[w_id] = np.std(w_err_list)
                    # w_abs_err_std_dict[w_id] = np.std(w_abs_err_list)
                    # w_norm_err_std_dict[w_id] = np.std(w_norm_err_list)
                    # w_norm_abs_err_std_dict[w_id] = np.std(w_norm_abs_err_list)
                    # w_cyn_std_dict[w_id] = np.std(w_cyn_list)
                    # w_gull_std_dict[w_id] = np.std(w_gull_list)
                    # w_acc_std_dict[w_id] = np.std(w_acc_list)







            ##################################################



        #         fig_f = True
        #         # fig_f = False
        #         # fig_f_1 = True
        #         fig_f_1 = False
        #         if fig_f==True:
        #
        #         # df_tmp = pd.DataFrame({'val' : all_w_acc_list})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
        #         # # mplpl.hist(list(df_tmp['val']), normed=1, color='g')
        #         #
        #         #
        #         # mplpl.xlim([-1.5, 1.5])
        #         # mplpl.ylim([0, 1.5])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Readers accuracy', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_acc_list),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) +'_acc_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_acc_density| alt text| width = 500px}}')
        #         #
        #         #
        #         #
        #         # # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
        #         # df_tmp = pd.DataFrame({'val' : w_acc_avg_l})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
        #         # mplpl.hist(list(df_tmp['val']), normed=1, color=col)
        #         #
        #         #
        #         # # mplpl.plot(gt_set, pt_mean,  color='k')
        #         # mplpl.xlim([-1.5, 1.5])
        #         # mplpl.ylim([0, 5])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Individual readers accuracy', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_acc_avg_l),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_acc_pt'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_acc_pt| alt text| width = 500px}}')
        #         #
        #         #
        #         #
        #         #
        #         # df_tmp = pd.DataFrame({'val' : all_w_pt_list})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
        #         # # mplpl.hist(list(df_tmp['val']), normed=1, color='g')
        #         #
        #         #
        #         # mplpl.xlim([-1.5, 1.5])
        #         # mplpl.ylim([0, 1.5])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Readers percevied truth value', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_pt_list),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) +'_pt_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_pt_density| alt text| width = 500px}}')
        #         #
        #         #
        #         #
        #         #
        #         # # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
        #         # df_tmp = pd.DataFrame({'val' : w_pt_avg_l})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
        #         # mplpl.hist(list(df_tmp['val']), normed=1, color=col)
        #         #
        #         #
        #         # # mplpl.plot(gt_set, pt_mean,  color='k')
        #         # mplpl.xlim([-1.2, 1.2])
        #         # mplpl.ylim([0, 3.5])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Individual readers percevied truth value', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_pt_avg_l),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_pt_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_pt_density| alt text| width = 500px}}')
        #         #
        #         #
        #         #
        #         #
        #         # df_tmp = pd.DataFrame({'val' : w_err_avg_l})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # mplpl.hist(list(df_tmp['val']), normed=1, color=col)
        #         #
        #         #
        #         # mplpl.xlim([-1.2, 1.2])
        #         # mplpl.ylim([0, 3.5])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Individual readers perception bias value', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_err_avg_l),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_pb_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_pb_density| alt text| width = 500px}}')
        #         #
        #         #
        #         #
        #         # df_tmp = pd.DataFrame({'val' : w_abs_err_avg_l})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # mplpl.hist(list(df_tmp['val']), normed=1, color=col)
        #         #
        #         # mplpl.xlim([0, 1.2])
        #         # mplpl.ylim([0, 5])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Individual readers absolute perception bias value', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_abs_err_avg_l),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_apb_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_apb_density| alt text| width = 500px}}||')
        #         #
        #         #
        #         #
        #         # df_tmp = pd.DataFrame({'val' : w_gull_avg_l})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # mplpl.hist(list(df_tmp['val']), normed=1, color=col)
        #         #
        #         # mplpl.xlim([0, 1.2])
        #         # mplpl.ylim([0, 10])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Individual readers gullibility value', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_gull_avg_l),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_gull_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_gull_density| alt text| width = 500px}}||')
        #         #
        #         #
        #         #
        #         #
        #         # df_tmp = pd.DataFrame({'val' : w_cyn_avg_l})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # mplpl.hist(list(df_tmp['val']), normed=1, color=col)
        #         #
        #         # mplpl.xlim([0, 1.2])
        #         # mplpl.ylim([0, 10])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Individual readers cynicallity value', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_cyn_avg_l),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_cyn_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_cyn_density| alt text| width = 500px}}||\n\n')
        #
        #
        #             tweet_l_sort = sorted(w_pt_avg_dict, key=w_pt_avg_dict.get, reverse=False)
        #             pt_l = []
        #             for t_id in tweet_l_sort:
        #                 pt_l.append(w_pt_avg_dict[t_id])
        #
        #
        #             tweet_l_sort = sorted(w_acc_avg_dict, key=w_acc_avg_dict.get, reverse=False)
        #             acc_l = []
        #             for t_id in tweet_l_sort:
        #                 acc_l.append(w_acc_avg_dict[t_id])
        #
        #
        #             tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
        #             err_l = []
        #             for t_id in tweet_l_sort:
        #                 err_l.append(w_err_avg_dict[t_id])
        #
        #
        #             tweet_l_sort = sorted(w_abs_err_avg_dict, key=w_abs_err_avg_dict.get, reverse=False)
        #             abs_err_l = []
        #             for t_id in tweet_l_sort:
        #                 abs_err_l.append(w_abs_err_avg_dict[t_id])
        #
        #
        #             tweet_l_sort = sorted(w_gull_avg_dict, key=w_gull_avg_dict.get, reverse=False)
        #             gull_l = []
        #             for t_id in tweet_l_sort:
        #                 gull_l.append(w_gull_avg_dict[t_id])
        #
        #
        #             tweet_l_sort = sorted(w_cyn_avg_dict, key=w_cyn_avg_dict.get, reverse=False)
        #             cyn_l = []
        #             for t_id in tweet_l_sort:
        #                 cyn_l.append(w_cyn_avg_dict[t_id])
        #
        #             #####################################################33
        #
        #
        #             # num_bins = len(pt_l)
        #             # counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
        #             # cdf = np.cumsum(counts)
        #             # scale = 1.0 / cdf[-1]
        #             # ncdf = scale * cdf
        #             # mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='')
        #             # mplpl.ylabel('CDF', fontsize=18)
        #             # mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
        #             # mplpl.grid()
        #             # mplpl.title(data_name)
        #             # #         mplpl.legend(loc="upper right")
        #             # mplpl.xlim([-1, 1])
        #             # mplpl.ylim([0, 1])
        #             # pp = remotedir + '/fig/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f  + '_' + str(thr)+'_user_pt_cdf'
        #             # mplpl.savefig(pp, format='png')
        #             # mplpl.figure()
        #             #
        #             # outF.write(
        #             #     '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/top_bot_acc/' + data_n+ '_'  + top_f+ '_' + str(thr)+ '_user_pt_cdf| alt text| width = 500px}}')
        #             #
        #             # num_bins = len(acc_l)
        #             # counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #             # cdf = np.cumsum(counts)
        #             # scale = 1.0 / cdf[-1]
        #             # ncdf = scale * cdf
        #             # mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='')
        #             # mplpl.ylabel('CDF', fontsize=18)
        #             # mplpl.xlabel('Accuracy', fontsize=18)
        #             # mplpl.title(data_name)
        #             # #         mplpl.legend(loc="upper right")
        #             # mplpl.xlim([0, 1])
        #             # mplpl.ylim([0, 1])
        #             # mplpl.grid()
        #             # pp = remotedir + '/fig/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f +'_' + str(thr) + '_user_acc_cdf'
        #             # mplpl.savefig(pp, format='png')
        #             # mplpl.figure()
        #             # outF.write(
        #             #     '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f+ '_' + str(thr) + '_user_acc_cdf| alt text| width = 500px}}')
        #             #
        #             # num_bins = len(err_l)
        #             # counts, bin_edges = np.histogram(err_l, bins=num_bins, normed=True)
        #             # cdf = np.cumsum(counts)
        #             # scale = 1.0 / cdf[-1]
        #             # ncdf = scale * cdf
        #             # mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='')
        #             # mplpl.ylabel('CDF', fontsize=18)
        #             # mplpl.xlabel('Perception bias (PB)', fontsize=18)
        #             # mplpl.title(data_name)
        #             # #         mplpl.legend(loc="upper right")
        #             # mplpl.xlim([-1, 1])
        #             # mplpl.ylim([0, 1])
        #             # mplpl.grid()
        #             # pp = remotedir + '/fig/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f+'_' + str(thr) + '_user_err_cdf'
        #             # mplpl.savefig(pp, format='png')
        #             # mplpl.figure()
        #             #
        #             # outF.write(
        #             #     '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f +'_' + str(thr) + '_user_err_cdf| alt text| width = 500px}}')
        #             #
        #             # num_bins = len(abs_err_l)
        #             # counts, bin_edges = np.histogram(abs_err_l, bins=num_bins, normed=True)
        #             # cdf = np.cumsum(counts)
        #             # scale = 1.0 / cdf[-1]
        #             # ncdf = scale * cdf
        #             # mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='All users')
        #             # mplpl.ylabel('CDF', fontsize=18)
        #             # mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
        #             # mplpl.title(data_name)
        #             # #         mplpl.legend(loc="upper right")
        #             # mplpl.xlim([0, 1])
        #             # mplpl.ylim([0, 1])
        #             # mplpl.grid()
        #             # pp = remotedir + '/fig/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f +'_' + str(thr) + '_user_abs-err_cdf'
        #             # mplpl.savefig(pp, format='png')
        #             # mplpl.figure()
        #             # outF.write(
        #             #     '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f+ '_' + str(thr) + '_user_abs-err_cdf| alt text| width = 500px}}')
        #             #
        #             # # outF.write('|| Table ||\n\n')
        #             #
        #             # num_bins = len(gull_l)
        #             # counts, bin_edges = np.histogram(gull_l, bins=num_bins, normed=True)
        #             # cdf = np.cumsum(counts)
        #             # scale = 1.0 / cdf[-1]
        #             # ncdf = scale * cdf
        #             # mplpl.plot(bin_edges[1:], ncdf, c='m', lw=5, label='')
        #             # mplpl.ylabel('CDF', fontsize=18)
        #             # mplpl.xlabel('Gullibility', fontsize=18)
        #             # mplpl.grid()
        #             # mplpl.title(data_name)
        #             # #         mplpl.legend(loc="upper right")
        #             # mplpl.xlim([0, 1])
        #             # mplpl.ylim([0, 1])
        #             # pp = remotedir + '/fig/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f +'_' + str(thr) + '_user_gull_cdf'
        #             # mplpl.savefig(pp, format='png')
        #             # mplpl.figure()
        #             #
        #             # outF.write(
        #             #     '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f +'_' + str(thr) + '_user_gull_cdf| alt text| width = 500px}}')
        #             #
        #             # num_bins = len(cyn_l)
        #             # counts, bin_edges = np.histogram(cyn_l, bins=num_bins, normed=True)
        #             # cdf = np.cumsum(counts)
        #             # scale = 1.0 / cdf[-1]
        #             # ncdf = scale * cdf
        #             # mplpl.plot(bin_edges[1:], ncdf, c='k', lw=5, label='')
        #             # mplpl.ylabel('CDF', fontsize=18)
        #             # mplpl.xlabel('Cynicality', fontsize=18)
        #             # mplpl.grid()
        #             # mplpl.title(data_name)
        #             # #         mplpl.legend(loc="upper right")
        #             # mplpl.xlim([0, 1])
        #             # mplpl.ylim([0, 1])
        #             # pp = remotedir + '/fig/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f+ '_' + str(thr) + '_user_cyn_cdf'
        #             # mplpl.savefig(pp, format='png')
        #             # mplpl.figure()
        #             #
        #             # outF.write(
        #             #     '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f+ '_' + str(thr) + '_user_cyn_cdf| alt text| width = 500px}}||\n')
        #
        #
        #
        #
        #
        #         ########################
        #
        # #####################################################33
        #
        #         #
        #         # num_bins = len(pt_l)
        #         # counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
        #         # cdf = np.cumsum(counts)
        #         # scale = 1.0 / cdf[-1]
        #         # ncdf = scale * cdf
        #         # mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='')
        #         # mplpl.ylabel('CDF', fontsize=18)
        #         # mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
        #         # mplpl.grid()
        #         # mplpl.title(data_name)
        #         # #         mplpl.legend(loc="upper right")
        #         # mplpl.xlim([-1, 1])
        #         # mplpl.ylim([0, 1])
        #         # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_pt_cdf'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_pt_cdf| alt text| width = 500px}}')
        #         #
        #         #
        #         # num_bins = len(acc_l)
        #         # counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         # cdf = np.cumsum(counts)
        #         # scale = 1.0 / cdf[-1]
        #         # ncdf = scale * cdf
        #         # mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='')
        #         # mplpl.ylabel('CDF', fontsize=18)
        #         # mplpl.xlabel('Accuracy', fontsize=18)
        #         # mplpl.title(data_name)
        #         # #         mplpl.legend(loc="upper right")
        #         # mplpl.xlim([-1, 1])
        #         # mplpl.ylim([0, 1])
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_acc_cdf'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_acc_cdf| alt text| width = 500px}}')
        #         #
        #         #
        #         # num_bins = len(err_l)
        #         # counts, bin_edges = np.histogram(err_l, bins=num_bins, normed=True)
        #         # cdf = np.cumsum(counts)
        #         # scale = 1.0 / cdf[-1]
        #         # ncdf = scale * cdf
        #         # mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='')
        #         # mplpl.ylabel('CDF', fontsize=18)
        #         # mplpl.xlabel('Perception bias (PB)', fontsize=18)
        #         # mplpl.title(data_name)
        #         # #         mplpl.legend(loc="upper right")
        #         # mplpl.xlim([-1, 1])
        #         # mplpl.ylim([0, 1])
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_err_cdf'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_err_cdf| alt text| width = 500px}}')
        #         #
        #         # num_bins = len(abs_err_l)
        #         # counts, bin_edges = np.histogram(abs_err_l, bins=num_bins, normed=True)
        #         # cdf = np.cumsum(counts)
        #         # scale = 1.0 / cdf[-1]
        #         # ncdf = scale * cdf
        #         # mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='All users')
        #         # mplpl.ylabel('CDF', fontsize=18)
        #         # mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
        #         # mplpl.title(data_name)
        #         # #         mplpl.legend(loc="upper right")
        #         # mplpl.xlim([0, 1])
        #         # mplpl.ylim([0, 1])
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_abs-err_cdf'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_abs-err_cdf| alt text| width = 500px}}')
        #         #
        #         # # outF.write('|| Table ||\n\n')
        #         #
        #         # num_bins = len(gull_l)
        #         # counts, bin_edges = np.histogram(gull_l, bins=num_bins, normed=True)
        #         # cdf = np.cumsum(counts)
        #         # scale = 1.0 / cdf[-1]
        #         # ncdf = scale * cdf
        #         # mplpl.plot(bin_edges[1:], ncdf, c='m', lw=5, label='')
        #         # mplpl.ylabel('CDF', fontsize=18)
        #         # mplpl.xlabel('Gullibility', fontsize=18)
        #         # mplpl.grid()
        #         # mplpl.title(data_name)
        #         # #         mplpl.legend(loc="upper right")
        #         # mplpl.xlim([0, 1])
        #         # mplpl.ylim([0, 1])
        #         # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_gull_cdf'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_gull_cdf| alt text| width = 500px}}')
        #         #
        #         # num_bins = len(cyn_l)
        #         # counts, bin_edges = np.histogram(cyn_l, bins=num_bins, normed=True)
        #         # cdf = np.cumsum(counts)
        #         # scale = 1.0 / cdf[-1]
        #         # ncdf = scale * cdf
        #         # mplpl.plot(bin_edges[1:], ncdf, c='k', lw=5, label='')
        #         # mplpl.ylabel('CDF', fontsize=18)
        #         # mplpl.xlabel('Cynicality', fontsize=18)
        #         # mplpl.grid()
        #         # mplpl.title(data_name)
        #         # #         mplpl.legend(loc="upper right")
        #         # mplpl.xlim([0, 1])
        #         # mplpl.ylim([0, 1])
        #         # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_cyn_cdf'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_cyn_cdf| alt text| width = 500px}}||\n')
        #         #
        #         #
        #         #
        #         #
        #         # exit()
        #     else:
        #
        #         AVG_list = []
        #         print(np.mean(all_acc))
        #         outF = open(remotedir + 'output.txt', 'w')
        #
        #         tweet_all_var = {}
        #         tweet_all_dev_avg = {}
        #         tweet_all_avg = {}
        #         tweet_all_gt_var = {}
        #         tweet_all_dev_avg_l = []
        #         tweet_all_dev_med_l = []
        #         tweet_all_dev_var_l = []
        #         tweet_all_avg_l = []
        #         tweet_all_med_l = []
        #         tweet_all_var_l = []
        #         tweet_all_gt_var_l = []
        #         diff_group_disp_l = []
        #         dem_disp_l = []
        #         rep_disp_l = []
        #
        #         tweet_all_dev_avg = {}
        #         tweet_all_dev_med = {}
        #         tweet_all_dev_var = {}
        #
        #         tweet_all_dev_avg_l = []
        #         tweet_all_dev_med_l = []
        #         tweet_all_dev_var_l = []
        #
        #         tweet_all_abs_dev_avg = {}
        #         tweet_all_abs_dev_med = {}
        #         tweet_all_abs_dev_var = {}
        #
        #         tweet_all_abs_dev_avg_l = []
        #         tweet_all_abs_dev_med_l = []
        #         tweet_all_abs_dev_var_l = []
        #         tweet_all_dev_avg_rnd = {}
        #         tweet_all_abs_dev_avg_rnd = {}
        #
        #         diff_group_disp_dict = {}
        #         if dataset == 'snopes':
        #             data_n = 'sp'
        #             news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
        #             ind_l = [1, 2, 3]
        #         elif dataset == 'politifact':
        #             data_n = 'pf'
        #             news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
        #             ind_l = [1, 2, 3]
        #         elif dataset == 'mia':
        #             data_n = 'mia'
        #             news_cat_list = ['rumor', 'non-rumor']
        #             ind_l = [1]
        #
        #         for cat_l in news_cat_list:
        #             outF.write('== ' + str(cat_l) + ' ==\n\n')
        #             print('== ' + str(cat_l) + ' ==')
        #             tweet_dev_avg = {}
        #             tweet_dev_med = {}
        #             tweet_dev_var = {}
        #             tweet_abs_dev_avg = {}
        #             tweet_abs_dev_med = {}
        #             tweet_abs_dev_var = {}
        #
        #             tweet_avg = {}
        #             tweet_med = {}
        #             tweet_var = {}
        #             tweet_gt_var = {}
        #
        #             tweet_dev_avg_rnd = {}
        #             tweet_abs_dev_avg_rnd = {}
        #
        #
        #             tweet_dev_avg_l = []
        #             tweet_dev_med_l = []
        #             tweet_dev_var_l = []
        #             tweet_abs_dev_avg_l = []
        #             tweet_abs_dev_med_l = []
        #             tweet_abs_dev_var_l = []
        #
        #             tweet_avg_l = []
        #             tweet_med_l = []
        #             tweet_var_l = []
        #             tweet_gt_var_l = []
        #             AVG_susc_list = []
        #             AVG_wl_list = []
        #             all_acc = []
        #             AVG_dev_list = []
        #             # for lean in [-1, 0, 1]:
        #
        #                 # AVG_susc_list = []
        #                 # AVG_wl_list = []
        #                 # all_acc = []
        #                 # df_m = df_m[df_m['leaning'] == lean]
        #                 # if lean == 0:
        #                 #     col = 'g'
        #                 #     lean_cat = 'neutral'
        #                 # elif lean == 1:
        #                 #     col = 'b'
        #                 #     lean_cat = 'democrat'
        #                 # elif lean == -1:
        #                 #     col = 'r'
        #                 #     lean_cat = 'republican'
        #                 # print(lean_cat)
        #             for ind in ind_l:
        #
        #                 if balance_f == 'balanced':
        #                     inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
        #                 else:
        #                     inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'
        #
        #                 inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
        #                 df[ind] = pd.read_csv(inp1, sep="\t")
        #                 df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #                 df_m = df[ind].copy()
        #                 df_mm = df_m.copy()
        #
        #                 df_m = df_m[df_m['ra_gt'] == cat_l]
        #                 # df_mm = df_m[df_m['ra_gt']==cat_l]
        #                 # df_m = df_m[df_m['leaning'] == lean]
        #
        #                 groupby_ftr = 'tweet_id'
        #                 grouped = df_m.groupby(groupby_ftr, sort=False)
        #                 grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
        #
        #                 for t_id in grouped.groups.keys():
        #                     df_tmp = df_m[df_m['tweet_id'] == t_id]
        #
        #                     df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
        #                     df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
        #                     df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
        #                     ind_t = df_tmp.index.tolist()[0]
        #                     weights = []
        #                     df_tmp = df_m[df_m['tweet_id'] == t_id]
        #                     ind_t = df_tmp.index.tolist()[0]
        #                     weights = []
        #
        #                     weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
        #                     val_list = list(df_tmp['rel_v'])
        #                     tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
        #                     tweet_avg[t_id] = np.mean(val_list)
        #                     tweet_med[t_id] = np.median(val_list)
        #                     tweet_var[t_id] = np.var(val_list)
        #                     tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
        #
        #                     tweet_avg_l.append(np.mean(val_list))
        #                     tweet_med_l.append(np.median(val_list))
        #                     tweet_var_l.append(np.var(val_list))
        #                     tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
        #
        #
        #
        #
        #                     tweet_all_avg[t_id] = np.mean(val_list)
        #                     tweet_all_var[t_id] = np.var(val_list)
        #                     tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
        #
        #                     tweet_all_avg_l.append(np.mean(val_list))
        #                     tweet_all_med_l.append(np.median(val_list))
        #                     tweet_all_var_l.append(np.var(val_list))
        #                     tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
        #
        #
        #
        #                     val_list = list(df_tmp['err'])
        #                     abs_var_err = [np.abs(x) for x in val_list]
        #                     tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
        #                     tweet_dev_avg[t_id] = np.mean(val_list)
        #                     tweet_dev_med[t_id] = np.median(val_list)
        #                     tweet_dev_var[t_id] = np.var(val_list)
        #
        #                     tweet_dev_avg_l.append(np.mean(val_list))
        #                     tweet_dev_med_l.append(np.median(val_list))
        #                     tweet_dev_var_l.append(np.var(val_list))
        #
        #                     tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
        #                     tweet_abs_dev_med[t_id] = np.median(abs_var_err)
        #                     tweet_abs_dev_var[t_id] = np.var(abs_var_err)
        #
        #                     tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
        #                     tweet_abs_dev_med_l.append(np.median(abs_var_err))
        #                     tweet_abs_dev_var_l.append(np.var(abs_var_err))
        #
        #
        #                     tweet_all_dev_avg[t_id] = np.mean(val_list)
        #                     tweet_all_dev_med[t_id] = np.median(val_list)
        #                     tweet_all_dev_var[t_id] = np.var(val_list)
        #
        #                     tweet_all_dev_avg_l.append(np.mean(val_list))
        #                     tweet_all_dev_med_l.append(np.median(val_list))
        #                     tweet_all_dev_var_l.append(np.var(val_list))
        #
        #                     tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
        #                     tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
        #                     tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)
        #
        #                     tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
        #                     tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
        #                     tweet_all_abs_dev_var_l.append(np.var(abs_var_err))
        #
        #
        #
        #                     sum_rnd_abs_perc = 0
        #                     sum_rnd_perc = 0
        #                     for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
        #                         sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
        #                         sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
        #                     random_perc = np.abs(sum_rnd_perc / float(7))
        #                     random_abs_perc = sum_rnd_abs_perc / float(7)
        #
        #                     tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
        #                     tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
        #
        #                     tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
        #                     tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
        #
        #             gt_l = []
        #             pt_l = []
        #             disputability_l = []
        #             perc_l = []
        #             abs_perc_l = []
        #             # for t_id in tweet_l_sort:
        #             #     gt_l.append(tweet_gt_var[t_id])
        #             #     pt_l.append(tweet_avg[t_id])
        #             #     disputability_l.append(tweet_var[t_id])
        #             #     perc_l.append(tweet_dev_avg[t_id])
        #             #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
        #
        #
        #
        #             # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
        #             tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
        #             # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
        #             # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
        #             # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
        #             # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
        #
        #             # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
        #
        #
        #             if dataset == 'snopes':
        #                 data_addr = 'snopes'
        #             elif dataset == 'politifact':
        #                 data_addr = 'politifact/fig'
        #             elif dataset == 'mia':
        #                 data_addr = 'mia/fig'
        #
        #             count = 0
        #             outF.write(
        #                 '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
        #             # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
        #
        #             for t_id in tweet_l_sort:
        #                 count+=1
        #                 if balance_f=='balanced':
        #                     outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
        #                                + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
        #                                + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
        #                                str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
        #                                str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
        #                                str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
        #                                str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
        #                     # +
        #
        #                 else:
        #                     outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
        #                                # str(np.round(diff_group_disp_dict[t_id], 3)) +
        #                                '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
        #                                 + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
        #                                 '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
        #                                str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
        #                                str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
        #                                str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
        #                                str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
        #
        #
        #
        #
        #         if dataset == 'snopes':
        #             data_addr = 'snopes'
        #         elif dataset == 'politifact':
        #             data_addr = 'politifact/fig'
        #         elif dataset == 'mia':
        #             data_addr = 'mia/fig'
        #
        #         # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
        #         # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
        #         tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
        #         # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
        #         # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
        #         # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
        #         # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
        #
        #         # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
        #         # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
        #
        #         tweet_napb_dict_high_disp = {}
        #         tweet_napb_dict_low_disp = {}
        #         for t_id in tweet_l_sort[:20]:
        #             # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
        #             tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]
        #
        #         for t_id in tweet_l_sort[-20:]:
        #             # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
        #             tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]
        #
        #         kk = 0
        #
        #         for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
        #             if kk==0:
        #                 tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
        #             else:
        #                 tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)
        #
        #             kk+=1
        #             count = 0
        #             outF.write(
        #                 '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
        #             for t_id in tweet_l_sort:
        #                 count += 1
        #                 # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
        #                 if balance_f == 'balanced':
        #                     outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
        #                                + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
        #                                str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
        #                                str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
        #                                str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
        #                                str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
        #                                str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
        #                                str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
        #                     # +
        #                     #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
        #                     #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')
        #
        #                 else:
        #                     outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
        #                                str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
        #                                str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
        #                                str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
        #                                str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
        #                                str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
        #                                str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
        #                 # +
        #                 # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
        #                 # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_stastistics_top-bottom_accurate_readers":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []



        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        # for dataset in  ['snopes','politifact','mia']:
        for dataset in ['mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                    # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    #     tweet_lable_dict[tweet_id] = 'undecided'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                col = 'purple'
                marker = 'o'
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            elif dataset=='politifact':
                col = 'c'
                marker='>'
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                col = 'orange'
                marker='*'
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'


            inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(1) + '_final.csv'
            df_test = pd.read_csv(inp1, sep="\t")

            df_c = collections.defaultdict()
            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []
            user_acc_dict = {}
            total_tid_list = []
            f_part_tis_l = []
            s_part_tis_l = []
            t_list_dict = collections.defaultdict(list)
            df_f = {}
            df_s = {}
            user_acc_dict_f = {}
            user_acc_dict_s = {}
            for ind in ind_l:
                total_tid_list = []
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                t_list_dict=collections.defaultdict(list)
                df_m = df[ind].copy()
                t_i_list = list(set(df_m['tweet_id']))
                t_cat_dict = collections.defaultdict(list)

                # random.shuffle(t_i_list)
                # tmp_f = t_i_list[:int(len(t_i_list)/2)]
                # tmp_s = t_i_list[int(len(t_i_list)/2):]
                # f_part_tis_l +=tmp_f
                # s_part_tis_l += tmp_s


                for t_i in t_i_list:
                    cat = tweet_lable_dict[t_i]
                    t_cat_dict[cat].append(t_i)

                for cat in t_cat_dict:
                    t_tmp_list = list(set(t_cat_dict[cat]))
                    random.shuffle(t_tmp_list)
                    tmp_f = t_tmp_list[:int(len(t_tmp_list)/2)]
                    tmp_s = t_tmp_list[int(len(t_tmp_list)/2):]
                    f_part_tis_l +=tmp_f
                    s_part_tis_l += tmp_s





            for ind in ind_l:

                df[ind].loc[:, 'abs_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_abs_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'user_acc'] = df[ind]['tweet_id'] * 0.0
                df_m = df[ind].copy()

                df_s[ind] = df_m[df_m['tweet_id'].isin(s_part_tis_l)]
                df_f[ind] = df_m[df_m['tweet_id'].isin(f_part_tis_l)]


                groupby_ftr = 'tweet_id'
                grouped = df_f[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df_f[ind].groupby(groupby_ftr, sort=False).sum()


                for ind_t in df_f[ind].index.tolist():

                    t_id = df_f[ind]['tweet_id'][ind_t]
                    err = df_f[ind]['err'][ind_t]
                    abs_err = np.abs(err)
                    df_f[ind]['abs_err'][ind_t] = abs_err
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_f[ind]['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_f[ind]['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)


                    norm_err = err / float(random_perc)
                    norm_abs_err = abs_err / float(random_abs_perc)
                    df_f[ind]['norm_err'][ind_t] = norm_err
                    df_f[ind]['norm_abs_err'][ind_t] = norm_abs_err

                groupby_ftr = 'worker_id'
                grouped = df_f[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df_f[ind].groupby(groupby_ftr, sort=False).sum()

                for ind_w in df_f[ind].index.tolist():
                    w_id = df_f[ind]['worker_id'][ind_w]
                    df_tmp = df_f[ind][df_f[ind]['worker_id']==w_id]
                    w_acc_list_tmp = list(df_tmp['acc'])
                    acc_c = 0
                    nacc_c = 0
                    for el in w_acc_list_tmp:
                        if el == 0:
                            nacc_c += 1
                        elif el == 1:
                            acc_c += 1

                    w_ind_acc = (acc_c / float(acc_c + nacc_c))
                    df_f[ind]['user_acc'][ind_w] = w_ind_acc
                    user_acc_dict_f[w_id] = w_ind_acc
                # df[ind] = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_s[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df_s[ind].groupby(groupby_ftr, sort=False).sum()

                for ind_t in df_s[ind].index.tolist():

                    t_id = df_s[ind]['tweet_id'][ind_t]
                    err = df_s[ind]['err'][ind_t]
                    abs_err = np.abs(err)
                    df_s[ind]['abs_err'][ind_t] = abs_err
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2 / float(3), -1 / float(3), 0, 1 / float(3), 2 / float(3), 1]:
                        sum_rnd_perc += val - df_s[ind]['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_s[ind]['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    norm_err = err / float(random_perc)
                    norm_abs_err = abs_err / float(random_abs_perc)
                    df_s[ind]['norm_err'][ind_t] = norm_err
                    df_s[ind]['norm_abs_err'][ind_t] = norm_abs_err

                groupby_ftr = 'worker_id'
                grouped = df_s[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df_s[ind].groupby(groupby_ftr, sort=False).sum()

                for ind_w in df_s[ind].index.tolist():
                    w_id = df_s[ind]['worker_id'][ind_w]
                    df_tmp = df_s[ind][df_s[ind]['worker_id'] == w_id]
                    w_acc_list_tmp = list(df_tmp['acc'])
                    acc_c = 0
                    nacc_c = 0
                    for el in w_acc_list_tmp:
                        if el == 0:
                            nacc_c += 1
                        elif el == 1:
                            acc_c += 1

                    w_ind_acc = (acc_c / float(acc_c + nacc_c))
                    df_s[ind]['user_acc'][ind_w] = w_ind_acc
                    user_acc_dict_s[w_id] = w_ind_acc

            w_pt_avg_l = []
            w_err_avg_l = []
            w_abs_err_avg_l = []
            w_norm_err_avg_l = []
            w_norm_abs_err_avg_l = []
            w_acc_avg_l = []

            w_pt_std_l = []
            w_err_std_l = []
            w_abs_err_std_l = []
            w_norm_err_std_l = []
            w_norm_abs_err_std_l = []
            w_acc_std_l = []

            w_pt_avg_dict = collections.defaultdict()
            w_err_avg_dict = collections.defaultdict()
            w_abs_err_avg_dict = collections.defaultdict()
            w_norm_err_avg_dict = collections.defaultdict()
            w_norm_abs_err_avg_dict = collections.defaultdict()
            w_acc_avg_dict = collections.defaultdict()

            w_acc_std_dict = collections.defaultdict()

            all_w_pt_list  = []
            all_w_err_list = []
            all_w_abs_err_list = []
            all_w_norm_err_list = []
            all_w_norm_abs_err_list  = []
            all_w_acc_list = []

            all_w_cyn_list = []
            all_w_gull_list = []
            w_cyn_avg_l = []
            w_gull_avg_l = []
            w_cyn_std_l= []
            w_gull_std_l = []
            w_cyn_avg_dict =collections.defaultdict()
            w_gull_avg_dict =collections.defaultdict()
            w_cyn_std_dict =collections.defaultdict()
            w_gull_std_dict = collections.defaultdict()


            thr=0.1
            user_acc_l_sort_f = sorted(user_acc_dict_f, key=user_acc_dict_f.get, reverse=True)



            X = []
            Y = []
            thr_list = [0.1, 0.2, 0.3, 0.4, 0.5]#, 0.6, 0.7, 0.8, 0.9, 1]
            acc_top_list = []
            correlation_top_list = []
            w_acc_list_s = collections.defaultdict(list)
            count_thr = -1
            for thr in thr_list:#, 0.6, 0.7, 0.8, 0.9, 1]:
                count_thr+=1
                w_acc_list = []
                w_pt_avg_l = []
                w_err_avg_l = []
                w_abs_err_avg_l = []
                w_norm_err_avg_l = []
                w_norm_abs_err_avg_l = []
                w_acc_avg_l = []

                w_pt_std_l = []
                w_err_std_l = []
                w_abs_err_std_l = []
                w_norm_err_std_l = []
                w_norm_abs_err_std_l = []
                w_acc_std_l = []

                w_pt_avg_dict = collections.defaultdict()
                w_err_avg_dict = collections.defaultdict()
                w_abs_err_avg_dict = collections.defaultdict()
                w_norm_err_avg_dict = collections.defaultdict()
                w_norm_abs_err_avg_dict = collections.defaultdict()
                w_acc_avg_dict = collections.defaultdict()

                w_acc_std_dict = collections.defaultdict()

                all_w_pt_list = []
                all_w_err_list = []
                all_w_abs_err_list = []
                all_w_norm_err_list = []
                all_w_norm_abs_err_list = []
                all_w_acc_list = []

                all_w_cyn_list = []
                all_w_gull_list = []
                w_cyn_avg_l = []
                w_gull_avg_l = []
                w_cyn_std_l = []
                w_gull_std_l = []
                w_cyn_avg_dict = collections.defaultdict()
                w_gull_avg_dict = collections.defaultdict()
                w_cyn_std_dict = collections.defaultdict()
                w_gull_std_dict = collections.defaultdict()

                if count_thr==0:
                    top_acc_users_f = user_acc_l_sort_f[:int(thr_list[count_thr] * len(user_acc_l_sort_f))]
                else:
                    top_acc_users_f = user_acc_l_sort_f[
                                      int(thr_list[count_thr-1] * len(user_acc_l_sort_f)):int(thr_list[count_thr] * len(user_acc_l_sort_f))]

                # bottom_acc_users = user_acc_l_sort[int((1 - thr) * len(user_acc_l_sort)):]
                user_dem_att = collections.defaultdict()
                top_f = True
                # top_f = False
                if top_f == False:
                    top_f='bot'
                else:
                    top_f=''

                PT_l = []
                GT_l = []
                mplpl.rcParams['figure.figsize'] = 5.4, 3.8
                mplpl.rc('xtick', labelsize='x-large')
                mplpl.rc('ytick', labelsize='x-large')
                mplpl.rc('legend', fontsize='medium')
                for ww_id in top_acc_users_f:

                    w_acc =  user_acc_dict_s[ww_id]
                    w_acc_list_s[thr].append(w_acc)


                mplpl.scatter([thr]*len(w_acc_list_s[thr]),w_acc_list_s[thr], s=40,color=col, marker=marker)#, label=data_name)
                mplpl.scatter(thr, np.mean(w_acc_list_s[thr]), s=200, color='g', marker=marker)
                Y.append(np.mean(w_acc_list_s[thr]))
                X.append(thr)

        mplpl.plot(X, Y, c='g', linewidth=4, label='Average')
        mplpl.rc('xtick', labelsize='large')
        mplpl.rc('ytick', labelsize='large')
        # mplpl.rc('xtick.major', size=3, pad=3)
        # mplpl.rc('xtick.minor', size=2, pad=3)
        mplpl.rc('legend', fontsize='large')
        mplpl.xlabel('Top k fraction of users ranked by accuracy',fontsize=14.8, fontweight='bold')
        mplpl.ylabel('Users accuracy',fontsize=16, fontweight='bold')
        mplpl.title(data_name,fontsize = 'x-large')

        mplpl.xlim([0.05, .55])
        mplpl.ylim([0, 1])
        mplpl.grid()
        mplpl.subplots_adjust(bottom=0.24)
        mplpl.subplots_adjust(left=0.18)        #
        mplpl.legend(loc="lower left",fontsize = 'large')

        pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/user_based/experts_crowds_'+data_name
        mplpl.savefig(pp + '.pdf', format='pdf')
        mplpl.savefig(pp + '.png', format='png')

    if args.t == "AMT_dataset_reliable_user-level_processing_all_dataset_weighted_stastistics_top-bottom_accurate_readers_together":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []



        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','politifact','mia']:
        # for dataset in ['mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                    # if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    #     tweet_lable_dict[tweet_id] = 'undecided'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                col = 'purple'
                marker = 'o'
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            elif dataset=='politifact':
                col = 'c'
                marker='>'
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                col = 'orange'
                marker='*'
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'


            inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(1) + '_final.csv'
            df_test = pd.read_csv(inp1, sep="\t")

            df_c = collections.defaultdict()
            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []
            user_acc_dict = {}
            total_tid_list = []
            f_part_tis_l = []
            s_part_tis_l = []
            t_list_dict = collections.defaultdict(list)
            df_f = {}
            df_s = {}
            user_acc_dict_f = {}
            user_acc_dict_s = {}
            for ind in ind_l:
                total_tid_list = []
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                t_list_dict=collections.defaultdict(list)
                df_m = df[ind].copy()
                t_i_list = list(set(df_m['tweet_id']))
                t_cat_dict = collections.defaultdict(list)
                for t_i in t_i_list:
                    cat = tweet_lable_dict[t_i]
                    t_cat_dict[cat].append(t_i)

                for cat in t_cat_dict:
                    t_tmp_list = list(set(t_cat_dict[cat]))
                    random.shuffle(t_tmp_list)
                    tmp_f = t_tmp_list[:int(len(t_tmp_list)/2)]
                    tmp_s = t_tmp_list[int(len(t_tmp_list)/2):]
                    if dataset=='mia':
                        f_part_tis_l +=tmp_s
                        s_part_tis_l += tmp_f
                    else:
                        f_part_tis_l +=tmp_f
                        s_part_tis_l += tmp_s





            for ind in ind_l:

                df[ind].loc[:, 'abs_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'norm_abs_err'] = df[ind]['tweet_id'] * 0.0
                df[ind].loc[:, 'user_acc'] = df[ind]['tweet_id'] * 0.0
                df_m = df[ind].copy()

                df_s[ind] = df_m[df_m['tweet_id'].isin(s_part_tis_l)]
                df_f[ind] = df_m[df_m['tweet_id'].isin(f_part_tis_l)]


                groupby_ftr = 'tweet_id'
                grouped = df_f[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df_f[ind].groupby(groupby_ftr, sort=False).sum()


                for ind_t in df_f[ind].index.tolist():

                    t_id = df_f[ind]['tweet_id'][ind_t]
                    err = df_f[ind]['err'][ind_t]
                    abs_err = np.abs(err)
                    df_f[ind]['abs_err'][ind_t] = abs_err
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_f[ind]['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_f[ind]['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)


                    norm_err = err / float(random_perc)
                    norm_abs_err = abs_err / float(random_abs_perc)
                    df_f[ind]['norm_err'][ind_t] = norm_err
                    df_f[ind]['norm_abs_err'][ind_t] = norm_abs_err

                groupby_ftr = 'worker_id'
                grouped = df_f[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df_f[ind].groupby(groupby_ftr, sort=False).sum()

                for ind_w in df_f[ind].index.tolist():
                    w_id = df_f[ind]['worker_id'][ind_w]
                    df_tmp = df_f[ind][df_f[ind]['worker_id']==w_id]
                    w_acc_list_tmp = list(df_tmp['acc'])
                    acc_c = 0
                    nacc_c = 0
                    for el in w_acc_list_tmp:
                        if el == 0:
                            nacc_c += 1
                        elif el == 1:
                            acc_c += 1

                    w_ind_acc = (acc_c / float(acc_c + nacc_c))
                    df_f[ind]['user_acc'][ind_w] = w_ind_acc
                    user_acc_dict_f[w_id] = w_ind_acc
                # df[ind] = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_s[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df_s[ind].groupby(groupby_ftr, sort=False).sum()

                for ind_t in df_s[ind].index.tolist():

                    t_id = df_s[ind]['tweet_id'][ind_t]
                    err = df_s[ind]['err'][ind_t]
                    abs_err = np.abs(err)
                    df_s[ind]['abs_err'][ind_t] = abs_err
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2 / float(3), -1 / float(3), 0, 1 / float(3), 2 / float(3), 1]:
                        sum_rnd_perc += val - df_s[ind]['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_s[ind]['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    norm_err = err / float(random_perc)
                    norm_abs_err = abs_err / float(random_abs_perc)
                    df_s[ind]['norm_err'][ind_t] = norm_err
                    df_s[ind]['norm_abs_err'][ind_t] = norm_abs_err

                groupby_ftr = 'worker_id'
                grouped = df_s[ind].groupby(groupby_ftr, sort=False)
                grouped_sum = df_s[ind].groupby(groupby_ftr, sort=False).sum()

                for ind_w in df_s[ind].index.tolist():
                    w_id = df_s[ind]['worker_id'][ind_w]
                    df_tmp = df_s[ind][df_s[ind]['worker_id'] == w_id]
                    w_acc_list_tmp = list(df_tmp['acc'])
                    acc_c = 0
                    nacc_c = 0
                    for el in w_acc_list_tmp:
                        if el == 0:
                            nacc_c += 1
                        elif el == 1:
                            acc_c += 1

                    w_ind_acc = (acc_c / float(acc_c + nacc_c))
                    df_s[ind]['user_acc'][ind_w] = w_ind_acc
                    user_acc_dict_s[w_id] = w_ind_acc

            w_pt_avg_l = []
            w_err_avg_l = []
            w_abs_err_avg_l = []
            w_norm_err_avg_l = []
            w_norm_abs_err_avg_l = []
            w_acc_avg_l = []

            w_pt_std_l = []
            w_err_std_l = []
            w_abs_err_std_l = []
            w_norm_err_std_l = []
            w_norm_abs_err_std_l = []
            w_acc_std_l = []

            w_pt_avg_dict = collections.defaultdict()
            w_err_avg_dict = collections.defaultdict()
            w_abs_err_avg_dict = collections.defaultdict()
            w_norm_err_avg_dict = collections.defaultdict()
            w_norm_abs_err_avg_dict = collections.defaultdict()
            w_acc_avg_dict = collections.defaultdict()

            w_acc_std_dict = collections.defaultdict()

            all_w_pt_list  = []
            all_w_err_list = []
            all_w_abs_err_list = []
            all_w_norm_err_list = []
            all_w_norm_abs_err_list  = []
            all_w_acc_list = []

            all_w_cyn_list = []
            all_w_gull_list = []
            w_cyn_avg_l = []
            w_gull_avg_l = []
            w_cyn_std_l= []
            w_gull_std_l = []
            w_cyn_avg_dict =collections.defaultdict()
            w_gull_avg_dict =collections.defaultdict()
            w_cyn_std_dict =collections.defaultdict()
            w_gull_std_dict = collections.defaultdict()


            thr=0.1
            user_acc_l_sort_f = sorted(user_acc_dict_f, key=user_acc_dict_f.get, reverse=True)



            X = []
            Y = []
            thr_list = [0.1, 0.2, 0.3, 0.4, 0.5]#, 0.6, 0.7, 0.8, 0.9, 1]
            acc_top_list = []
            correlation_top_list = []
            w_acc_list_s = collections.defaultdict(list)
            count_thr = -1
            for thr in thr_list:#, 0.6, 0.7, 0.8, 0.9, 1]:
                count_thr+=1
                w_acc_list = []
                w_pt_avg_l = []
                w_err_avg_l = []
                w_abs_err_avg_l = []
                w_norm_err_avg_l = []
                w_norm_abs_err_avg_l = []
                w_acc_avg_l = []

                w_pt_std_l = []
                w_err_std_l = []
                w_abs_err_std_l = []
                w_norm_err_std_l = []
                w_norm_abs_err_std_l = []
                w_acc_std_l = []

                w_pt_avg_dict = collections.defaultdict()
                w_err_avg_dict = collections.defaultdict()
                w_abs_err_avg_dict = collections.defaultdict()
                w_norm_err_avg_dict = collections.defaultdict()
                w_norm_abs_err_avg_dict = collections.defaultdict()
                w_acc_avg_dict = collections.defaultdict()

                w_acc_std_dict = collections.defaultdict()

                all_w_pt_list = []
                all_w_err_list = []
                all_w_abs_err_list = []
                all_w_norm_err_list = []
                all_w_norm_abs_err_list = []
                all_w_acc_list = []

                all_w_cyn_list = []
                all_w_gull_list = []
                w_cyn_avg_l = []
                w_gull_avg_l = []
                w_cyn_std_l = []
                w_gull_std_l = []
                w_cyn_avg_dict = collections.defaultdict()
                w_gull_avg_dict = collections.defaultdict()
                w_cyn_std_dict = collections.defaultdict()
                w_gull_std_dict = collections.defaultdict()

                if count_thr==0:
                    top_acc_users_f = user_acc_l_sort_f[:int(thr_list[count_thr] * len(user_acc_l_sort_f))]
                else:
                    top_acc_users_f = user_acc_l_sort_f[
                                      int(thr_list[count_thr-1] * len(user_acc_l_sort_f)):int(thr_list[count_thr] * len(user_acc_l_sort_f))]
                # top_acc_users_f = user_acc_l_sort_f[:int(thr_list[count_thr] * len(user_acc_l_sort_f))]
                # bottom_acc_users = user_acc_l_sort[int((1 - thr) * len(user_acc_l_sort)):]
                user_dem_att = collections.defaultdict()
                top_f = True
                # top_f = False
                if top_f == False:
                    top_f='bot'
                else:
                    top_f=''

                PT_l = []
                GT_l = []
                mplpl.rcParams['figure.figsize'] = 5.4, 3.8
                mplpl.rc('xtick', labelsize='x-large')
                mplpl.rc('ytick', labelsize='x-large')
                mplpl.rc('legend', fontsize='medium')
                for ww_id in top_acc_users_f:

                    w_acc =  user_acc_dict_s[ww_id]
                    w_acc_list_s[thr].append(w_acc)


                # mplpl.scatter([thr]*len(w_acc_list_s[thr]),w_acc_list_s[thr], s=40,color=col, marker=marker)#, label=data_name)
                mplpl.scatter(thr, np.mean(w_acc_list_s[thr]), s=100, color=col, marker=marker)
                Y.append(np.mean(w_acc_list_s[thr]))
                X.append(thr)

            mplpl.plot(X, Y, c=col, linewidth=3, label=data_name)
        mplpl.rc('xtick', labelsize='large')
        mplpl.rc('ytick', labelsize='large')
        # mplpl.rc('xtick.major', size=3, pad=3)
        # mplpl.rc('xtick.minor', size=2, pad=3)
        mplpl.rc('legend', fontsize='large')
        mplpl.xlabel('Top k fraction of users ranked by accuracy',fontsize=14.8, fontweight='bold')
        mplpl.ylabel('Average users accuracy',fontsize=16, fontweight='bold')
        # mplpl.title(data_name,fontsize = 'large')

        mplpl.xlim([0.05, .55])
        mplpl.ylim([0, 1])
        mplpl.grid()
        mplpl.subplots_adjust(bottom=0.24)
        mplpl.subplots_adjust(left=0.18)        #
        mplpl.legend(loc="lower left",fontsize = 'large')

        pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/user_based/experts_crowds_together'
        mplpl.savefig(pp + '.pdf', format='pdf')
        mplpl.savefig(pp + '.png', format='png')


    # legend_properties = {'weight': 'bold'}
    #
    # #
    # mplpl.ylabel('CDF', fontsize=20, fontweight='bold')
    # mplpl.xlabel('Mean Perception Bias', fontsize=20, fontweight='bold')
    # mplpl.legend(loc="upper left", prop=legend_properties, fontsize='medium', ncol=1)
    # # mplpl.title(data_name)
    # # mplpl.legend(loc="upper left",fontsize = 'large')
    # mplpl.xlim([-1.5, 1.5])
    # mplpl.ylim([0, 1])
    # mplpl.grid()
    # mplpl.subplots_adjust(bottom=0.24)
    # mplpl.subplots_adjust(left=0.18)
    # # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/NAPB_cdf_alldataset'
    # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/MPB_cdf_alldataset'
    # mplpl.savefig(pp + '.pdf', format='pdf')
    # mplpl.savefig(pp + '.png', format='png')

    if args.t == "AMT_dataset_reliable_processing_all_dataset_weighted_stastistics_top-disputability-NAPB":



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1


            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'
                if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                    tweet_lable_dict[tweet_id] = 'undecided'

        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []



        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'

                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                #     if int(tweet_id) in [100012, 100016, 100053, 100038, 100048]:
                #         tweet_lable_dict[tweet_id] = 'undecided'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
                news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
                news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')






            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
                col  ='purple'
                marker ='o'

            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
                col  = 'c'
                marker = '^'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors/Non-Rumors'
                col = 'orange'
                marker ='*'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_diff_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()
            tweet_kldiv_group = collections.defaultdict()

            tweet_chi_group = collections.defaultdict()
            tweet_chi_group_1 = collections.defaultdict()
            tweet_chi_group_2 = collections.defaultdict()



            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []

            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []


                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)

                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)


                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []



                    # n_dem, bins, patches = Plab.hist(dem_val_list,
                    #                                      bins=frange(0, 1, 0.05), normed=1)
                    # n_rep, bins, patches = Plab.hist(rep_val_list,
                    #                                      bins=frange(0, 1, 0.05), normed=1)
                    # n_neut, bins, patches = Plab.hist(neut_val_list,
                    #                                   bins=frange(0, 1, 0.05), normed=1)
                    #
                    # dem_rep_chi = chi_sqr(n_dem, n_rep)
                    # dem_neut_chi = chi_sqr(n_dem, n_neut)
                    # neut_rep_chi = chi_sqr(n_neut, n_rep)



                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_diff_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) + np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)

                    # if ~(dem_rep_chi > 0 or dem_rep_chi <0 or dem_rep_chi ==0):
                    #     tweet_chi_group[t_id] = 0
                    # else:
                    #     tweet_chi_group[t_id] = dem_rep_chi
                    #
                    # if ~(dem_neut_chi > 0 or dem_neut_chi <0 or dem_neut_chi ==0):
                    #     tweet_chi_group_1[t_id] = 0
                    # else:
                    #     tweet_chi_group_1[t_id] = dem_neut_chi
                    #
                    # if ~(dem_neut_chi > 0 or dem_neut_chi <0 or dem_neut_chi ==0):
                    #     tweet_chi_group_2[t_id] = 0
                    # else:
                    #     tweet_chi_group_2[t_id] = dem_rep_chi


                    # tweet_chi_group[t_id] = np.var([tweet_chi_group[t_id], tweet_chi_group_1[t_id], tweet_chi_group_2[t_id]])


                    tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)






            # tweet_abs_perc_rnd_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
            # tweet_perc_rnd_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
            # tweet_abs_perc_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
            # tweet_perc_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)


            out_dict = tweet_chi_group
            out_dict = tweet_abs_dev_avg_rnd
            tweet_disp_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
            # tweet_disp_sort = sorted(out_dict, key=out_dict.get, reverse=True)

            # exit()
            thr_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
            acc_top_list = []

            gt_l = []
            pt_l = []
            disputability_l = []
            perc_l = []
            abs_perc_l = []
            abs_perc_rnd_l = []
            perc_rnd_l = []
            tweet_skew_ll = []
            thr = 5
            prec_rnd_acc = 0
            abs_prec_rnd_acc = 0
            norm_abs_perc_bias_l = []
            abs_perc_bias_l = []
            norm_abs_perc_bias_ll = []
            abs_perc_bias_ll = []
            k_l = []

            # thr
            out_dict = tweet_abs_dev_avg_rnd
            for i in range(int(len(tweet_disp_sort)/float(thr))):
                k = (i+1)*thr
                perc_rnd_l = []
                abs_perc_rnd_l = []
                disputability_l = []
                above_avg = 0
                less_avg = 0
                above_avg_rnd = 0
                less_avg_rnd = 0
                above_avg = 0
                less_avg = 0
                for j in range(k):

                    perc_rnd_l.append(out_dict[tweet_disp_sort[j]])
                    abs_perc_rnd_l.append(out_dict[tweet_disp_sort[j]])

                    if out_dict[tweet_disp_sort[j]] > np.mean(out_dict.values()):
                        above_avg_rnd+=1
                    elif out_dict[tweet_disp_sort[j]] < np.mean(out_dict.values()):
                        less_avg_rnd+=1


                    if tweet_abs_dev_avg[tweet_disp_sort[j]] > np.mean(tweet_abs_dev_avg.values()):
                        above_avg+=1
                    elif tweet_abs_dev_avg[tweet_disp_sort[j]] < np.mean(tweet_abs_dev_avg.values()):
                        less_avg+=1

                norm_abs_perc_bias_ll.append(np.mean(abs_perc_rnd_l))
                abs_perc_bias_ll.append(np.mean(perc_rnd_l))

                norm_abs_perc_bias_l.append(above_avg_rnd/float(above_avg_rnd+less_avg_rnd))
                abs_perc_bias_l.append(above_avg/float(above_avg+less_avg))
                k_l.append(np.round(k/float(len(tweet_disp_sort)), 3))

            # mplpl.scatter(k_l,norm_abs_perc_bias_l ,  s=30,color='c',marker='o', label='Normalized absolute perception bias(NAPB)')
            # mplpl.plot(k_l, norm_abs_perc_bias_l, color='c')
            # # mplpl.xlim([-.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their NAPB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_nabs_perception'
            # # mplpl.savefig(pp, format='png')
            # # mplpl.figure()
            # #
            # #
            # # mplpl.scatter(k_l,abs_perc_bias_l ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # # mplpl.plot(k_l, abs_perc_bias_l, color='g')
            # #
            # #
            # # mplpl.xlim([-0.02, 1.02])
            # # mplpl.ylim([0, 1.02])
            # # mplpl.ylabel('Fraction of news that their APB \n is bigger than avg', fontsize=18)
            # # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)
            # #
            # # mplpl.legend(loc="lower right")
            # #
            # # mplpl.grid()
            # # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg.values())),4)))
            # # pp = remotedir + '/fig/fig_exp1/news_based/initial/' + data_n + '_disput_abs_perception'
            # # mplpl.savefig(pp, format='png')
            # #
            # # mplpl.figure()
            # #
            # #
            # #
            # #
            mplpl.scatter(k_l,norm_abs_perc_bias_ll ,  s=80,color=col,marker=marker, label=data_name)
            mplpl.plot(k_l, norm_abs_perc_bias_ll, color=col, linewidth=3.0)




            # mplpl.scatter(k_l,abs_perc_bias_ll ,  s=30,color='g',marker='o', label='Absolute perception bias(APB)')
            # mplpl.plot(k_l, abs_perc_bias_ll, color='g')


            # mplpl.xlim([-0.02, 1.02])
            # mplpl.ylim([0.5, 1.02])
            # mplpl.ylabel('Avg APB of news stories', fontsize=18)
            # mplpl.xlabel('K top fraction of news ranked based on disputability', fontsize=18)

        mplpl.xlim([-.02, 1.02])
        mplpl.ylim([0.5, 1.05])
        mplpl.ylabel('Average NAPB of news stories', fontsize=18)
        mplpl.xlabel('Top k fraction of news ranked by disputability', fontsize=18)
        mplpl.legend(loc="lower right")

        mplpl.grid()
        # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean((tweet_abs_dev_avg_rnd.values())), 4)))
        # pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/disput_avg-nabs_perception_balanced'
        pp = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/fig/news_based/disput_avg-nabs_perception.pdf'
        mplpl.savefig(pp, format='pdf')
        mplpl.figure()






                    # w_pt_std_dict[w_id] = np.std(w_pt_list)
                    # w_err_std_dict[w_id] = np.std(w_err_list)
                    # w_abs_err_std_dict[w_id] = np.std(w_abs_err_list)
                    # w_norm_err_std_dict[w_id] = np.std(w_norm_err_list)
                    # w_norm_abs_err_std_dict[w_id] = np.std(w_norm_abs_err_list)
                    # w_cyn_std_dict[w_id] = np.std(w_cyn_list)
                    # w_gull_std_dict[w_id] = np.std(w_gull_list)
                    # w_acc_std_dict[w_id] = np.std(w_acc_list)







            ##################################################



        #         fig_f = True
        #         # fig_f = False
        #         # fig_f_1 = True
        #         fig_f_1 = False
        #         if fig_f==True:
        #
        #         # df_tmp = pd.DataFrame({'val' : all_w_acc_list})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
        #         # # mplpl.hist(list(df_tmp['val']), normed=1, color='g')
        #         #
        #         #
        #         # mplpl.xlim([-1.5, 1.5])
        #         # mplpl.ylim([0, 1.5])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Readers accuracy', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_acc_list),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) +'_acc_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_acc_density| alt text| width = 500px}}')
        #         #
        #         #
        #         #
        #         # # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
        #         # df_tmp = pd.DataFrame({'val' : w_acc_avg_l})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
        #         # mplpl.hist(list(df_tmp['val']), normed=1, color=col)
        #         #
        #         #
        #         # # mplpl.plot(gt_set, pt_mean,  color='k')
        #         # mplpl.xlim([-1.5, 1.5])
        #         # mplpl.ylim([0, 5])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Individual readers accuracy', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_acc_avg_l),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_acc_pt'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_acc_pt| alt text| width = 500px}}')
        #         #
        #         #
        #         #
        #         #
        #         # df_tmp = pd.DataFrame({'val' : all_w_pt_list})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
        #         # # mplpl.hist(list(df_tmp['val']), normed=1, color='g')
        #         #
        #         #
        #         # mplpl.xlim([-1.5, 1.5])
        #         # mplpl.ylim([0, 1.5])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Readers percevied truth value', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(all_w_pt_list),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) +'_pt_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_pt_density| alt text| width = 500px}}')
        #         #
        #         #
        #         #
        #         #
        #         # # df_tmp = pd.DataFrame(w_pt_avg_l, col=['val'])
        #         # df_tmp = pd.DataFrame({'val' : w_pt_avg_l})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # # mplpl.hist(list(df_tmp['val']), weights=weights, color=col)
        #         # mplpl.hist(list(df_tmp['val']), normed=1, color=col)
        #         #
        #         #
        #         # # mplpl.plot(gt_set, pt_mean,  color='k')
        #         # mplpl.xlim([-1.2, 1.2])
        #         # mplpl.ylim([0, 3.5])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Individual readers percevied truth value', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_pt_avg_l),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_pt_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_pt_density| alt text| width = 500px}}')
        #         #
        #         #
        #         #
        #         #
        #         # df_tmp = pd.DataFrame({'val' : w_err_avg_l})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # mplpl.hist(list(df_tmp['val']), normed=1, color=col)
        #         #
        #         #
        #         # mplpl.xlim([-1.2, 1.2])
        #         # mplpl.ylim([0, 3.5])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Individual readers perception bias value', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_err_avg_l),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_pb_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_pb_density| alt text| width = 500px}}')
        #         #
        #         #
        #         #
        #         # df_tmp = pd.DataFrame({'val' : w_abs_err_avg_l})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # mplpl.hist(list(df_tmp['val']), normed=1, color=col)
        #         #
        #         # mplpl.xlim([0, 1.2])
        #         # mplpl.ylim([0, 5])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Individual readers absolute perception bias value', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_abs_err_avg_l),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_apb_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_apb_density| alt text| width = 500px}}||')
        #         #
        #         #
        #         #
        #         # df_tmp = pd.DataFrame({'val' : w_gull_avg_l})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # mplpl.hist(list(df_tmp['val']), normed=1, color=col)
        #         #
        #         # mplpl.xlim([0, 1.2])
        #         # mplpl.ylim([0, 10])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Individual readers gullibility value', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_gull_avg_l),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_gull_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_gull_density| alt text| width = 500px}}||')
        #         #
        #         #
        #         #
        #         #
        #         # df_tmp = pd.DataFrame({'val' : w_cyn_avg_l})
        #         # weights = []
        #         # weights.append(np.ones_like(list(df_tmp['val'])) / float(len(df_tmp)))
        #         # col_r = 'm'
        #         # try:
        #         #     df_tmp['val'].plot(kind='kde', lw=4, color=col_r)
        #         # except:
        #         #     print('hmm')
        #         #
        #         # mplpl.hist(list(df_tmp['val']), normed=1, color=col)
        #         #
        #         # mplpl.xlim([0, 1.2])
        #         # mplpl.ylim([0, 10])
        #         # mplpl.ylabel('Density', fontsize=18)
        #         # mplpl.xlabel('Individual readers cynicallity value', fontsize=18)
        #         # mplpl.title(data_name + ' , avg : ' + str(np.round(np.mean(w_cyn_avg_l),4)))
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/demographic/' + data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_cyn_density'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/demographic/'+ data_n + '_'+str(feat)+'_'+str(feat_cat) + '_avg_cyn_density| alt text| width = 500px}}||\n\n')
        #
        #
        #             tweet_l_sort = sorted(w_pt_avg_dict, key=w_pt_avg_dict.get, reverse=False)
        #             pt_l = []
        #             for t_id in tweet_l_sort:
        #                 pt_l.append(w_pt_avg_dict[t_id])
        #
        #
        #             tweet_l_sort = sorted(w_acc_avg_dict, key=w_acc_avg_dict.get, reverse=False)
        #             acc_l = []
        #             for t_id in tweet_l_sort:
        #                 acc_l.append(w_acc_avg_dict[t_id])
        #
        #
        #             tweet_l_sort = sorted(w_err_avg_dict, key=w_err_avg_dict.get, reverse=False)
        #             err_l = []
        #             for t_id in tweet_l_sort:
        #                 err_l.append(w_err_avg_dict[t_id])
        #
        #
        #             tweet_l_sort = sorted(w_abs_err_avg_dict, key=w_abs_err_avg_dict.get, reverse=False)
        #             abs_err_l = []
        #             for t_id in tweet_l_sort:
        #                 abs_err_l.append(w_abs_err_avg_dict[t_id])
        #
        #
        #             tweet_l_sort = sorted(w_gull_avg_dict, key=w_gull_avg_dict.get, reverse=False)
        #             gull_l = []
        #             for t_id in tweet_l_sort:
        #                 gull_l.append(w_gull_avg_dict[t_id])
        #
        #
        #             tweet_l_sort = sorted(w_cyn_avg_dict, key=w_cyn_avg_dict.get, reverse=False)
        #             cyn_l = []
        #             for t_id in tweet_l_sort:
        #                 cyn_l.append(w_cyn_avg_dict[t_id])
        #
        #             #####################################################33
        #
        #
        #             # num_bins = len(pt_l)
        #             # counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
        #             # cdf = np.cumsum(counts)
        #             # scale = 1.0 / cdf[-1]
        #             # ncdf = scale * cdf
        #             # mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='')
        #             # mplpl.ylabel('CDF', fontsize=18)
        #             # mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
        #             # mplpl.grid()
        #             # mplpl.title(data_name)
        #             # #         mplpl.legend(loc="upper right")
        #             # mplpl.xlim([-1, 1])
        #             # mplpl.ylim([0, 1])
        #             # pp = remotedir + '/fig/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f  + '_' + str(thr)+'_user_pt_cdf'
        #             # mplpl.savefig(pp, format='png')
        #             # mplpl.figure()
        #             #
        #             # outF.write(
        #             #     '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/top_bot_acc/' + data_n+ '_'  + top_f+ '_' + str(thr)+ '_user_pt_cdf| alt text| width = 500px}}')
        #             #
        #             # num_bins = len(acc_l)
        #             # counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #             # cdf = np.cumsum(counts)
        #             # scale = 1.0 / cdf[-1]
        #             # ncdf = scale * cdf
        #             # mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='')
        #             # mplpl.ylabel('CDF', fontsize=18)
        #             # mplpl.xlabel('Accuracy', fontsize=18)
        #             # mplpl.title(data_name)
        #             # #         mplpl.legend(loc="upper right")
        #             # mplpl.xlim([0, 1])
        #             # mplpl.ylim([0, 1])
        #             # mplpl.grid()
        #             # pp = remotedir + '/fig/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f +'_' + str(thr) + '_user_acc_cdf'
        #             # mplpl.savefig(pp, format='png')
        #             # mplpl.figure()
        #             # outF.write(
        #             #     '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f+ '_' + str(thr) + '_user_acc_cdf| alt text| width = 500px}}')
        #             #
        #             # num_bins = len(err_l)
        #             # counts, bin_edges = np.histogram(err_l, bins=num_bins, normed=True)
        #             # cdf = np.cumsum(counts)
        #             # scale = 1.0 / cdf[-1]
        #             # ncdf = scale * cdf
        #             # mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='')
        #             # mplpl.ylabel('CDF', fontsize=18)
        #             # mplpl.xlabel('Perception bias (PB)', fontsize=18)
        #             # mplpl.title(data_name)
        #             # #         mplpl.legend(loc="upper right")
        #             # mplpl.xlim([-1, 1])
        #             # mplpl.ylim([0, 1])
        #             # mplpl.grid()
        #             # pp = remotedir + '/fig/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f+'_' + str(thr) + '_user_err_cdf'
        #             # mplpl.savefig(pp, format='png')
        #             # mplpl.figure()
        #             #
        #             # outF.write(
        #             #     '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f +'_' + str(thr) + '_user_err_cdf| alt text| width = 500px}}')
        #             #
        #             # num_bins = len(abs_err_l)
        #             # counts, bin_edges = np.histogram(abs_err_l, bins=num_bins, normed=True)
        #             # cdf = np.cumsum(counts)
        #             # scale = 1.0 / cdf[-1]
        #             # ncdf = scale * cdf
        #             # mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='All users')
        #             # mplpl.ylabel('CDF', fontsize=18)
        #             # mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
        #             # mplpl.title(data_name)
        #             # #         mplpl.legend(loc="upper right")
        #             # mplpl.xlim([0, 1])
        #             # mplpl.ylim([0, 1])
        #             # mplpl.grid()
        #             # pp = remotedir + '/fig/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f +'_' + str(thr) + '_user_abs-err_cdf'
        #             # mplpl.savefig(pp, format='png')
        #             # mplpl.figure()
        #             # outF.write(
        #             #     '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f+ '_' + str(thr) + '_user_abs-err_cdf| alt text| width = 500px}}')
        #             #
        #             # # outF.write('|| Table ||\n\n')
        #             #
        #             # num_bins = len(gull_l)
        #             # counts, bin_edges = np.histogram(gull_l, bins=num_bins, normed=True)
        #             # cdf = np.cumsum(counts)
        #             # scale = 1.0 / cdf[-1]
        #             # ncdf = scale * cdf
        #             # mplpl.plot(bin_edges[1:], ncdf, c='m', lw=5, label='')
        #             # mplpl.ylabel('CDF', fontsize=18)
        #             # mplpl.xlabel('Gullibility', fontsize=18)
        #             # mplpl.grid()
        #             # mplpl.title(data_name)
        #             # #         mplpl.legend(loc="upper right")
        #             # mplpl.xlim([0, 1])
        #             # mplpl.ylim([0, 1])
        #             # pp = remotedir + '/fig/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f +'_' + str(thr) + '_user_gull_cdf'
        #             # mplpl.savefig(pp, format='png')
        #             # mplpl.figure()
        #             #
        #             # outF.write(
        #             #     '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f +'_' + str(thr) + '_user_gull_cdf| alt text| width = 500px}}')
        #             #
        #             # num_bins = len(cyn_l)
        #             # counts, bin_edges = np.histogram(cyn_l, bins=num_bins, normed=True)
        #             # cdf = np.cumsum(counts)
        #             # scale = 1.0 / cdf[-1]
        #             # ncdf = scale * cdf
        #             # mplpl.plot(bin_edges[1:], ncdf, c='k', lw=5, label='')
        #             # mplpl.ylabel('CDF', fontsize=18)
        #             # mplpl.xlabel('Cynicality', fontsize=18)
        #             # mplpl.grid()
        #             # mplpl.title(data_name)
        #             # #         mplpl.legend(loc="upper right")
        #             # mplpl.xlim([0, 1])
        #             # mplpl.ylim([0, 1])
        #             # pp = remotedir + '/fig/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f+ '_' + str(thr) + '_user_cyn_cdf'
        #             # mplpl.savefig(pp, format='png')
        #             # mplpl.figure()
        #             #
        #             # outF.write(
        #             #     '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/top_bot_acc/' + data_n + '_'+ top_f+ '_' + str(thr) + '_user_cyn_cdf| alt text| width = 500px}}||\n')
        #
        #
        #
        #
        #
        #         ########################
        #
        # #####################################################33
        #
        #         #
        #         # num_bins = len(pt_l)
        #         # counts, bin_edges = np.histogram(pt_l, bins=num_bins, normed=True)
        #         # cdf = np.cumsum(counts)
        #         # scale = 1.0 / cdf[-1]
        #         # ncdf = scale * cdf
        #         # mplpl.plot(bin_edges[1:], ncdf, c='c', lw=5, label='')
        #         # mplpl.ylabel('CDF', fontsize=18)
        #         # mplpl.xlabel('Perception truth value (PTL)', fontsize=18)
        #         # mplpl.grid()
        #         # mplpl.title(data_name)
        #         # #         mplpl.legend(loc="upper right")
        #         # mplpl.xlim([-1, 1])
        #         # mplpl.ylim([0, 1])
        #         # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_pt_cdf'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_pt_cdf| alt text| width = 500px}}')
        #         #
        #         #
        #         # num_bins = len(acc_l)
        #         # counts, bin_edges = np.histogram(acc_l, bins=num_bins, normed=True)
        #         # cdf = np.cumsum(counts)
        #         # scale = 1.0 / cdf[-1]
        #         # ncdf = scale * cdf
        #         # mplpl.plot(bin_edges[1:], ncdf, c='g', lw=5, label='')
        #         # mplpl.ylabel('CDF', fontsize=18)
        #         # mplpl.xlabel('Accuracy', fontsize=18)
        #         # mplpl.title(data_name)
        #         # #         mplpl.legend(loc="upper right")
        #         # mplpl.xlim([-1, 1])
        #         # mplpl.ylim([0, 1])
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_acc_cdf'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_acc_cdf| alt text| width = 500px}}')
        #         #
        #         #
        #         # num_bins = len(err_l)
        #         # counts, bin_edges = np.histogram(err_l, bins=num_bins, normed=True)
        #         # cdf = np.cumsum(counts)
        #         # scale = 1.0 / cdf[-1]
        #         # ncdf = scale * cdf
        #         # mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='')
        #         # mplpl.ylabel('CDF', fontsize=18)
        #         # mplpl.xlabel('Perception bias (PB)', fontsize=18)
        #         # mplpl.title(data_name)
        #         # #         mplpl.legend(loc="upper right")
        #         # mplpl.xlim([-1, 1])
        #         # mplpl.ylim([0, 1])
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_err_cdf'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_err_cdf| alt text| width = 500px}}')
        #         #
        #         # num_bins = len(abs_err_l)
        #         # counts, bin_edges = np.histogram(abs_err_l, bins=num_bins, normed=True)
        #         # cdf = np.cumsum(counts)
        #         # scale = 1.0 / cdf[-1]
        #         # ncdf = scale * cdf
        #         # mplpl.plot(bin_edges[1:], ncdf, c='y', lw=5, label='All users')
        #         # mplpl.ylabel('CDF', fontsize=18)
        #         # mplpl.xlabel('Absolute perception bias (APB)', fontsize=18)
        #         # mplpl.title(data_name)
        #         # #         mplpl.legend(loc="upper right")
        #         # mplpl.xlim([0, 1])
        #         # mplpl.ylim([0, 1])
        #         # mplpl.grid()
        #         # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_abs-err_cdf'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_abs-err_cdf| alt text| width = 500px}}')
        #         #
        #         # # outF.write('|| Table ||\n\n')
        #         #
        #         # num_bins = len(gull_l)
        #         # counts, bin_edges = np.histogram(gull_l, bins=num_bins, normed=True)
        #         # cdf = np.cumsum(counts)
        #         # scale = 1.0 / cdf[-1]
        #         # ncdf = scale * cdf
        #         # mplpl.plot(bin_edges[1:], ncdf, c='m', lw=5, label='')
        #         # mplpl.ylabel('CDF', fontsize=18)
        #         # mplpl.xlabel('Gullibility', fontsize=18)
        #         # mplpl.grid()
        #         # mplpl.title(data_name)
        #         # #         mplpl.legend(loc="upper right")
        #         # mplpl.xlim([0, 1])
        #         # mplpl.ylim([0, 1])
        #         # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_gull_cdf'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_gull_cdf| alt text| width = 500px}}')
        #         #
        #         # num_bins = len(cyn_l)
        #         # counts, bin_edges = np.histogram(cyn_l, bins=num_bins, normed=True)
        #         # cdf = np.cumsum(counts)
        #         # scale = 1.0 / cdf[-1]
        #         # ncdf = scale * cdf
        #         # mplpl.plot(bin_edges[1:], ncdf, c='k', lw=5, label='')
        #         # mplpl.ylabel('CDF', fontsize=18)
        #         # mplpl.xlabel('Cynicality', fontsize=18)
        #         # mplpl.grid()
        #         # mplpl.title(data_name)
        #         # #         mplpl.legend(loc="upper right")
        #         # mplpl.xlim([0, 1])
        #         # mplpl.ylim([0, 1])
        #         # pp = remotedir + '/fig/fig_exp1/user_based/initial/' + data_n + '_user_cyn_cdf'
        #         # mplpl.savefig(pp, format='png')
        #         # mplpl.figure()
        #         #
        #         # outF.write('||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/user_based/initial/'+ data_n + '_user_cyn_cdf| alt text| width = 500px}}||\n')
        #         #
        #         #
        #         #
        #         #
        #         # exit()
        #     else:
        #
        #         AVG_list = []
        #         print(np.mean(all_acc))
        #         outF = open(remotedir + 'output.txt', 'w')
        #
        #         tweet_all_var = {}
        #         tweet_all_dev_avg = {}
        #         tweet_all_avg = {}
        #         tweet_all_gt_var = {}
        #         tweet_all_dev_avg_l = []
        #         tweet_all_dev_med_l = []
        #         tweet_all_dev_var_l = []
        #         tweet_all_avg_l = []
        #         tweet_all_med_l = []
        #         tweet_all_var_l = []
        #         tweet_all_gt_var_l = []
        #         diff_group_disp_l = []
        #         dem_disp_l = []
        #         rep_disp_l = []
        #
        #         tweet_all_dev_avg = {}
        #         tweet_all_dev_med = {}
        #         tweet_all_dev_var = {}
        #
        #         tweet_all_dev_avg_l = []
        #         tweet_all_dev_med_l = []
        #         tweet_all_dev_var_l = []
        #
        #         tweet_all_abs_dev_avg = {}
        #         tweet_all_abs_dev_med = {}
        #         tweet_all_abs_dev_var = {}
        #
        #         tweet_all_abs_dev_avg_l = []
        #         tweet_all_abs_dev_med_l = []
        #         tweet_all_abs_dev_var_l = []
        #         tweet_all_dev_avg_rnd = {}
        #         tweet_all_abs_dev_avg_rnd = {}
        #
        #         diff_group_disp_dict = {}
        #         if dataset == 'snopes':
        #             data_n = 'sp'
        #             news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
        #             ind_l = [1, 2, 3]
        #         elif dataset == 'politifact':
        #             data_n = 'pf'
        #             news_cat_list = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
        #             ind_l = [1, 2, 3]
        #         elif dataset == 'mia':
        #             data_n = 'mia'
        #             news_cat_list = ['rumor', 'non-rumor']
        #             ind_l = [1]
        #
        #         for cat_l in news_cat_list:
        #             outF.write('== ' + str(cat_l) + ' ==\n\n')
        #             print('== ' + str(cat_l) + ' ==')
        #             tweet_dev_avg = {}
        #             tweet_dev_med = {}
        #             tweet_dev_var = {}
        #             tweet_abs_dev_avg = {}
        #             tweet_abs_dev_med = {}
        #             tweet_abs_dev_var = {}
        #
        #             tweet_avg = {}
        #             tweet_med = {}
        #             tweet_var = {}
        #             tweet_gt_var = {}
        #
        #             tweet_dev_avg_rnd = {}
        #             tweet_abs_dev_avg_rnd = {}
        #
        #
        #             tweet_dev_avg_l = []
        #             tweet_dev_med_l = []
        #             tweet_dev_var_l = []
        #             tweet_abs_dev_avg_l = []
        #             tweet_abs_dev_med_l = []
        #             tweet_abs_dev_var_l = []
        #
        #             tweet_avg_l = []
        #             tweet_med_l = []
        #             tweet_var_l = []
        #             tweet_gt_var_l = []
        #             AVG_susc_list = []
        #             AVG_wl_list = []
        #             all_acc = []
        #             AVG_dev_list = []
        #             # for lean in [-1, 0, 1]:
        #
        #                 # AVG_susc_list = []
        #                 # AVG_wl_list = []
        #                 # all_acc = []
        #                 # df_m = df_m[df_m['leaning'] == lean]
        #                 # if lean == 0:
        #                 #     col = 'g'
        #                 #     lean_cat = 'neutral'
        #                 # elif lean == 1:
        #                 #     col = 'b'
        #                 #     lean_cat = 'democrat'
        #                 # elif lean == -1:
        #                 #     col = 'r'
        #                 #     lean_cat = 'republican'
        #                 # print(lean_cat)
        #             for ind in ind_l:
        #
        #                 if balance_f == 'balanced':
        #                     inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_balanced.csv'
        #                 else:
        #                     inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final.csv'
        #
        #                 inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp' + str(ind) + '.csv'
        #                 df[ind] = pd.read_csv(inp1, sep="\t")
        #                 df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #                 df_m = df[ind].copy()
        #                 df_mm = df_m.copy()
        #
        #                 df_m = df_m[df_m['ra_gt'] == cat_l]
        #                 # df_mm = df_m[df_m['ra_gt']==cat_l]
        #                 # df_m = df_m[df_m['leaning'] == lean]
        #
        #                 groupby_ftr = 'tweet_id'
        #                 grouped = df_m.groupby(groupby_ftr, sort=False)
        #                 grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()
        #
        #                 for t_id in grouped.groups.keys():
        #                     df_tmp = df_m[df_m['tweet_id'] == t_id]
        #
        #                     df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
        #                     df_tmp_dem = df_tmp_m[df_tmp_m['leaning'] == 1]
        #                     df_tmp_rep = df_tmp_m[df_tmp_m['leaning'] == -1]
        #                     ind_t = df_tmp.index.tolist()[0]
        #                     weights = []
        #                     df_tmp = df_m[df_m['tweet_id'] == t_id]
        #                     ind_t = df_tmp.index.tolist()[0]
        #                     weights = []
        #
        #                     weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
        #                     val_list = list(df_tmp['rel_v'])
        #                     tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
        #                     tweet_avg[t_id] = np.mean(val_list)
        #                     tweet_med[t_id] = np.median(val_list)
        #                     tweet_var[t_id] = np.var(val_list)
        #                     tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
        #
        #                     tweet_avg_l.append(np.mean(val_list))
        #                     tweet_med_l.append(np.median(val_list))
        #                     tweet_var_l.append(np.var(val_list))
        #                     tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
        #
        #
        #
        #
        #                     tweet_all_avg[t_id] = np.mean(val_list)
        #                     tweet_all_var[t_id] = np.var(val_list)
        #                     tweet_all_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]
        #
        #                     tweet_all_avg_l.append(np.mean(val_list))
        #                     tweet_all_med_l.append(np.median(val_list))
        #                     tweet_all_var_l.append(np.var(val_list))
        #                     tweet_all_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
        #
        #
        #
        #                     val_list = list(df_tmp['err'])
        #                     abs_var_err = [np.abs(x) for x in val_list]
        #                     tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
        #                     tweet_dev_avg[t_id] = np.mean(val_list)
        #                     tweet_dev_med[t_id] = np.median(val_list)
        #                     tweet_dev_var[t_id] = np.var(val_list)
        #
        #                     tweet_dev_avg_l.append(np.mean(val_list))
        #                     tweet_dev_med_l.append(np.median(val_list))
        #                     tweet_dev_var_l.append(np.var(val_list))
        #
        #                     tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
        #                     tweet_abs_dev_med[t_id] = np.median(abs_var_err)
        #                     tweet_abs_dev_var[t_id] = np.var(abs_var_err)
        #
        #                     tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
        #                     tweet_abs_dev_med_l.append(np.median(abs_var_err))
        #                     tweet_abs_dev_var_l.append(np.var(abs_var_err))
        #
        #
        #                     tweet_all_dev_avg[t_id] = np.mean(val_list)
        #                     tweet_all_dev_med[t_id] = np.median(val_list)
        #                     tweet_all_dev_var[t_id] = np.var(val_list)
        #
        #                     tweet_all_dev_avg_l.append(np.mean(val_list))
        #                     tweet_all_dev_med_l.append(np.median(val_list))
        #                     tweet_all_dev_var_l.append(np.var(val_list))
        #
        #                     tweet_all_abs_dev_avg[t_id] = np.mean(abs_var_err)
        #                     tweet_all_abs_dev_med[t_id] = np.median(abs_var_err)
        #                     tweet_all_abs_dev_var[t_id] = np.var(abs_var_err)
        #
        #                     tweet_all_abs_dev_avg_l.append(np.mean(abs_var_err))
        #                     tweet_all_abs_dev_med_l.append(np.median(abs_var_err))
        #                     tweet_all_abs_dev_var_l.append(np.var(abs_var_err))
        #
        #
        #
        #                     sum_rnd_abs_perc = 0
        #                     sum_rnd_perc = 0
        #                     for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
        #                         sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
        #                         sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
        #                     random_perc = np.abs(sum_rnd_perc / float(7))
        #                     random_abs_perc = sum_rnd_abs_perc / float(7)
        #
        #                     tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
        #                     tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
        #
        #                     tweet_all_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
        #                     tweet_all_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
        #
        #             gt_l = []
        #             pt_l = []
        #             disputability_l = []
        #             perc_l = []
        #             abs_perc_l = []
        #             # for t_id in tweet_l_sort:
        #             #     gt_l.append(tweet_gt_var[t_id])
        #             #     pt_l.append(tweet_avg[t_id])
        #             #     disputability_l.append(tweet_var[t_id])
        #             #     perc_l.append(tweet_dev_avg[t_id])
        #             #     abs_perc_l.append(tweet_abs_dev_avg[t_id])
        #
        #
        #
        #             # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
        #             tweet_l_sort = sorted(tweet_var, key=tweet_var.get, reverse=True)
        #             # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
        #             # tweet_l_sort = sorted(tweet_abs_dev_avg, key=tweet_abs_dev_avg.get, reverse=True)
        #             # tweet_l_sort = sorted(tweet_dev_avg_rnd, key=tweet_dev_avg_rnd.get, reverse=True)
        #             # tweet_l_sort = sorted(tweet_abs_dev_avg_rnd, key=tweet_abs_dev_avg_rnd.get, reverse=True)
        #
        #             # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get, reverse=True)
        #
        #
        #             if dataset == 'snopes':
        #                 data_addr = 'snopes'
        #             elif dataset == 'politifact':
        #                 data_addr = 'politifact/fig'
        #             elif dataset == 'mia':
        #                 data_addr = 'mia/fig'
        #
        #             count = 0
        #             outF.write(
        #                 '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
        #             # '|| || news || Category|| grouped disputablity||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
        #
        #             for t_id in tweet_l_sort:
        #                 count+=1
        #                 if balance_f=='balanced':
        #                     outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
        #                                + str(np.round(diff_group_disp_dict[t_id], 3)) + '||'+ str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'
        #                                + '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
        #                                str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
        #                                str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
        #                                str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
        #                                str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
        #                     # +
        #
        #                 else:
        #                     outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] +
        #                                # str(np.round(diff_group_disp_dict[t_id], 3)) +
        #                                '||'+  str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id])+'||'
        #                                 + str(tweet_all_dev_avg_rnd[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +
        #                                 '||{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
        #                                str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
        #                                str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
        #                                str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
        #                                str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
        #
        #
        #
        #
        #         if dataset == 'snopes':
        #             data_addr = 'snopes'
        #         elif dataset == 'politifact':
        #             data_addr = 'politifact/fig'
        #         elif dataset == 'mia':
        #             data_addr = 'mia/fig'
        #
        #         # tweet_l_sort = sorted(diff_group_disp_dict, key=diff_group_disp_dict.get, reverse=True)
        #         # tweet_l_sort = sorted(tweet_all_avg, key=tweet_all_avg.get, reverse=True)
        #         tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
        #         # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
        #         # tweet_l_sort = sorted(tweet_all_abs_dev_avg, key=tweet_all_abs_dev_avg.get, reverse=True)
        #         # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
        #         # tweet_l_sort = sorted(tweet_all_dev_avg_rnd, key=tweet_all_dev_avg_rnd.get, reverse=True)
        #
        #         # tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
        #         # tweet_l_sort = sorted(tweet_all_dev_avg, key=tweet_all_dev_avg.get, reverse=True)
        #
        #         tweet_napb_dict_high_disp = {}
        #         tweet_napb_dict_low_disp = {}
        #         for t_id in tweet_l_sort[:20]:
        #             # tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
        #             tweet_napb_dict_high_disp[t_id] = tweet_all_abs_dev_avg[t_id]
        #
        #         for t_id in tweet_l_sort[-20:]:
        #             # tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg_rnd[t_id]
        #             tweet_napb_dict_low_disp[t_id] = tweet_all_abs_dev_avg[t_id]
        #
        #         kk = 0
        #
        #         for tweet_dict in [tweet_napb_dict_high_disp, tweet_napb_dict_low_disp]:
        #             if kk==0:
        #                 tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=False)
        #             else:
        #                 tweet_l_sort = sorted(tweet_dict, key=tweet_dict.get, reverse=True)
        #
        #             kk+=1
        #             count = 0
        #             outF.write(
        #                 '|| || news || Category|| Perception bias <<BR>> Absolute perception bias||Perception bias <<BR>> Absolute perception bias (rnd)||All rederas judgment dist || Democrats || Republicans || Neutrals ||\n')
        #             for t_id in tweet_l_sort:
        #                 count += 1
        #                 # ind_t = df_tmp_m[df_tmp_m['tweet_id']=t_id].index.tolist()
        #                 if balance_f == 'balanced':
        #                     outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||'
        #                                + str(np.round(diff_group_disp_dict[t_id], 3)) + '||' +
        #                                str(tweet_all_dev_avg[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) +'||'+
        #                                str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/balanced/' +
        #                                str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
        #                                str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
        #                                str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/balanced/' +
        #                                str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
        #                     # +
        #                     #            '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
        #                     #            str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')
        #
        #                 else:
        #                     outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
        #                                str(tweet_all_dev_avg[t_id]) + '<<BR>>' + str(tweet_all_abs_dev_avg[t_id]) + '||' +
        #                                str(tweet_all_dev_avg_rnd[t_id]) +'<<BR>>' + str(tweet_all_abs_dev_avg_rnd[t_id]) +'||'+
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/' +
        #                                str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
        #                                str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
        #                                str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
        #                                '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + data_addr + '/fig_exp1/news_based/leaning_demographic/' +
        #                                str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||\n')
        #                 # +
        #                 # '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
        #                 # str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')






    if args.t == "AMT_dataset_reliable_news_processing_snopes_weighted_visualisation":

            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            print(inp_all)
            source_dict = {}
            text_dict = {}
            date_dict = {}
            c_ind_news = 0
            c_ind_source = 0
            c_ind_date = 0
            news_txt = {}
            news_source = {}
            news_cat = {}
            news_date = {}
            news_topic = {}
            topic_count_dict = collections.defaultdict(int)

            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            line_count = 0
            tmp_dict = {}
            claims_list = []
            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable

            # run = 'plot'
            run = 'analysis'
            run = 'second-analysis'
            exp1_list = sample_tweets_exp1
            df = collections.defaultdict()
            df_w = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0
            # for ind in [1,2,3]:
            all_acc = []

            ##########################prepare balanced data (same number of rep, dem, neut #############

            #
            # for ind in [1,2,3]:
            #     inp1 = remotedir  +'amt_answers_sp_claims_exp'+str(ind)+'_final.csv'
            #     inp1_w = remotedir  +'worker_amt_answers_sp_claims_exp'+str(ind)+'.csv'
            #     df[ind] = pd.read_csv(inp1, sep="\t")
            #     df_w[ind] = pd.read_csv(inp1_w, sep="\t")
            #
            #     df_m = df[ind].copy()
            #
            #
            #
            #     rep_num = len(df_m[df_m['leaning']==-1])/float(50)
            #     dem_num = len(df_m[df_m['leaning'] == 1])/float(50)
            #     neut_num = len(df_m[df_m['leaning'] == 0])/float(50)
            #
            #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
            #
            #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
            #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
            #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
            #
            #     random.shuffle(dem_workers)
            #     random.shuffle(rep_workers)
            #     random.shuffle(neut_workers)
            #
            #     dem_workers = dem_workers[:min_num]
            #     rep_workers = rep_workers[:min_num]
            #     neut_workers = neut_workers[:min_num]
            #
            #     all_workers = []
            #     all_workers += dem_workers
            #     all_workers += rep_workers
            #     all_workers += neut_workers
            #
            #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
            #
            #     df[ind].to_csv(remotedir + 'amt_answers_sp_claims_exp'+str(ind)+'_final_balanced.csv',
            #                 columns=df[ind].columns, sep="\t", index=False)

            # exit()

            # balance_f = 'balanced'
            balance_f = 'un_balanced'

            fig_f = True
            fig_f = False
            for ind in [3]:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_sp_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_sp_claims_exp' + str(ind) + '_final.csv'
                inp1_w = remotedir + 'worker_amt_answers_sp_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                # df_tmp = df_m[df_m['tweet_id'] == t_id]
                for t_id in grouped.groups.keys():

                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []
                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])
                    accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))

                    all_acc.append(accuracy)
                    if fig_f == True:
                        try:
                            df_tmp['rel_v'].plot(kind='kde', lw=4, color='g', label='Worker Judgment')
                        except:
                            print('hmm')

                        mplpl.hist(list(df_tmp['rel_v']), weights=weights, color='g')

                        mplpl.plot([df_tmp['rel_gt_v'][ind_t], df_tmp['rel_gt_v'][ind_t]], [0, 0.5], color='r',
                                   label='Ground Truth', linewidth=10)

                        mplpl.ylabel('Frequency')
                        mplpl.xlabel('Truth value of claim')
                        mplpl.title(
                            df_tmp['ra_gt'][ind_t] + '  Accuracy : ' + str(np.round(accuracy, 3)) + '\n Avg : ' + str(
                                np.round(np.mean(val_list), 3))
                            + ', med : ' + str(np.round(np.median(val_list), 3)) + ', Var : ' + str(
                                np.round(np.var(val_list), 3)))
                        mplpl.legend(loc="upper right")
                        mplpl.xlim([-2, 2])
                        mplpl.ylim([0, 1])
                        if balance_f == 'balanced':
                            pp = remotedir + '/fig/fig_exp1/news_based/balanced/' + str(t_id) + '_rel_dist'
                        else:
                            pp = remotedir + '/fig/fig_exp1/news_based/' + str(t_id) + '_rel_dist'
                        # pp = remotedir  + '/fig/fig_exp1/acc/'+ str(t_id)+'_rel_dist'
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()

                    weights = []
                    weights.append(np.ones_like(list(df_tmp['err'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['susc'])
                    # val_list = list(df_tmp['err'])
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)

                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))
                    if fig_f == True:

                        try:
                            df_tmp['susc'].plot(kind='kde', lw=4, color='c', label='Worker Judgment')
                        except:
                            print('hmm')

                        mplpl.hist(list(df_tmp['susc']), weights=weights, color='c')

                        # mplpl.plot([df_tmp['rel_gt_v'][ind_t], df_tmp['rel_gt_v'][ind_t]], [0, 0.5], color='r',
                        #            label='Ground Truth', linewidth=10)

                        mplpl.ylabel('Frequency')
                        mplpl.xlabel('Worker judgment value - Ground truth value')
                        mplpl.title(df_tmp['ra_gt'][ind_t] + '\n Avg : ' + str(np.round(np.mean(val_list), 3))
                                    + ', med : ' + str(np.round(np.median(val_list), 3)) + ', Var : ' + str(
                            np.round(np.var(val_list), 3)))
                        mplpl.legend(loc="upper right")
                        tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                        # mplpl.subplots_adjust(bottom=0.25)
                        mplpl.xlim([-1, 1])
                        mplpl.ylim([0, 3])
                        if balance_f == 'balanced':
                            pp = remotedir + '/fig/fig_exp1/news_based/balanced/' + str(t_id) + '_p_susc_dist'

                        else:
                            pp = remotedir + '/fig/fig_exp1/news_based/' + str(t_id) + '_p_susc_dist'
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()

            # exit()
            AVG_list = []
            print(np.mean(all_acc))
            outF = open(remotedir + 'table_out.txt', 'w')

            news_cat_list = ['FALSE', 'MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
            for cat_l in news_cat_list:
                outF.write('== ' + cat_l + ' ==\n\n')
                tweet_dev_avg = {}
                tweet_dev_med = {}
                tweet_dev_var = {}
                # tweet_avg = {}
                # tweet_med = {}
                # tweet_var = {}
                # tweet_gt_var = {}

                tweet_dev_avg_l = []
                tweet_dev_med_l = []
                tweet_dev_var_l = []
                tweet_avg_l = []
                tweet_med_l = []
                tweet_var_l = []
                tweet_gt_var_l = []
                AVG_susc_list = []
                AVG_wl_list = []
                all_acc = []
                for ind in [1, 2, 3]:
                    # for ind in [3]:

                    if balance_f == 'balanced':
                        inp1 = remotedir + 'amt_answers_sp_claims_exp' + str(ind) + '_final_balanced.csv'
                    else:
                        inp1 = remotedir + 'amt_answers_sp_claims_exp' + str(ind) + '_final.csv'
                    inp1_w = remotedir + 'worker_amt_answers_sp_claims_exp' + str(ind) + '.csv'
                    df[ind] = pd.read_csv(inp1, sep="\t")
                    df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                    df_m = df[ind].copy()

                    df_m = df_m[df_m['ra_gt'] == cat_l]
                    groupby_ftr = 'tweet_id'
                    grouped = df_m.groupby(groupby_ftr, sort=False)
                    grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                    for t_id in grouped.groups.keys():
                        df_tmp = df_m[df_m['tweet_id'] == t_id]
                        ind_t = df_tmp.index.tolist()[0]
                        weights = []
                        val_list = list(df_tmp['rel_v'])
                        tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                        # tweet_avg[t_id] = [np.mean(val_list)]
                        # tweet_med[t_id] = [np.median(val_list)]
                        # tweet_var[t_id] = [np.var(val_list)]
                        tweet_gt_var[t_id] = [df_tmp['rel_gt_v'][ind_t]]

                        tweet_avg_l.append(np.mean(val_list))
                        tweet_med_l.append(np.median(val_list))
                        tweet_var_l.append(np.var(val_list))
                        tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                        AVG_wl_list += list(df_tmp['rel_v'])

                        AVG_susc_list += list(df_tmp['susc'])
                        val_list = list(df_tmp['susc'])
                        tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                        tweet_dev_avg[t_id] = np.mean(val_list)
                        tweet_dev_med[t_id] = np.median(val_list)
                        tweet_dev_var[t_id] = np.var(val_list)
                        df_tt = df_tmp[df_tmp['acc'] > -1]

                        accuracy = np.sum(df_tt['acc']) / float(len(df_tt))

                        all_acc.append(accuracy)

                        # all_acc.append(np.sum(df_tmp['acc'])/float(len(df_tmp)))
                        #
                        # tweet_dev_avg_l.append(np.mean(val_list))
                        # tweet_dev_med_l.append(np.median(val_list))
                        # tweet_dev_var_l.append(np.var(val_list))
                print(np.mean(all_acc))
                weights = []
                weights.append(np.ones_like(AVG_wl_list) / float(len(AVG_wl_list)))

                # df_tt = pd.DataFrame(np.array(AVG_wl_list), columns=["wl_g"])
                # # df_tt['wl_g'] = df_tt['wl_g']/([np.max(df_tt['wl_g'])]*len(df_tt))
                # # df_tt['wl_g'] = df_tt['wl_g']/([2]*len(df_tt))
                # try:
                #     df_tt['wl_g'].plot(kind='kde', lw=4, color='g', label='Worker Judgment')
                # except:
                #     print('hmm')
                #
                # mplpl.hist(AVG_wl_list, weights=weights, color='g')
                # #
                # mplpl.ylabel('Frequency')
                # mplpl.xlabel('Truth value of claim')
                # mplpl.title('Avg : ' + str(np.round(np.mean(AVG_wl_list),3))
                # + ', med : '+ str(np.round(np.median(AVG_wl_list),3)) + ', Var : ' + str(np.round(np.var(AVG_wl_list),3)))
                # mplpl.legend(loc="upper right")
                # mplpl.xlim([-2,2])
                # mplpl.ylim([0,1])
                # pp = remotedir  + '/fig/fig_exp1/news_based/'+ str(cat_l)+'_rel_dist'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                # #
                # weights = []
                # weights.append(np.ones_like(AVG_susc_list) / float(len(AVG_susc_list)))
                #
                # df_tt = pd.DataFrame(np.array(AVG_susc_list), columns=["gt"])
                # try:
                #     df_tt['gt'].plot(kind='kde', lw=4, color='c', label='Ground Truth')
                # except:
                #     print('hmm')
                #
                # mplpl.hist(AVG_susc_list, weights=weights, color='c')
                # #
                # mplpl.ylabel('Frequency')
                # mplpl.xlabel('Truth value of claim')
                # mplpl.title('Avg : ' + str(np.round(np.mean(AVG_susc_list),3))
                # + ', med : '+ str(np.round(np.median(AVG_susc_list),3)) + ', Var : ' + str(np.round(np.var(AVG_susc_list),3)))
                # mplpl.legend(loc="upper right")
                # mplpl.xlim([0,1])
                # mplpl.ylim([0,3])
                # pp = remotedir  + '/fig/fig_exp1/news_based/'+ str(cat_l)+'_susc_dist'
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()




                # print(np.corrcoef(tweet_avg_l,tweet_gt_var_l)[0][1])
                # print(np.corrcoef(tweet_med_l,tweet_gt_var_l)[0][1])
                # print(np.corrcoef(tweet_var_l,tweet_gt_var_l)[0][1])
                #
                # print(np.corrcoef(tweet_dev_avg_l, tweet_gt_var_l)[0][1])
                # print(np.corrcoef(tweet_dev_med_l, tweet_gt_var_l)[0][1])
                # print(np.corrcoef(tweet_dev_var_l, tweet_gt_var_l)[0][1])




                # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get,reverse=False)
                # tweet_l_sort = sorted(tweet_med, key=tweet_med.get,reverse=False)
                # tweet_l_sort = sorted(tweet_var, key=tweet_var.get,reverse=False)

                # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get,reverse=False)
                # tweet_l_sort = sorted(tweet_dev_med, key=tweet_dev_med.get,reverse=False)
                tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get, reverse=True)
                count = 0
                for t_id in tweet_l_sort:
                    count += 1
                    outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                               str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                               str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')









                    # exit()


                    #
                    #
                    #
                    #
                    # mplpl.scatter(range(len(dem_susc_list_sort)),dem_susc_list_sort, s=40,color='b',marker='o', label='Democrats : ' + str(np.round(np.mean(dem_susc_list_sort),3)))
                    # mplpl.plot(range(len(dem_susc_list_sort)), dem_susc_list_sort, color='b')
                    # mplpl.scatter(range(len(rep_susc_list_sort)),rep_susc_list_sort, s=40,color='r',marker='s', label='Republicans : ' + str(np.round(np.mean(rep_susc_list_sort),3)))
                    # mplpl.plot(range(len(rep_susc_list_sort)), rep_susc_list_sort, color='r')
                    # mplpl.scatter(range(len(neut_susc_list_sort)),neut_susc_list_sort, s=40,color='g',marker='<', label='Neutrals : ' + str(np.round(np.mean(neut_susc_list_sort),3)))
                    # mplpl.plot(range(len(neut_susc_list_sort)), neut_susc_list_sort, color='g')
                    # mplpl.xlabel('Workers rank')
                    # mplpl.ylabel('Susceptibility')
                    # # mplpl.title('All users, Mean : ' + str(np.mean(dem_gull_list_sort)))
                    # # mplpl.ylim([0,1])
                    # mplpl.legend(loc="upper left")
                    # pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_dem_rep_neut_susc_workers'
                    # # pp = remotedir + local_dir_saving + '/fig_exp2/dem_rep_neut_gullibility_workers'
                    # mplpl.savefig(pp, format='png')
                    # mplpl.figure()
                    #
                    #
                    # mplpl.scatter(range(len(dem_gull_list_sort)),dem_gull_list_sort, s=40,color='b',marker='o', label='Democrats : ' + str(np.round(np.mean(dem_gull_list_sort),3)))
                    # mplpl.plot(range(len(dem_gull_list_sort)), dem_gull_list_sort, color='b')
                    # mplpl.scatter(range(len(rep_gull_list_sort)),rep_gull_list_sort, s=40,color='r',marker='s', label='Republicans : ' + str(np.round(np.mean(rep_gull_list_sort),3)))
                    # mplpl.plot(range(len(rep_gull_list_sort)), rep_gull_list_sort, color='r')
                    # mplpl.scatter(range(len(neut_gull_list_sort)),neut_gull_list_sort, s=40,color='g',marker='<', label='Neutrals : ' + str(np.round(np.mean(neut_gull_list_sort),3)))
                    # mplpl.plot(range(len(neut_gull_list_sort)), neut_gull_list_sort, color='g')
                    # mplpl.xlabel('Workers rank')
                    # mplpl.ylabel('Gulliblity')
                    # # mplpl.title('All users, Mean : ' + str(np.mean(dem_gull_list_sort)))
                    # # mplpl.ylim([0,1])
                    # mplpl.legend(loc="upper left")
                    # pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_dem_rep_neut_gullibility_workers'
                    # # pp = remotedir + local_dir_saving + '/fig_exp2/dem_rep_neut_gullibility_workers'
                    # mplpl.savefig(pp, format='png')
                    # mplpl.figure()
                    #
                    #
                    # mplpl.scatter(range(len(dem_cyn_list_sort)), dem_cyn_list_sort, s=40,color='b',marker='o', label='Democrats : ' + str(np.round(np.mean(dem_cyn_list_sort),3)))
                    # mplpl.plot(range(len(dem_cyn_list_sort)), dem_cyn_list_sort, color='b')
                    # mplpl.scatter(range(len(rep_cyn_list_sort)), rep_cyn_list_sort, s=40,color='r',marker='s', label='Republicans : ' + str(np.round(np.mean(rep_cyn_list_sort),3)))
                    # mplpl.plot(range(len(rep_cyn_list_sort)), rep_cyn_list_sort,color='r')
                    # mplpl.scatter(range(len(neut_cyn_list_sort)), neut_cyn_list_sort, s=40,color='g',marker='<', label='Neutrals : ' + str(np.round(np.mean(neut_cyn_list_sort),3)))
                    # mplpl.plot(range(len(neut_cyn_list_sort)), neut_cyn_list_sort, color='g')
                    # mplpl.xlabel('Workers rank')
                    # mplpl.ylabel('Cynicality')
                    # # mplpl.title('All users, Mean : ' + str(np.mean(dem_cyn_list_sort)))
                    # # mplpl.ylim([0,1])

    if args.t == "AMT_dataset_reliable_news_processing_snopes_weighted_demographic_visualisation":

        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
        inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
        print(inp_all)
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)

        news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
        news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

        line_count = 0
        tmp_dict = {}
        claims_list = []
        for i in range(0, 5):
            df_cat = news_cat_list[i]
            df_cat_f = news_cat_list_f[i]
            inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
            cat_count = 0
            for line in inF:
                claims_list.append(line)
                cat_count += 1

        sample_tweets_exp1 = []

        tweet_txt_dict = {}
        tweet_date_dict = {}
        tweet_lable_dict = {}

        for line in claims_list:
            line_splt = line.split('<<||>>')
            publisher_name = int(line_splt[2])
            tweet_txt = line_splt[3]
            tweet_id = publisher_name
            cat_lable = line_splt[4]
            dat = line_splt[5]
            dt_splt = dat.split(' ')[0].split('-')
            m_day = int(dt_splt[2])
            m_month = int(dt_splt[1])
            m_year = int(dt_splt[0])
            m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
            tweet_txt_dict[tweet_id] = tweet_txt
            tweet_date_dict[tweet_id] = m_date
            tweet_lable_dict[tweet_id] = cat_lable

        # run = 'plot'
        run = 'analysis'
        run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_1={}
        tweet_dev_med={}
        tweet_dev_var={}
        tweet_avg={}
        tweet_med={}
        tweet_var={}
        tweet_gt_var = {}

        tweet_dev_avg_l=[]
        tweet_dev_med_l=[]
        tweet_dev_var_l=[]
        tweet_avg_l=[]
        tweet_med_l=[]
        tweet_var_l=[]
        tweet_gt_var_l=[]
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0
        # for ind in [1,2,3]:


        all_acc =[]
        # balance_f = 'balanced'
        balance_f = 'un_balanced'

        fig_f = True
        fig_f = False
        for ind in [3]:
            if balance_f=='balanced':
                inp1 = remotedir  +'amt_answers_sp_claims_exp'+str(ind)+'_final_balanced.csv'
            else:
                inp1 = remotedir  +'amt_answers_sp_claims_exp'+str(ind)+'_final.csv'

            inp1_w = remotedir  +'worker_amt_answers_sp_claims_exp'+str(ind)+'.csv'
            df[ind] = pd.read_csv(inp1, sep="\t")
            df_w[ind] = pd.read_csv(inp1_w, sep="\t")

            df_m = df[ind].copy()

            for lean in [-1,0,1]:
                df_m = df[ind].copy()
                df_mm = df[ind].copy()

                if lean==0:
                    col = 'g'
                    lean_cat = 'neutral'
                elif lean==1:
                    col = 'b'
                    lean_cat = 'democrat'
                elif lean == -1:
                    col = 'r'
                    lean_cat = 'republican'

                df_m = df_m[df_m['leaning']==lean]

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                # df_tmp = df_m[df_m['tweet_id'] == t_id]
                for t_id in grouped.groups.keys():


                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    weights = []
                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list),np.median(val_list),np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]

                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    df_tt = df_tmp[df_tmp['acc']>-1]

                    accuracy = np.sum(df_tt['acc'])/float(len(df_tt))

                    all_acc.append(accuracy)
                    if fig_f==True:
                        try:
                            df_tmp['rel_v'].plot(kind='kde', lw=4, color=col, label = 'Worker Judgment')
                        except:
                            print('hmm')

                        mplpl.hist(list(df_tmp['rel_v']), weights=weights, color=col)

                        mplpl.plot([df_tmp['rel_gt_v'][ind_t],df_tmp['rel_gt_v'][ind_t]],[0,0.5], color = 'k', label='Ground Truth',linewidth=10)

                        mplpl.ylabel('Frequency')
                        mplpl.xlabel('Truth value of claim')
                        mplpl.title(df_tmp['ra_gt'][ind_t] + '  Accuracy : ' + str(np.round(accuracy,3)) + ' workers leaning : ' + lean_cat
                                    +'\n Number : ' + str(len(val_list)) +' Avg : ' + str(np.round(np.mean(val_list),3))
                                    + ', med : '+ str(np.round(np.median(val_list),3)) + ', Var : ' + str(np.round(np.var(val_list),3)))
                        mplpl.legend(loc="upper right")
                        mplpl.xlim([-2,2])
                        mplpl.ylim([0,1])
                        # pp = remotedir  + '/fig/fig_exp1/news_based/'+ str(t_id)+'_rel_dist'
                        pp = remotedir  + '/fig/fig_exp1/news_based/leaning_demographic/balanced/'+ str(t_id)+'_rel_dist_' + lean_cat
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()

                    weights = []
                    weights.append(np.ones_like(list(df_tmp['err'])) / float(len(df_tmp)))

                    ##################
                    val_list = list(df_tmp_m['susc'])
                    # val_list = list(df_tmp['err'])
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list),np.median(val_list),np.var(val_list)]
                    tweet_dev_avg_1[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)

                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))
                    if fig_f==True:
                        try:
                            df_tmp['susc'].plot(kind='kde', lw=4, color='c', label='Worker Judgment')
                        except:
                            print('hmm')

                        mplpl.hist(list(df_tmp['susc']), weights=weights, color='c')

                        # mplpl.plot([df_tmp['rel_gt_v'][ind_t], df_tmp['rel_gt_v'][ind_t]], [0, 0.5], color='r',
                        #            label='Ground Truth', linewidth=10)

                        mplpl.ylabel('Frequency')
                        mplpl.xlabel('Worker judgment value - Ground truth value')
                        mplpl.title(df_tmp['ra_gt'][ind_t] +  ' workers leaning : ' + lean_cat + '\n Avg : ' + str(np.round(np.mean(val_list), 3))
                                    + ', med : ' + str(np.round(np.median(val_list), 3)) + ', Var : ' + str(
                            np.round(np.var(val_list), 3)))
                        mplpl.legend(loc="upper right")
                        tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                        # mplpl.subplots_adjust(bottom=0.25)
                        mplpl.xlim([-1,1])
                        mplpl.ylim([0,3])
                        pp = remotedir  + '/fig/fig_exp1/news_based/leaning_demographic/' + str(t_id) + '_p_susc_dist_' + lean_cat
                        mplpl.savefig(pp, format='png')
                        mplpl.figure()
        # exit()
        AVG_list = []
        print(np.mean(all_acc))
        outF = open(remotedir + 'table_out_1.txt', 'w')
        tweet_all_var = {}
        news_cat_list = ['FALSE','MOSTLY FALSE',  'MIXTURE', 'MOSTLY TRUE','TRUE']
        for cat_l in news_cat_list:
            outF.write('== ' + cat_l + ' ==\n\n')
            print('== ' + cat_l + ' ==')
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            # tweet_avg = {}
            # tweet_med = {}
            # tweet_var = {}
            # tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            AVG_susc_list = []
            AVG_wl_list = []
            all_acc = []

            for lean in [-1,0,1]:

                AVG_susc_list = []
                AVG_wl_list = []
                all_acc = []
                df_m = df_m[df_m['leaning']==lean]
                if lean==0:
                    col = 'g'
                    lean_cat = 'neutral'
                elif lean==1:
                    col = 'b'
                    lean_cat = 'democrat'
                elif lean == -1:
                    col = 'r'
                    lean_cat = 'republican'
                print(lean_cat)
                for ind in [1,2,3]:
                # for ind in [3]:

                    if balance_f == 'balanced':
                        inp1 = remotedir + 'amt_answers_sp_claims_exp' + str(ind) + '_final_balanced.csv'
                    else:
                        inp1 = remotedir + 'amt_answers_sp_claims_exp' + str(ind) + '_final.csv'

                    inp1_w = remotedir  +'worker_amt_answers_sp_claims_exp'+str(ind)+'.csv'
                    df[ind] = pd.read_csv(inp1, sep="\t")
                    df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                    df_m = df[ind].copy()

                    df_m = df_m[df_m['ra_gt']==cat_l]
                    df_mm = df_m[df_m['ra_gt']==cat_l]
                    df_m = df_m[df_m['leaning']==lean]
                    groupby_ftr = 'tweet_id'
                    grouped = df_m.groupby(groupby_ftr, sort=False)
                    grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                    for t_id in grouped.groups.keys():
                        df_tmp = df_m[df_m['tweet_id'] == t_id]
                        df_tmp_m = df_mm[df_mm['tweet_id'] == t_id]
                        ind_t = df_tmp.index.tolist()[0]
                        weights = []
                        val_list = list(df_tmp['rel_v'])
                        tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                        # tweet_avg[t_id] = [np.mean(val_list)]
                        # tweet_med[t_id] = [np.median(val_list)]
                        # tweet_var[t_id] = [np.var(val_list)]
                        tweet_gt_var[t_id] = [df_tmp['rel_gt_v'][ind_t]]

                        tweet_avg_l.append(np.mean(val_list))
                        tweet_med_l.append(np.median(val_list))
                        tweet_var_l.append(np.var(val_list))
                        tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                        AVG_wl_list+=list(df_tmp['rel_v'])


                        AVG_susc_list += list(df_tmp['susc'])

                        #####################
                        val_list = list(df_tmp_m['rel_v'])
                        tweet_all_var[t_id] = np.var(val_list)

                        val_list = list(df_tmp_m['susc'])
                        tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                        tweet_dev_avg[t_id] = np.mean(val_list)
                        # tweet_dev_avg[t_id] =tweet_dev_avg_1[t_id]
                        tweet_dev_med[t_id] = np.median(val_list)
                        tweet_dev_var[t_id] = np.var(val_list)
                        df_tt = df_tmp[df_tmp['acc'] > -1]

                        accuracy = np.sum(df_tt['acc']) / float(len(df_tt))

                        all_acc.append(accuracy)

                        # all_acc.append(np.sum(df_tmp['acc'])/float(len(df_tmp)))
                        #
                        # tweet_dev_avg_l.append(np.mean(val_list))
                        # tweet_dev_med_l.append(np.median(val_list))
                        # tweet_dev_var_l.append(np.var(val_list))
                print(np.mean(all_acc))
                weights=[]
                weights.append(np.ones_like(AVG_wl_list) / float(len(AVG_wl_list)))

                df_tt = pd.DataFrame(np.array(AVG_wl_list), columns=["wl_g"])
                # df_tt['wl_g'] = df_tt['wl_g']/([np.max(df_tt['wl_g'])]*len(df_tt))
                # df_tt['wl_g'] = df_tt['wl_g']/([2]*len(df_tt))

                if fig_f==True:
                    try:
                        df_tt['wl_g'].plot(kind='kde', lw=4, color=col, label='Worker Judgment')
                    except:
                        print('hmm')

                    mplpl.hist(AVG_wl_list, weights=weights, color=col)
                    #
                    mplpl.ylabel('Frequency')
                    mplpl.xlabel('Truth value of claim')
                    mplpl.title('Avg : ' + str(np.round(np.mean(AVG_wl_list),3))
                    + ', med : '+ str(np.round(np.median(AVG_wl_list),3)) + ', Var : ' + str(np.round(np.var(AVG_wl_list),3)))
                    mplpl.legend(loc="upper right")
                    mplpl.xlim([-2,2])
                    mplpl.ylim([0,1])
                    if balance_f=='balanced':
                        pp = remotedir  + '/fig/fig_exp1/news_based/leaning_demographic/'+ str(cat_l)+'_rel_dist_' + lean_cat
                    else:
                        pp = remotedir  + '/fig/fig_exp1/news_based/leaning_demographic/balanced/'+ str(cat_l)+'_rel_dist_' + lean_cat
                    mplpl.savefig(pp, format='png')
                    mplpl.figure()
                #
                weights = []
                # weights.append(np.ones_like(AVG_susc_list) / float(len(AVG_susc_list)))
                #
                # df_tt = pd.DataFrame(np.array(AVG_susc_list), columns=["gt"])
                # try:
                #     df_tt['gt'].plot(kind='kde', lw=4, color='c', label='Ground Truth')
                # except:
                #     print('hmm')
                #
                # mplpl.hist(AVG_susc_list, weights=weights, color='c')
                # #
                # mplpl.ylabel('Frequency')
                # mplpl.xlabel('Truth value of claim')
                # mplpl.title('Avg : ' + str(np.round(np.mean(AVG_susc_list),3))
                # + ', med : '+ str(np.round(np.median(AVG_susc_list),3)) + ', Var : ' + str(np.round(np.var(AVG_susc_list),3)))
                # mplpl.legend(loc="upper right")
                # mplpl.xlim([0,1])
                # mplpl.ylim([0,3])
                # pp = remotedir  + '/fig/fig_exp1/news_based/leaning_demographic/'+ str(cat_l)+'_susc_dist_' + lean_cat
                # mplpl.savefig(pp, format='png')
                # mplpl.figure()
                #
                #
                #

                # print(np.corrcoef(tweet_avg_l,tweet_gt_var_l)[0][1])
                # print(np.corrcoef(tweet_med_l,tweet_gt_var_l)[0][1])
                # print(np.corrcoef(tweet_var_l,tweet_gt_var_l)[0][1])
                #
                # print(np.corrcoef(tweet_dev_avg_l, tweet_gt_var_l)[0][1])
                # print(np.corrcoef(tweet_dev_med_l, tweet_gt_var_l)[0][1])
                # print(np.corrcoef(tweet_dev_var_l, tweet_gt_var_l)[0][1])




                # tweet_l_sort = sorted(tweet_avg, key=tweet_avg.get,reverse=False)
                # tweet_l_sort = sorted(tweet_med, key=tweet_med.get,reverse=False)
                # tweet_l_sort = sorted(tweet_var, key=tweet_var.get,reverse=False)

                # tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get,reverse=False)
                # tweet_l_sort = sorted(tweet_dev_med, key=tweet_dev_med.get,reverse=False)
            tweet_l_sort = sorted(tweet_dev_avg, key=tweet_dev_avg.get,reverse=True)
            count = 0
            for t_id in tweet_l_sort:
                count+=1
                if balance_f=='balanced':
                    outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id]+'||'+
                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/'+
                               str(t_id) + '_rel_dist| alt text| width = 500px}} ||'+
                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/leaning_demographic/balanced/'+
                               str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||'+
                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/leaning_demographic/balanced/'+
                               str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||'+
                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/leaning_demographic/balanced/'+
                               str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||'+
                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
                               str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

                else:
                    outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                               str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/leaning_demographic/' +
                               str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/leaning_demographic/' +
                               str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/leaning_demographic/' +
                               str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||' +
                               '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                               str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

        tweet_l_sort = sorted(tweet_all_var, key=tweet_all_var.get, reverse=True)
        count = 0
        for t_id in tweet_l_sort:
            count += 1
            if balance_f == 'balanced':
                outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
                           str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/leaning_demographic/balanced/' +
                           str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/leaning_demographic/balanced/' +
                           str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/leaning_demographic/balanced/' +
                           str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/balanced/' +
                           str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

            else:
                outF.write('||' + str(count) + '||' + tweet_txt_dict[t_id] + '||' + tweet_lable_dict[t_id] + '||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                           str(t_id) + '_rel_dist| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/leaning_demographic/' +
                           str(t_id) + '_rel_dist_democrat| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/leaning_demographic/' +
                           str(t_id) + '_rel_dist_republican| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/leaning_demographic/' +
                           str(t_id) + '_rel_dist_neutral| alt text| width = 500px}} ||' +
                           '{{http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/snopes/fig_exp1/news_based/' +
                           str(t_id) + '_p_susc_dist| alt text| width = 500px}} ||\n')

    if args.t == "AMT_dataset_reliable_news_processing_exp2_weighted_rel_leaning":
        publisher_leaning = 1
        threshold = 10
        local_dir_saving = ''
        sp = 'sp_all_'
        # sp = 'sp_'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/data/'
        # remotedir = 'reliable_news/data/'


        final_inp_exp1 = open(remotedir + local_dir_saving
                                 + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

        sample_tweets_exp1 = json.load(final_inp_exp1)

        input_rumor = open(remotedir + local_dir_saving + 'rumer_tweets', 'r')
        input_non_rumor = open(remotedir + local_dir_saving + 'non_rumer_tweets','r')

        input = remotedir + local_dir_saving + 'news_labling_time.csv'
        df_time = pd.read_csv(input, sep="\t")



        tweet_id = 100010
        publisher_name = 110
        tweet_popularity = {}
        tweet_text_dic = {}
        for input_file in [input_rumor, input_non_rumor]:
            for line in input_file:
                line.replace('\n','')
                line_splt = line.split('\t')
                tweet_txt = line_splt[1]
                tweet_link = line_splt[1]
                tweet_id+=1
                publisher_name+=1
                tweet_popularity[tweet_id] = int(line_splt[2])
                tweet_text_dic[tweet_id] = tweet_txt


        run = 'plot'
        # run = 'analysis'
        exp1_list = sample_tweets_exp1


        out_list = []
        cnn_list = []
        foxnews_list = []
        ap_list = []
        tweet_txt_dict = {}
        tweet_link_dict = {}
        tweet_publisher_dict = {}
        tweet_rumor= {}
        tweet_non_rumor = {}
        pub_dict = collections.defaultdict(list)
        for tweet in exp1_list:

            tweet_id = tweet[0]
            publisher_name = tweet[1]
            tweet_txt = tweet[2]
            tweet_link = tweet[3]
            tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
            tweet_txt_dict[tweet_id] = tweet_txt
            tweet_link_dict[tweet_id] = tweet_link
            tweet_publisher_dict[tweet_id] = publisher_name
            if int(tweet_id)<100060:
                tweet_rumor[tweet_id]=-1
            else:
                tweet_non_rumor[tweet_id]=1

        if run == 'analysis':

                print("analysis")


        else:
            # approach = 'weighted'
            approach = 'non-weighted'
            # outF = open(remotedir+local_dir_saving+'amt_consensus_output_wiki_offensive_2.txt','w')

            input = remotedir + local_dir_saving  + 'amt_answers_reliable_news_exp2.csv'
            df = pd.read_csv(input, sep="\t")
            tweet_dem_score = {};
            tweet_rep_score = {};
            tweet_cons_score = {}
            tweet_cons_score_1 = {};
            tweet_cons_score_2 = {};
            tweet_cons_score_3 = {}


            # sum_feat = df.groupby(groupby_ftr).sum()
            # count_feat = df.groupby(groupby_ftr).count()

            df.loc[:, 'rel_score'] = df['tweet_id'] * 0.0
            df.loc[:, 'intst_score'] = df['tweet_id'] * 0.0
            df.loc[:, 'lable'] = df['tweet_id'] * 0.0
            df.loc[:, 'delta_time'] = df['tweet_id'] * 0.0

            df.loc[:, 'gull'] = df['tweet_id'] * 0.0
            df.loc[:, 'cyn'] = df['tweet_id'] * 0.0
            tweet_rel_dict = collections.defaultdict(list)
            tweet_intst_dict = collections.defaultdict(list)
            rel_4 = 0
            for index in df.index.tolist():
                tweet_id = df['tweet_id'][index]
                if tweet_id == 1:
                    continue

                ra = df['ra'][index]
                if approach == 'weighted':
                    if ra==1:
                        rel = -3
                    elif ra==2:
                        rel=-2
                    elif ra==3:
                        rel=-1
                    elif ra==4:
                        rel=0
                    elif ra==5:
                        rel = 1
                    elif ra==6:
                        rel = 2
                    elif ra==7:
                        rel = 3

                elif approach == 'non-weighted':
                    if ra==1:
                        rel = -1
                    elif ra==2:
                        rel=-1
                    elif ra==3:
                        rel=-1
                    elif ra==4:
                        rel=0
                    elif ra==5:
                        rel = 1
                    elif ra==6:
                        rel = 1
                    elif ra==7:
                        rel = 1



                df['rel_score'][index] = rel

                tweet_rel_dict[tweet_id].append(rel)

                if tweet_id in tweet_rumor:
                    df['lable'][index] = 0
                    df['cyn'][index] = 0
                    df['gull'][index] = 0
                    if rel >0:
                        df['gull'][index] = 1

                else:
                    df['lable'][index] = 1
                    df['gull'][index] = 0
                    df['cyn'][index] = 0
                    if rel < 0:
                        df['cyn'][index] = 1

            df_dem = df[df['leaning'] == 1]
            df_rep = df[df['leaning'] == 2]
            df_neut = df[df['leaning'] == 3]
            rep_workers_list = list(set(df_rep['worker_id']))
            dem_workers_list = list(set(df_dem['worker_id']))
            neut_workers_list = list(set(df_neut['worker_id']))

            # random.shuffle(dem_workers_list,random.random)
            random.shuffle(dem_workers_list)
            dem_workers_list = dem_workers_list[:6]
            df_dem = df_dem[df_dem['worker_id'].isin(dem_workers_list)]

            # random.shuffle(neut_workers_list,random.random)
            random.shuffle(neut_workers_list)
            neut_workers_list = neut_workers_list[:6]
            df_neut = df_neut[df_neut['worker_id'].isin(neut_workers_list)]

            all_workers_list = []
            all_workers_list += dem_workers_list
            all_workers_list += rep_workers_list
            all_workers_list += neut_workers_list
            df = df[df['worker_id'].isin(all_workers_list)]

            groupby_ftr = 'worker_id'
            grouped = df.groupby(groupby_ftr, sort=False)
            grouped_sum = df.groupby(groupby_ftr, sort=False).sum()

            # for workerid in grouped.groups.keys():
            #     tweet_id_l = grouped.groups[workerid]
            #     for tweetid in tweet_id_l:
            #         index = df[df['tweet_id'] == tweet_id].index.tolist()[0]
            #         if df['lable'][index]==0:
            #             gull +=

            df_dem = df[df['leaning']==1]
            df_rep = df[df['leaning']==2]
            df_neut = df[df['leaning']==3]
            rep_workers_list = list(set(df_rep['worker_id']))
            dem_workers_list = list(set(df_dem['worker_id']))
            neut_workers_list = list(set(df_neut['worker_id']))


            df_gr = df.groupby('worker_id', sort=False)
            worker_id_l = df_gr.groups.keys()
            cc = 0
            pr_tim = 0
            cur_tim = 0

            # news_time_labling_csv.close()
            input = remotedir + local_dir_saving  + 'news_labling_time.csv'
            df_time = pd.read_csv(input, sep="\t")



            # print(np.max(df['time']) - np.min(df['time']))
            sum_t = 0
            for col in all_workers_list:
                sum_t += len(df_time[df_time[str(col)]<=20])/float(80)

            print(sum_t/float(len(all_workers_list)))
            groupby_ftr = 'worker_id'
            grouped_sum = df.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_dem = df_dem.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_rep = df_rep.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_neut = df_neut.groupby(groupby_ftr, sort=False).sum()

            df_rumor = df[df['lable']==0]
            df_non_rumor = df[df['lable']==1]

            df_dem_rumor = df_dem[df_dem['lable']==0]
            df_dem_non_rumor = df_dem[df_dem['lable']==1]
            df_rep_rumor = df_rep[df_rep['lable']==0]
            df_rep_non_rumor = df_rep[df_rep['lable']==1]
            df_neut_rumor = df_neut[df_neut['lable']==0]
            df_neut_non_rumor = df_neut[df_neut['lable']==1]


            tweet_rel_score = collections.defaultdict()
            tweet_intst_score = collections.defaultdict()
            tweet_rel_score_l = []
            tweet_intst_score_l = []
            tweet_dem_rel_score_l = []
            tweet_non_rumor_intst_score_l = []
            tweet_rep_rel_score_l = []
            tweet_neut_rel_score_l = []
            tweet_rumor_intst_score_l = []
            tweet_pop = []
            tweet_pop_rumor = []
            tweet_pop_non_rumor = []


            # users='all'
            # users = 'dem'
            # users = 'rep'
            # users = 'neut'

            news_story = 'all'
            # news_story = 'rumors'
            # news_story = 'real'
            #
            if news_story == 'all':
                df_m = df.copy()
                tweet_rel_list = tweet_rel_dict.keys()
            elif news_story == 'rumors':
                df_m = df[df['lable']==0]
                tweet_rel_list =  list(set(tweet_rumor))
            elif news_story == 'real':
                df_m = df[df['lable']==1]
                tweet_rel_list =  list(set(tweet_non_rumor))

            ind_l = df_m.index.tolist()
            # df_m.iloc[ind_l]
#             all_users_tweet_id = list(set(df.loc[ind_l]['tweet_id']))
#             dem_users_tweet_id = list(set(df_dem.loc[ind_l]['tweet_id']))
#             rep_users_tweet_id = list(set(df_rep.loc[ind_l]['tweet_id']))
#             neut_users_tweet_id = list(set(df_neut.loc[ind_l]['tweet_id']))

            groupby_ftr = 'tweet_id'
            grouped = df_m.groupby(groupby_ftr, sort=False)
            grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

            groupby_ftr = 'worker_id'
            grouped = df_m.groupby(groupby_ftr, sort=False)
            grouped_sum_w = df_m.groupby(groupby_ftr, sort=False).sum()


            groupby_ftr = 'tweet_id'
            grouped_dem_sum = df_dem.groupby(groupby_ftr, sort=False).sum()

            groupby_ftr = 'worker_id'
            grouped_dem_sum_w = df_dem.groupby(groupby_ftr, sort=False).sum()

            groupby_ftr = 'tweet_id'
            grouped_rep_sum = df_rep.groupby(groupby_ftr, sort=False).sum()

            groupby_ftr = 'worker_id'
            grouped_rep_sum_w = df_rep.groupby(groupby_ftr, sort=False).sum()


            groupby_ftr = 'tweet_id'
            grouped_neut_sum = df_neut.groupby(groupby_ftr, sort=False).sum()

            groupby_ftr = 'worker_id'
            grouped_neut_sum_w = df_neut.groupby(groupby_ftr, sort=False).sum()

            tweet_dem_rel_score = {}
            tweet_rep_rel_score = {}
            tweet_neut_rel_score = {}
            for tweet_id in tweet_rel_list:

                tweet_rel_score[tweet_id] = grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w))
                tweet_dem_rel_score[tweet_id] = grouped_dem_sum['rel_score'][tweet_id] / float(len(grouped_dem_sum_w))
                tweet_rep_rel_score[tweet_id] = grouped_rep_sum['rel_score'][tweet_id] / float(len(grouped_rep_sum_w))
                tweet_neut_rel_score[tweet_id] = grouped_neut_sum['rel_score'][tweet_id] / float(len(grouped_neut_sum_w))

            # tweet_rel_score_l_sort = sorted(tweet_rel_score, key=tweet_rel_score.get,reverse=False)
            # tweet_dem_rel_score_l_sort = sorted(tweet_dem_rel_score, key=tweet_dem_rel_score.get,reverse=False)
            # tweet_rep_rel_score_l_sort = sorted(tweet_rep_rel_score, key=tweet_rep_rel_score.get,reverse=False)
            # tweet_neut_rel_score_l_sort = sorted(tweet_neut_rel_score, key=tweet_neut_rel_score.get,reverse=False)

            tweet_rel_score_l_sort = sorted(tweet_rel_score.values())
            tweet_dem_rel_score_l_sort = sorted(tweet_dem_rel_score.values())
            tweet_rep_rel_score_l_sort = sorted(tweet_rep_rel_score.values())
            tweet_neut_rel_score_l_sort = sorted(tweet_neut_rel_score.values())
            # tweet_dem_rel_score_l_sort = sorted(tweet_dem_rel_score_l)
            # tweet_rep_rel_score_l_sort = sorted(tweet_rep_rel_score_l)
            # tweet_neut_rel_score_l_sort = sorted(tweet_neut_rel_score_l)
            # tweet_rel_score_l_sort = sorted(tweet_rel_score_l)



            mplpl.scatter(range(len(tweet_rel_score_l_sort)),tweet_rel_score_l_sort, s=40,color='k',marker='o', label='All users')
            mplpl.plot(range(len(tweet_rel_score_l_sort)), tweet_rel_score_l_sort, color='k')
            mplpl.scatter(range(len(tweet_dem_rel_score_l_sort)),tweet_dem_rel_score_l_sort, s=40,color='b',marker='s', label='Democrats')
            mplpl.plot(range(len(tweet_dem_rel_score_l_sort)), tweet_dem_rel_score_l_sort, color='b')
            mplpl.scatter(range(len(tweet_rep_rel_score_l_sort)),tweet_rep_rel_score_l_sort, s=40,color='r',marker='<', label='Republicans')
            mplpl.plot(range(len(tweet_rep_rel_score_l_sort)), tweet_rep_rel_score_l_sort, color='r')
            mplpl.scatter(range(len(tweet_neut_rel_score_l_sort)),tweet_neut_rel_score_l_sort, s=40,color='g',marker='^', label='Neutrals')
            mplpl.plot(range(len(tweet_neut_rel_score_l_sort)), tweet_neut_rel_score_l_sort, color='g')
            mplpl.xlabel('News')
            mplpl.ylabel('Reliability')
            # mplpl.title('All users, Mean : ' + str(np.mean(dem_gull_list_sort)))
            # mplpl.ylim([0,1])
            mplpl.legend(loc="upper left")
            if approach =='weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_'+news_story+'_reliability_news_1'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/'+news_story+'_reliability_news_1'
            mplpl.savefig(pp, format='png')
            mplpl.figure()


            num_bins = len(tweet_rel_score_l_sort)
            counts, bin_edges = np.histogram(tweet_rel_score_l_sort, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='k', lw=3, label='All users')


            num_bins = len(tweet_dem_rel_score_l_sort)
            counts, bin_edges = np.histogram(tweet_dem_rel_score_l_sort, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='b', lw=3, label='Democrats')

            num_bins = len(tweet_rep_rel_score_l_sort)
            counts, bin_edges = np.histogram(tweet_rep_rel_score_l_sort, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='r', lw=3, label='Republicans')

            num_bins = len(tweet_neut_rel_score_l_sort)
            counts, bin_edges = np.histogram(tweet_neut_rel_score_l_sort, bins=num_bins, normed=True)
            cdf = np.cumsum(counts)
            scale = 1.0 / cdf[-1]
            ncdf = scale * cdf
            mplpl.plot(bin_edges[1:], ncdf, c='g', lw=3, label='Neutrals')


            mplpl.xlabel('Reliability')
            mplpl.ylabel('CDF')
            # mplpl.title('Neut users, Mean : ' + str(np.mean(all_gull_list)))
            # mplpl.xlim([0,1])
            # mplpl.ylim([0,1])
            mplpl.legend(loc="upper left")
            if approach == 'weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_'+news_story+'_reliability_CDF_1'
            elif approach == 'non-weighted':
                pp = remotedir + local_dir_saving + '/fig_exp2/'+news_story+'_reliability_CDF_1'
            mplpl.savefig(pp, format='png')
            mplpl.figure()

        if args.t == "AMT_dataset_reliable_news_processing_exp2_weighted_rel_leaning":
            publisher_leaning = 1
            threshold = 10
            local_dir_saving = ''
            sp = 'sp_all_'
            # sp = 'sp_'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/data/'
            # remotedir = 'reliable_news/data/'


            final_inp_exp1 = open(remotedir + local_dir_saving
                                  + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir + local_dir_saving + 'non_rumer_tweets', 'r')

            input = remotedir + local_dir_saving + 'news_labling_time.csv'
            df_time = pd.read_csv(input, sep="\t")

            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            run = 'plot'
            # run = 'analysis'
            exp1_list = sample_tweets_exp1

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id) < 100060:
                    tweet_rumor[tweet_id] = -1
                else:
                    tweet_non_rumor[tweet_id] = 1

            if run == 'analysis':

                print("analysis")


            else:
                # approach = 'weighted'
                approach = 'non-weighted'
                # outF = open(remotedir+local_dir_saving+'amt_consensus_output_wiki_offensive_2.txt','w')

                input = remotedir + local_dir_saving + 'amt_answers_reliable_news_exp2.csv'
                df = pd.read_csv(input, sep="\t")
                tweet_dem_score = {};
                tweet_rep_score = {};
                tweet_cons_score = {}
                tweet_cons_score_1 = {};
                tweet_cons_score_2 = {};
                tweet_cons_score_3 = {}

                # sum_feat = df.groupby(groupby_ftr).sum()
                # count_feat = df.groupby(groupby_ftr).count()

                df.loc[:, 'rel_score'] = df['tweet_id'] * 0.0
                df.loc[:, 'intst_score'] = df['tweet_id'] * 0.0
                df.loc[:, 'lable'] = df['tweet_id'] * 0.0
                df.loc[:, 'delta_time'] = df['tweet_id'] * 0.0

                df.loc[:, 'gull'] = df['tweet_id'] * 0.0
                df.loc[:, 'cyn'] = df['tweet_id'] * 0.0
                tweet_rel_dict = collections.defaultdict(list)
                tweet_intst_dict = collections.defaultdict(list)
                rel_4 = 0
                for index in df.index.tolist():
                    tweet_id = df['tweet_id'][index]
                    if tweet_id == 1:
                        continue

                    ra = df['ra'][index]
                    if approach == 'weighted':
                        if ra == 1:
                            rel = -3
                        elif ra == 2:
                            rel = -2
                        elif ra == 3:
                            rel = -1
                        elif ra == 4:
                            rel = 0
                        elif ra == 5:
                            rel = 1
                        elif ra == 6:
                            rel = 2
                        elif ra == 7:
                            rel = 3

                    elif approach == 'non-weighted':
                        if ra == 1:
                            rel = -1
                        elif ra == 2:
                            rel = -1
                        elif ra == 3:
                            rel = -1
                        elif ra == 4:
                            rel = 0
                        elif ra == 5:
                            rel = 1
                        elif ra == 6:
                            rel = 1
                        elif ra == 7:
                            rel = 1

                    df['rel_score'][index] = rel

                    tweet_rel_dict[tweet_id].append(rel)

                    if tweet_id in tweet_rumor:
                        df['lable'][index] = 0
                        df['cyn'][index] = 0
                        df['gull'][index] = 0
                        if rel > 0:
                            df['gull'][index] = 1

                    else:
                        df['lable'][index] = 1
                        df['gull'][index] = 0
                        df['cyn'][index] = 0
                        if rel < 0:
                            df['cyn'][index] = 1

                df_dem = df[df['leaning'] == 1]
                df_rep = df[df['leaning'] == 2]
                df_neut = df[df['leaning'] == 3]
                rep_workers_list = list(set(df_rep['worker_id']))
                dem_workers_list = list(set(df_dem['worker_id']))
                neut_workers_list = list(set(df_neut['worker_id']))

                # random.shuffle(dem_workers_list,random.random)
                random.shuffle(dem_workers_list)
                dem_workers_list = dem_workers_list[:6]
                df_dem = df_dem[df_dem['worker_id'].isin(dem_workers_list)]

                # random.shuffle(neut_workers_list,random.random)
                random.shuffle(neut_workers_list)
                neut_workers_list = neut_workers_list[:6]
                df_neut = df_neut[df_neut['worker_id'].isin(neut_workers_list)]

                all_workers_list = []
                all_workers_list += dem_workers_list
                all_workers_list += rep_workers_list
                all_workers_list += neut_workers_list
                df = df[df['worker_id'].isin(all_workers_list)]

                groupby_ftr = 'worker_id'
                grouped = df.groupby(groupby_ftr, sort=False)
                grouped_sum = df.groupby(groupby_ftr, sort=False).sum()

                # for workerid in grouped.groups.keys():
                #     tweet_id_l = grouped.groups[workerid]
                #     for tweetid in tweet_id_l:
                #         index = df[df['tweet_id'] == tweet_id].index.tolist()[0]
                #         if df['lable'][index]==0:
                #             gull +=

                df_dem = df[df['leaning'] == 1]
                df_rep = df[df['leaning'] == 2]
                df_neut = df[df['leaning'] == 3]
                rep_workers_list = list(set(df_rep['worker_id']))
                dem_workers_list = list(set(df_dem['worker_id']))
                neut_workers_list = list(set(df_neut['worker_id']))

                df_gr = df.groupby('worker_id', sort=False)
                worker_id_l = df_gr.groups.keys()
                cc = 0
                pr_tim = 0
                cur_tim = 0

                # news_time_labling_csv.close()
                input = remotedir + local_dir_saving + 'news_labling_time.csv'
                df_time = pd.read_csv(input, sep="\t")

                # print(np.max(df['time']) - np.min(df['time']))
                sum_t = 0
                for col in all_workers_list:
                    sum_t += len(df_time[df_time[str(col)] <= 20]) / float(80)

                print(sum_t / float(len(all_workers_list)))
                groupby_ftr = 'worker_id'
                grouped_sum = df.groupby(groupby_ftr, sort=False).sum()
                grouped_sum_dem = df_dem.groupby(groupby_ftr, sort=False).sum()
                grouped_sum_rep = df_rep.groupby(groupby_ftr, sort=False).sum()
                grouped_sum_neut = df_neut.groupby(groupby_ftr, sort=False).sum()

                df_rumor = df[df['lable'] == 0]
                df_non_rumor = df[df['lable'] == 1]

                df_dem_rumor = df_dem[df_dem['lable'] == 0]
                df_dem_non_rumor = df_dem[df_dem['lable'] == 1]
                df_rep_rumor = df_rep[df_rep['lable'] == 0]
                df_rep_non_rumor = df_rep[df_rep['lable'] == 1]
                df_neut_rumor = df_neut[df_neut['lable'] == 0]
                df_neut_non_rumor = df_neut[df_neut['lable'] == 1]

                tweet_rel_score = collections.defaultdict()
                tweet_intst_score = collections.defaultdict()
                tweet_rel_score_l = []
                tweet_intst_score_l = []
                tweet_dem_rel_score_l = []
                tweet_non_rumor_intst_score_l = []
                tweet_rep_rel_score_l = []
                tweet_neut_rel_score_l = []
                tweet_rumor_intst_score_l = []
                tweet_pop = []
                tweet_pop_rumor = []
                tweet_pop_non_rumor = []

                # users='all'
                # users = 'dem'
                # users = 'rep'
                # users = 'neut'

                news_story = 'all'
                # news_story = 'rumors'
                # news_story = 'real'
                #
                if news_story == 'all':
                    df_m = df.copy()
                    tweet_rel_list = tweet_rel_dict.keys()
                elif news_story == 'rumors':
                    df_m = df[df['lable'] == 0]
                    tweet_rel_list = list(set(tweet_rumor))
                elif news_story == 'real':
                    df_m = df[df['lable'] == 1]
                    tweet_rel_list = list(set(tweet_non_rumor))

                ind_l = df_m.index.tolist()
                # df_m.iloc[ind_l]
                #             all_users_tweet_id = list(set(df.loc[ind_l]['tweet_id']))
                #             dem_users_tweet_id = list(set(df_dem.loc[ind_l]['tweet_id']))
                #             rep_users_tweet_id = list(set(df_rep.loc[ind_l]['tweet_id']))
                #             neut_users_tweet_id = list(set(df_neut.loc[ind_l]['tweet_id']))

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()

                groupby_ftr = 'worker_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum_w = df_m.groupby(groupby_ftr, sort=False).sum()

                groupby_ftr = 'tweet_id'
                grouped_dem_sum = df_dem.groupby(groupby_ftr, sort=False).sum()

                groupby_ftr = 'worker_id'
                grouped_dem_sum_w = df_dem.groupby(groupby_ftr, sort=False).sum()

                groupby_ftr = 'tweet_id'
                grouped_rep_sum = df_rep.groupby(groupby_ftr, sort=False).sum()

                groupby_ftr = 'worker_id'
                grouped_rep_sum_w = df_rep.groupby(groupby_ftr, sort=False).sum()

                groupby_ftr = 'tweet_id'
                grouped_neut_sum = df_neut.groupby(groupby_ftr, sort=False).sum()

                groupby_ftr = 'worker_id'
                grouped_neut_sum_w = df_neut.groupby(groupby_ftr, sort=False).sum()

                tweet_dem_rel_score = {}
                tweet_rep_rel_score = {}
                tweet_neut_rel_score = {}
                for tweet_id in tweet_rel_list:
                    tweet_rel_score[tweet_id] = grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w))
                    tweet_dem_rel_score[tweet_id] = grouped_dem_sum['rel_score'][tweet_id] / float(
                        len(grouped_dem_sum_w))
                    tweet_rep_rel_score[tweet_id] = grouped_rep_sum['rel_score'][tweet_id] / float(
                        len(grouped_rep_sum_w))
                    tweet_neut_rel_score[tweet_id] = grouped_neut_sum['rel_score'][tweet_id] / float(
                        len(grouped_neut_sum_w))

                # tweet_rel_score_l_sort = sorted(tweet_rel_score, key=tweet_rel_score.get,reverse=False)
                # tweet_dem_rel_score_l_sort = sorted(tweet_dem_rel_score, key=tweet_dem_rel_score.get,reverse=False)
                # tweet_rep_rel_score_l_sort = sorted(tweet_rep_rel_score, key=tweet_rep_rel_score.get,reverse=False)
                # tweet_neut_rel_score_l_sort = sorted(tweet_neut_rel_score, key=tweet_neut_rel_score.get,reverse=False)

                tweet_rel_score_l_sort = sorted(tweet_rel_score.values())
                tweet_dem_rel_score_l_sort = sorted(tweet_dem_rel_score.values())
                tweet_rep_rel_score_l_sort = sorted(tweet_rep_rel_score.values())
                tweet_neut_rel_score_l_sort = sorted(tweet_neut_rel_score.values())
                # tweet_dem_rel_score_l_sort = sorted(tweet_dem_rel_score_l)
                # tweet_rep_rel_score_l_sort = sorted(tweet_rep_rel_score_l)
                # tweet_neut_rel_score_l_sort = sorted(tweet_neut_rel_score_l)
                # tweet_rel_score_l_sort = sorted(tweet_rel_score_l)



                mplpl.scatter(range(len(tweet_rel_score_l_sort)), tweet_rel_score_l_sort, s=40, color='k', marker='o',
                              label='All users')
                mplpl.plot(range(len(tweet_rel_score_l_sort)), tweet_rel_score_l_sort, color='k')
                mplpl.scatter(range(len(tweet_dem_rel_score_l_sort)), tweet_dem_rel_score_l_sort, s=40, color='b',
                              marker='s', label='Democrats')
                mplpl.plot(range(len(tweet_dem_rel_score_l_sort)), tweet_dem_rel_score_l_sort, color='b')
                mplpl.scatter(range(len(tweet_rep_rel_score_l_sort)), tweet_rep_rel_score_l_sort, s=40, color='r',
                              marker='<', label='Republicans')
                mplpl.plot(range(len(tweet_rep_rel_score_l_sort)), tweet_rep_rel_score_l_sort, color='r')
                mplpl.scatter(range(len(tweet_neut_rel_score_l_sort)), tweet_neut_rel_score_l_sort, s=40, color='g',
                              marker='^', label='Neutrals')
                mplpl.plot(range(len(tweet_neut_rel_score_l_sort)), tweet_neut_rel_score_l_sort, color='g')
                mplpl.xlabel('News')
                mplpl.ylabel('Reliability')
                # mplpl.title('All users, Mean : ' + str(np.mean(dem_gull_list_sort)))
                # mplpl.ylim([0,1])
                mplpl.legend(loc="upper left")
                if approach == 'weighted':
                    pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_' + news_story + '_reliability_news_1'
                elif approach == 'non-weighted':
                    pp = remotedir + local_dir_saving + '/fig_exp2/' + news_story + '_reliability_news_1'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

                num_bins = len(tweet_rel_score_l_sort)
                counts, bin_edges = np.histogram(tweet_rel_score_l_sort, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='k', lw=3, label='All users')

                num_bins = len(tweet_dem_rel_score_l_sort)
                counts, bin_edges = np.histogram(tweet_dem_rel_score_l_sort, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='b', lw=3, label='Democrats')

                num_bins = len(tweet_rep_rel_score_l_sort)
                counts, bin_edges = np.histogram(tweet_rep_rel_score_l_sort, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='r', lw=3, label='Republicans')

                num_bins = len(tweet_neut_rel_score_l_sort)
                counts, bin_edges = np.histogram(tweet_neut_rel_score_l_sort, bins=num_bins, normed=True)
                cdf = np.cumsum(counts)
                scale = 1.0 / cdf[-1]
                ncdf = scale * cdf
                mplpl.plot(bin_edges[1:], ncdf, c='g', lw=3, label='Neutrals')

                mplpl.xlabel('Reliability')
                mplpl.ylabel('CDF')
                # mplpl.title('Neut users, Mean : ' + str(np.mean(all_gull_list)))
                # mplpl.xlim([0,1])
                # mplpl.ylim([0,1])
                mplpl.legend(loc="upper left")
                if approach == 'weighted':
                    pp = remotedir + local_dir_saving + '/fig_exp2_weighted/weighted_' + news_story + '_reliability_CDF_1'
                elif approach == 'non-weighted':
                    pp = remotedir + local_dir_saving + '/fig_exp2/' + news_story + '_reliability_CDF_1'
                mplpl.savefig(pp, format='png')
                mplpl.figure()

    if args.t == "AMT_dataset_reliable_news_processing_exp2_news_users_dist_visulisation":
            publisher_leaning = 1
            threshold = 10
            local_dir_saving = ''
            sp = 'sp_all_'
            # sp = 'sp_'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/data/'
            # remotedir = 'reliable_news/data/'


            final_inp_exp1 = open(remotedir + local_dir_saving
                                  + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir + local_dir_saving + 'non_rumer_tweets', 'r')

            input = remotedir + local_dir_saving + 'news_labling_time.csv'
            df_time = pd.read_csv(input, sep="\t")

            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            run = 'plot'
            # run = 'analysis'
            exp1_list = sample_tweets_exp1

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id) < 100060:
                    tweet_rumor[tweet_id] = -1
                else:
                    tweet_non_rumor[tweet_id] = 1

            if run == 'analysis':

                print("analysis")


            else:
                approach = 'weighted'
                # approach = 'non-weighted'
                # outF = open(remotedir+local_dir_saving+'amt_consensus_output_wiki_offensive_2.txt','w')

                input = remotedir + local_dir_saving + 'amt_answers_reliable_news_exp2.csv'
                df = pd.read_csv(input, sep="\t")
                tweet_dem_score = {};
                tweet_rep_score = {};
                tweet_cons_score = {}
                tweet_cons_score_1 = {};
                tweet_cons_score_2 = {};
                tweet_cons_score_3 = {}

                # sum_feat = df.groupby(groupby_ftr).sum()
                # count_feat = df.groupby(groupby_ftr).count()

                df.loc[:, 'rel_score'] = df['tweet_id'] * 0.0
                df.loc[:, 'intst_score'] = df['tweet_id'] * 0.0
                df.loc[:, 'lable'] = df['tweet_id'] * 0.0
                df.loc[:, 'delta_time'] = df['tweet_id'] * 0.0

                df.loc[:, 'gull'] = df['tweet_id'] * 0.0
                df.loc[:, 'cyn'] = df['tweet_id'] * 0.0
                df.loc[:, 'susc'] = df['tweet_id'] * 0.0
                tweet_rel_dict = collections.defaultdict(list)
                tweet_intst_dict = collections.defaultdict(list)
                rel_4 = 0
                for index in df.index.tolist():
                    tweet_id = df['tweet_id'][index]
                    if tweet_id == 1:
                        continue

                    ra = df['ra'][index]
                    if approach == 'weighted':
                        if ra == 1:
                            rel = -3
                        elif ra == 2:
                            rel = -2
                        elif ra == 3:
                            rel = -1
                        elif ra == 4:
                            rel = 0
                        elif ra == 5:
                            rel = 1
                        elif ra == 6:
                            rel = 2
                        elif ra == 7:
                            rel = 3

                    elif approach == 'non-weighted':
                        if ra == 1:
                            rel = -1
                        elif ra == 2:
                            rel = -1
                        elif ra == 3:
                            rel = -1
                        elif ra == 4:
                            rel = 0
                        elif ra == 5:
                            rel = 1
                        elif ra == 6:
                            rel = 1
                        elif ra == 7:
                            rel = 1

                    df['rel_score'][index] = rel

                    tweet_rel_dict[tweet_id].append(rel)

                    if tweet_id in tweet_rumor:
                        df['lable'][index] = -1
                        # df['cyn'][index] = -1
                        # df['gull'][index] = 0
                        # if rel > 0:
                        # df['gull'][index] = rel/float(3)

                    else:
                        df['lable'][index] = 1
                        # df['gull'][index] = -1
                        # df['cyn'][index] = 0
                        # if rel < 0:
                        # df['cyn'][index] = -1* rel/float(3)

                    l_scr = df['lable'][index]
                    k_1 = 1
                    k_2 = 3

                    gull_val = rel/float(k_2) - l_scr/float(k_1)
                    gull_val = gull_val * ((np.sign(gull_val) +1) /float(2))

                    cyn_val = l_scr/float(k_1) - rel/float(k_2)
                    cyn_val = cyn_val * ((np.sign(cyn_val) +1) /float(2))

                    df['gull'][index] = gull_val/float(2)
                    df['cyn'][index] = cyn_val/float(2)
                    df['susc'][index] = (df['cyn'][index] + df['gull'][index])/float(2)

                df_dem = df[df['leaning'] == 1]
                df_rep = df[df['leaning'] == 2]
                df_neut = df[df['leaning'] == 3]
                rep_workers_list = list(set(df_rep['worker_id']))
                dem_workers_list = list(set(df_dem['worker_id']))
                neut_workers_list = list(set(df_neut['worker_id']))

                # random.shuffle(dem_workers_list,random.random)
                random.shuffle(dem_workers_list)
                dem_workers_list = dem_workers_list[:6]
                df_dem = df_dem[df_dem['worker_id'].isin(dem_workers_list)]

                # random.shuffle(neut_workers_list,random.random)
                random.shuffle(neut_workers_list)
                neut_workers_list = neut_workers_list[:6]
                df_neut = df_neut[df_neut['worker_id'].isin(neut_workers_list)]

                all_workers_list = []
                all_workers_list += dem_workers_list
                all_workers_list += rep_workers_list
                all_workers_list += neut_workers_list
                df = df[df['worker_id'].isin(all_workers_list)]

                groupby_ftr = 'worker_id'
                grouped = df.groupby(groupby_ftr, sort=False)
                grouped_sum = df.groupby(groupby_ftr, sort=False).sum()

                # for workerid in grouped.groups.keys():
                #     tweet_id_l = grouped.groups[workerid]
                #     for tweetid in tweet_id_l:
                #         index = df[df['tweet_id'] == tweet_id].index.tolist()[0]
                #         if df['lable'][index]==0:
                #             gull +=

                df_dem = df[df['leaning'] == 1]
                df_rep = df[df['leaning'] == 2]
                df_neut = df[df['leaning'] == 3]
                rep_workers_list = list(set(df_rep['worker_id']))
                dem_workers_list = list(set(df_dem['worker_id']))
                neut_workers_list = list(set(df_neut['worker_id']))

                workers_leaning = {}
                for workerid in list(df['worker_id'][:]):
                    if workerid in dem_workers_list:
                        workers_leaning[workerid] = 'Dem'
                    elif workerid in rep_workers_list:
                        workers_leaning[workerid] = 'Rep'
                    if workerid in neut_workers_list:
                        workers_leaning[workerid] = 'Neut'

                df_gr = df.groupby('worker_id', sort=False)
                worker_id_l = df_gr.groups.keys()
                cc = 0
                pr_tim = 0
                cur_tim = 0

                # news_time_labling_csv.close()
                input = remotedir + local_dir_saving + 'news_labling_time.csv'
                df_time = pd.read_csv(input, sep="\t")



                # users_or_news = 'news'
                users_or_news = 'users'

                if users_or_news=='news':
                    users = 'all'
                    # users = 'dem'
                    # users = 'rep'
                    # users = 'neut'
                    #

                    if users == 'all':
                        df_m = df.copy()
                        col = 'k'
                    elif users == 'dem':
                        df_m = df_dem.copy()
                        col = 'b'
                    elif users == 'rep':
                        df_m = df_rep.copy()
                        col = 'r'
                    elif users == 'neut':
                        df_m = df_neut.copy()
                        col = 'g'

                    tot_len = len(set(df_m['tweet_id']))
                    # for tweet_id in list(set(df_m['tweet_id'])):
                    #     if tweet_id==1:
                    #         continue
                    #     df_tmp = df_m[df_m['tweet_id']==tweet_id]
                    #     weights = []
                    #     weights.append(np.ones_like(list(df_tmp['rel_score'])) / float(len(df_tmp)))
                    #
                    #     try:
                    #         df_tmp['rel_score'].plot(kind='kde', lw=4, color=col)
                    #     except:
                    #         print('hmm')
                    #
                    #     mplpl.hist(list(df_tmp['rel_score']), weights=weights, color=col)
                    #     # mplpl.grid()
                    #     mplpl.xlabel('Reliability score', fontsize='large')
                    #     mplpl.ylabel('Density', fontsize='large')
                    #     mplpl.title(users + ' : ' + str(np.round(np.mean(list(df_tmp['rel_score'])),3)), fontsize='small')
                    #     mplpl.xlim([-6, 6])
                    #     mplpl.ylim([0, 1])
                    #     if approach == 'weighted':
                    #         pp = remotedir + local_dir_saving + '/fig_exp2_weighted/news_dist_fig/' + str(tweet_id) + '_reliability_pdf_' + users
                    #     elif approach == 'non-weighted':
                    #         pp = remotedir + local_dir_saving + '/fig_exp2/news_dist_fig/' + str(tweet_id) + '_reliability_pdf_' + users
                    #     mplpl.savefig(pp, format='png')
                    #     mplpl.figure()


                    # exit()

                    groupby_ftr = 'tweet_id'
                    grouped_mean = df_m.groupby(groupby_ftr, sort=False).mean()
                    tweet_rel_score = {}
                    tweet_rel_score_rumor = {}
                    tweet_rel_score_non_rumor = {}
                    for tweet_id in tweet_rumor:
                        tweet_rel_score_rumor[tweet_id] = grouped_mean['rel_score'][tweet_id]
                        tweet_rel_score[tweet_id] = grouped_mean['rel_score'][tweet_id]

                    for tweet_id in tweet_non_rumor:
                        tweet_rel_score_non_rumor[tweet_id] = grouped_mean['rel_score'][tweet_id]
                        tweet_rel_score[tweet_id] = grouped_mean['rel_score'][tweet_id]

                    tweet_rel_score_l_rumor_sort = sorted(tweet_rel_score_rumor, key=tweet_rel_score_rumor.get,reverse=True)
                    tweet_rel_score_l_non_rumor_sort = sorted(tweet_rel_score_non_rumor, key=tweet_rel_score_non_rumor.get,reverse=False)
                    tweet_rel_score_l_sort = sorted(tweet_rel_score, key=tweet_rel_score.get,reverse=False)

                    if approach=='non-weighted':
                        outF = open(remotedir + local_dir_saving + 'news_dist_non-weighted.txt', 'w')
                    elif approach=='weighted':
                        outF = open(remotedir + local_dir_saving + 'news_dist_weighted.txt', 'w')
                    outF.write('|| || tweet id || tweet text || rumor or not || Reliability score || popularity ||' +
                               'all users dist || Dem dist || Rep dist || Neut dist||\n')


                    ind_c = 0
                    for tweet_id in tweet_rel_score_l_sort:
                        if tweet_id ==1:
                            continue
                        fake_real = 'rumor'
                        ind_c+=1
                        if tweet_id in tweet_rumor:
                            fake_real = 'rumor'
                        else:
                            fake_real = 'non_rumor'
                        if approach == 'non-weighted':
                            pp = 'http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + 'fig_exp2/news_dist_fig/'\
                                 + str(tweet_id) + '_reliability_pdf_'
                        elif approach == 'weighted':
                            pp = 'http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + 'fig_exp2_weighted/news_dist_fig/'\
                                 + str(tweet_id) + '_reliability_pdf_'

                        outF.write('||' + str(ind_c) + '||' +   str(tweet_id) + '||' + tweet_text_dic[tweet_id] + '||' + fake_real + '||' +
                                   str(tweet_rel_score[tweet_id]) + '||' + str(tweet_popularity[tweet_id]) + '||'+
                                   '{{' +  pp + 'all| alt text| width = 500px}}' + '||' +
                                   '{{' + pp + 'dem| alt text| width = 500px}}' + '||' +
                                   '{{' + pp + 'rep| alt text| width = 500px}}' + '||' +
                                   '{{' + pp + 'neut| alt text| width = 500px}}' + '||\n')

                    outF.write('\n')
                    outF.write('|| || tweet id || tweet text || rumor or not || Reliability score || popularity ||' +
                               'all users dist || Dem dist || Rep dist || Neut dist||\n')

                    # exit()
                    ind_c = 0
                    for tweet_id in tweet_rel_score_l_rumor_sort:
                        if tweet_id ==1:
                            continue
                        fake_real = 'rumor'
                        ind_c+=1
                        if tweet_id in tweet_rumor:
                            fake_real = 'rumor'
                        else:
                            fake_real = 'non_rumor'
                        if approach == 'non-weighted':
                            pp = 'http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + 'fig_exp2/news_dist_fig/'\
                                 + str(tweet_id) + '_reliability_pdf_'
                        elif approach == 'weighted':
                            pp = 'http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + 'fig_exp2_weighted/news_dist_fig/'\
                                 + str(tweet_id) + '_reliability_pdf_'

                        outF.write('||' + str(ind_c) + '||' +   str(tweet_id) + '||' + tweet_text_dic[tweet_id] + '||' + fake_real + '||' +
                                   str(tweet_rel_score[tweet_id]) + '||' + str(tweet_popularity[tweet_id]) + '||'+
                                   '{{' +  pp + 'all| alt text| width = 500px}}' + '||' +
                                   '{{' + pp + 'dem| alt text| width = 500px}}' + '||' +
                                   '{{' + pp + 'rep| alt text| width = 500px}}' + '||' +
                                   '{{' + pp + 'neut| alt text| width = 500px}}' + '||\n')

                    outF.write('\n')
                    outF.write('|| || tweet id || tweet text || rumor or not || Reliability score || popularity ||' +
                               'all users dist || Dem dist || Rep dist || Neut dist||\n')
                    ind_c = 0
                    for tweet_id in tweet_rel_score_l_non_rumor_sort:
                        if tweet_id ==1:
                            continue
                        fake_real = 'rumor'
                        ind_c+=1
                        if tweet_id in tweet_rumor:
                            fake_real = 'rumor'
                        else:
                            fake_real = 'non_rumor'
                        if approach == 'non-weighted':
                            pp = 'http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + 'fig_exp2/news_dist_fig/'\
                                 + str(tweet_id) + '_reliability_pdf_'
                        elif approach == 'weighted':
                            pp = 'http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + 'fig_exp2_weighted/news_dist_fig/'\
                                 + str(tweet_id) + '_reliability_pdf_'

                        outF.write('||'+ str(ind_c) + '||' +   str(tweet_id) + '||' + tweet_text_dic[tweet_id] + '||' + fake_real + '||' +
                                   str(tweet_rel_score[tweet_id]) + '||' + str(tweet_popularity[tweet_id]) + '||'+
                                   '{{' +  pp + 'all| alt text| width = 500px}}' + '||' +
                                   '{{' + pp + 'dem| alt text| width = 500px}}' + '||' +
                                   '{{' + pp + 'rep| alt text| width = 500px}}' + '||' +
                                   '{{' + pp + 'neut| alt text| width = 500px}}' + '||\n')





                elif users_or_news =='users':
                    approach = 'non-weighted'

                    news_stories = ['all', 'rumors', 'real']
                    #
                    for news_story in news_stories:
                        if news_story == 'all':
                            df_m = df.copy()
                            col = 'k'
                            tweet_rel_list = tweet_rel_dict.keys()
                        elif news_story == 'rumors':
                            df_m = df[df['lable'] == -1]
                            tweet_rel_list = list(set(tweet_rumor))
                            col = 'c'
                        elif news_story == 'real':
                            df_m = df[df['lable'] == 1]
                            tweet_rel_list = list(set(tweet_non_rumor))
                            col = 'g'

                        for user_id in list(set(df_m['worker_id'])):
                            df_tmp = df_m[df_m['worker_id']==user_id]
                            weights = []
                            weights.append(np.ones_like(list(df_tmp['rel_score'])) / float(len(df_tmp)))

                            try:
                                df_tmp['rel_score'].plot(kind='kde', lw=4, color=col)
                            except:
                                print('hmm')

                            mplpl.hist(list(df_tmp['rel_score']), weights=weights, color=col)
                            labels = ['', '', '', 'Confirm it to be a rumor', 'Very likely to be a rumor', 'Possibly rumor',
                                      'Can\'t tell', 'Possibly real', 'Very likely to be real', 'Confirm it to be real','', '', '']
                            x = range(-6,7)
                            mplpl.xticks(x, labels, rotation='90')
                            mplpl.subplots_adjust(bottom=0.35)

                            # mplpl.grid()
                            # mplpl.xlabel('Reliability score', fontsize='large')
                            mplpl.ylabel('Density', fontsize='large')
                            mplpl.title(news_story + ' reliability mean : ' + str(np.round(np.mean(list(df_tmp['rel_score'])),3)), fontsize='small')
                            mplpl.xlim([-6, 6])
                            mplpl.ylim([0, 1])
                            if approach == 'weighted':
                                pp = remotedir + local_dir_saving + '/fig_exp2_weighted/users_dist_fig/' + str(user_id) + '_susceptibility_pdf_' + news_story
                            elif approach == 'non-weighted':
                                pp = remotedir + local_dir_saving + '/fig_exp2/users_dist_fig/' + str(user_id) + '_susceptibility_pdf_' + news_story
                            mplpl.savefig(pp, format='png')
                            mplpl.figure()


                    # news_story = 'rumors'
                    # news_story = 'real'
                    # #
                    # if news_story == 'all':
                    #     df_m = df.copy()
                    #     col = 'k'
                    #     tweet_rel_list = tweet_rel_dict.keys()
                    # elif news_story == 'rumors':
                    #     df_m = df[df['lable'] == 0]
                    #     tweet_rel_list = list(set(tweet_rumor))
                    #     col = 'c'
                    # elif news_story == 'real':
                    #     df_m = df[df['lable'] == 1]
                    #     tweet_rel_list = list(set(tweet_non_rumor))
                    #     col = 'g'
                    #
                    # for user_id in list(set(df_m['worker_id'])):
                    #     df_tmp = df_m[df_m['worker_id']==user_id]
                    #     weights = []
                    #     weights.append(np.ones_like(list(df_tmp['rel_score'])) / float(len(df_tmp)))
                    #
                    #     try:
                    #         df_tmp['rel_score'].plot(kind='kde', lw=4, color=col)
                    #     except:
                    #         print('hmm')
                    #
                    #     mplpl.hist(list(df_tmp['rel_score']), weights=weights, color=col)
                    #     # mplpl.grid()
                    #     labels = ['', '', '', 'Confirm it to be a rumor', 'Very likely to be a rumor', 'Possibly rumor',
                    #               'Can\'t tell', 'Possibly real', 'Very likely to be real', 'Confirm it to be real','', '', '']
                    #     x = range(-6,7)
                    #     mplpl.xticks(x, labels, rotation='90')
                    #     mplpl.subplots_adjust(bottom=0.35)
                    #
                    #     # mplpl.xlabel('Reliability score', fontsize='large')
                    #     mplpl.ylabel('Density', fontsize='large')
                    #     mplpl.title(news_story + ' : ' + str(np.round(np.mean(list(df_tmp['rel_score'])),3)), fontsize='small')
                    #     mplpl.xlim([-6, 6])
                    #     mplpl.ylim([0, 1])
                    #     if approach == 'weighted':
                    #         pp = remotedir + local_dir_saving + '/fig_exp2_weighted/users_dist_fig/' + str(user_id) + '_cynicality_pdf_' + news_story
                    #     elif approach == 'non-weighted':
                    #         pp = remotedir + local_dir_saving + '/fig_exp2/users_dist_fig/' + str(user_id) + '_cynicality_pdf_' + news_story
                    #     mplpl.savefig(pp, format='png')
                    #     mplpl.figure()
                    #

                    # dem_workers_list
                    # rep_workers_list
                    # all_workers_list += neut_workers_list
                    # df = df[df['worker_id'].isin(all_workers_list)]
                    # exit()
                    groupby_ftr = 'worker_id'
                    grouped = df.groupby(groupby_ftr, sort=False)
                    grouped_mean = df.groupby(groupby_ftr, sort=False).mean()

                    user_susc_dict= {}
                    user_gull_dict = {}
                    user_cyn_dict = {}
                    u_susc_list = []
                    u_cyn_list = []
                    u_gull_list = []
                    for user_id in set(list(df['worker_id'])):
                        u_gull = grouped_mean['gull'][user_id]
                        u_cyn = grouped_mean['cyn'][user_id]

                        u_susc = grouped_mean['susc'][user_id]

                        # u_gull_list.append(u_gull)
                        # u_cyn_list.append(u_cyn)
                        # u_susc_list.append(u_susc)

                        user_susc_dict[user_id] = u_susc
                        user_gull_dict[user_id] = u_gull
                        user_cyn_dict[user_id] = u_cyn

                    user_susc_sort = sorted(user_susc_dict, key=user_susc_dict.get,reverse=True)
                    user_gull_sort = sorted(user_gull_dict, key=user_gull_dict.get,reverse=True)
                    user_cyn_sort = sorted(user_cyn_dict, key=user_cyn_dict.get, reverse=True)


                    u_susc_list = []
                    u_cyn_list = []
                    u_gull_list = []
                    # for usrs_lst in [dem_workers_list, rep_workers_list, neut_workers_list]:

                    for user_id in user_susc_sort:
                        # if user_id not in usrs_lst:
                        #     continue
                        u_gull_list.append(user_gull_dict[user_id])
                        u_cyn_list.append(user_cyn_dict[user_id])
                        u_susc_list.append(user_susc_dict[user_id])


                    print(np.corrcoef(u_gull_list, u_cyn_list)[0][1])
                    print(np.corrcoef(u_susc_list, u_cyn_list)[0][1])
                    print(np.corrcoef(u_gull_list, u_susc_list)[0][1])

                    if approach == 'non-weighted':
                        outF = open(remotedir + local_dir_saving + 'users_dist_non-weighted.txt', 'w')
                    elif approach == 'weighted':
                        outF = open(remotedir + local_dir_saving + 'users_dist_weighted.txt', 'w')

                    ind_c = 0
                    # for usrs_lst in [dem_workers_list, rep_workers_list, neut_workers_list]:
                    outF.write('|| || usre id || Leaning ||Susceptiblity score || Gullibility score || Cynicallity score ||' +
                               'Labling dist for all news stories || Labling dist for real news stories || Labling dist for rumors news stories ||\n')

                    usrs_lst = dem_workers_list
                    usrs_lst = rep_workers_list
                    usrs_lst = neut_workers_list
                    for user_id in user_susc_sort:
                        if user_id not in usrs_lst:
                            continue

                        fake_real = 'rumor'
                        ind_c += 1
                        if tweet_id in tweet_rumor:
                            fake_real = 'rumor'
                        else:
                            fake_real = 'non_rumor'
                        if approach == 'non-weighted':
                            pp_all = 'http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + 'fig_exp2/users_dist_fig/' \
                                 + str(user_id) + '_susceptibility_pdf_'
                            # pp_g = 'http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + 'fig_exp2/users_dist_fig/' \
                            #      + str(user_id) + '_susceptibility_pdf_'
                            # pp_c = 'http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + 'fig_exp2/users_dist_fig/' \
                            #      + str(user_id) + '_susceptibility_pdf_'
                        elif approach == 'weighted':
                            pp_all = 'http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + 'fig_exp2_weighted/users_dist_fig/' \
                                 + str(user_id) + '_susceptibility_pdf_'
                            # pp_c = 'http://www.mpi-sws.mpg.de/~babaei/result-fig/reliability_news/' + 'fig_exp2_weighted/users_dist_fig/' \
                            #      + str(user_id) + '_susceptibility_pdf_'

                        outF.write('||' + str(ind_c) + '||' + str(user_id) + '||' + str(workers_leaning[user_id]) + '||' + str(user_susc_dict[user_id])+ '||' +
                                   str(user_gull_dict[user_id]) + '||' + str(user_cyn_dict[user_id]) + '||' +
                                   '{{' + pp_all + 'all| alt text| width = 500px}}' + '||' +
                                   '{{' + pp_all + 'rumors| alt text| width = 500px}}' + '||' +
                                   '{{' + pp_all + 'real| alt text| width = 500px}}' + '||\n')

                    outF.write('\n')

                # ind_l = df_m.index.tolist()
                # df_m.iloc[ind_l]
                #             all_users_tweet_id = list(set(df.loc[ind_l]['tweet_id']))
                #             dem_users_tweet_id = list(set(df_dem.loc[ind_l]['tweet_id']))
                #             rep_users_tweet_id = list(set(df_rep.loc[ind_l]['tweet_id']))
                #             neut_users_tweet_id = list(set(df_neut.loc[ind_l]['tweet_id']))



    if args.t == "AMT_dataset_reliable_news_processing_diff_topic_exp2":

        publisher_leaning = 1
        threshold = 10
        local_dir_saving = ''
        sp = 'sp_all_'
        # sp = 'sp_'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/data/'


        final_inp_exp1 = open(remotedir + local_dir_saving
                                 + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

        sample_tweets_exp1 = json.load(final_inp_exp1)

        input_rumor = open(remotedir + local_dir_saving + 'rumer_tweets', 'r')
        input_non_rumor = open(remotedir + local_dir_saving + 'non_rumer_tweets','r')





        tweet_id = 100010
        publisher_name = 110
        tweet_popularity = {}
        tweet_text_dic = {}
        for input_file in [input_rumor, input_non_rumor]:
            for line in input_file:
                line.replace('\n','')
                line_splt = line.split('\t')
                tweet_txt = line_splt[1]
                tweet_link = line_splt[1]
                tweet_id+=1
                publisher_name+=1
                tweet_popularity[tweet_id] = int(line_splt[2])
                tweet_text_dic[tweet_id] = tweet_txt

        # query1 = "select workerid, count(*) from mturk_crowd_signal_consensu_tweet_response_test group by workerid;"
        # query2 = "update mturk_crowd_signal_consensu_tweet_response_test set " \
        #          "workerid = mturk_crowd_signal_consensu_tweet_response_test.workerid + 100 from mturk_crowd_signal_consensu_tweet_response_test"

        # cursor.execute(query2)
        # test = cursor.fetchall()
        # mturk_crowd_signal_consensu_tweet_response_test



        run = 'plot'
        # run = 'analysis'
        exp1_list = sample_tweets_exp1


        out_list = []
        cnn_list = []
        foxnews_list = []
        ap_list = []
        tweet_txt_dict = {}
        tweet_link_dict = {}
        tweet_publisher_dict = {}
        tweet_rumor= {}
        tweet_non_rumor = {}
        pub_dict = collections.defaultdict(list)
        for tweet in exp1_list:

            tweet_id = tweet[0]
            publisher_name = tweet[1]
            tweet_txt = tweet[2]
            tweet_link = tweet[3]
            tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
            tweet_txt_dict[tweet_id] = tweet_txt
            tweet_link_dict[tweet_id] = tweet_link
            tweet_publisher_dict[tweet_id] = publisher_name
            if int(tweet_id)<100060:
                tweet_rumor[tweet_id]=-1
            else:
                tweet_non_rumor[tweet_id]=1

        if run == 'analysis':

            print("analysis")


        else:
            tweetid_relscor = {}
            input = remotedir + local_dir_saving  + 'amt_answers_reliable_news_exp2.csv'
            df = pd.read_csv(input, sep="\t")
            tweet_dem_score = {};
            tweet_rep_score = {};
            tweet_cons_score = {}
            tweet_cons_score_1 = {};
            tweet_cons_score_2 = {};
            tweet_cons_score_3 = {}


            # sum_feat = df.groupby(groupby_ftr).sum()
            # count_feat = df.groupby(groupby_ftr).count()

            df.loc[:, 'rel_score'] = df['tweet_id'] * 0.0
            df.loc[:, 'lable'] = df['tweet_id'] * 0.0

            df.loc[:, 'gull'] = df['tweet_id'] * 0.0
            df.loc[:, 'cyn'] = df['tweet_id'] * 0.0
            tweet_rel_dict = collections.defaultdict(list)
            tweet_intst_dict = collections.defaultdict(list)

            for index in df.index.tolist():
                tweet_id = df['tweet_id'][index]
                if tweet_id == 1:
                    continue

                ra = df['ra'][index]

                ra = df['ra'][index]
                # if ra==1:
                #     rel = -3
                # elif ra==2:
                #     rel=-2
                # elif ra==3:
                #     rel=-1
                # elif ra==4:
                #     rel=0
                # elif ra==5:
                #     rel = 1
                # elif ra==6:
                #     rel = 2
                # elif ra==7:
                #     rel = 3
                #

                if ra==1:
                    rel = -1
                elif ra==2:
                    rel=-1
                elif ra==3:
                    rel=-1
                elif ra==4:
                    rel=0
                elif ra==5:
                    rel = 1
                elif ra==6:
                    rel = 1
                elif ra==7:
                    rel = 1



                df['rel_score'][index] = rel

                tweet_rel_dict[tweet_id].append(rel)

                if tweet_id in tweet_rumor:
                    df['lable'][index] = 0
                    df['cyn'][index] = 0
                    df['gull'][index] = 0
                    if rel >0:
                        df['gull'][index] = 1

                else:
                    df['lable'][index] = 1
                    df['gull'][index] = 0
                    df['cyn'][index] = 0
                    if rel < 0:
                        df['cyn'][index] = 1


            groupby_ftr = 'tweet_id'
            grouped = df.groupby(groupby_ftr, sort=False)
            grouped_sum = df.groupby(groupby_ftr, sort=False).sum()

            groupby_ftr = 'worker_id'
            grouped = df.groupby(groupby_ftr, sort=False)
            grouped_sum_w = df.groupby(groupby_ftr, sort=False).sum()

            tweet_rel_score={}
            for tweet_id in tweet_rel_dict:

                tweet_rel_score[tweet_id] = grouped_sum['rel_score'][tweet_id] / float(len(grouped_sum_w))

            tweetid_rel = {}
            rumor_outF = open(remotedir + local_dir_saving + 'rumor_outF_exp2.txt', 'r')
            non_rumor_outF = open(remotedir + local_dir_saving + 'non-rumor_outF_exp2.txt', 'r')
            non_rumor_tid_l = []
            rumor_tid_l = []
            for line in rumor_outF:
                line_splt = line.split('||')
                rumor_tid_l.append(int(line_splt[1]))
                tweetid_rel[int(line_splt[1])] = float(line_splt[4])
            for line in non_rumor_outF:
                line_splt = line.split('||')
                non_rumor_tid_l.append(int(line_splt[1]))
                tweetid_rel[int(line_splt[1])] = float(line_splt[4])

            rumor_inF = open('rumor_news.txt', 'r')
            non_rumor_inF = open('non-rumor_news.txt', 'r')

            tweet_id_l = []
            tweet_txt_l = []
            rel_score_l = []
            intst_score_l = []
            popularity_l = []
            politic_l = []
            helath_l = []
            tech_l = []
            myth_l = []
            death_l = []
            work_l = []
            personal_l = []
            entertainment_l = []
            food_l = []
            andimal_l = []
            rumor_l = []
            f_count=0
            for inpF in [rumor_inF, non_rumor_inF]:
                cc=0
                for line in inpF:
                    line_splt = line.split('||')
                    if cc>0:
                        # print(cc)
                        if f_count==0:
                            tweet_id = rumor_tid_l[cc-1]
                        else:
                            tweet_id = non_rumor_tid_l[cc-1]


                        tweet_txt = line_splt[1]

                        if 'Non' in line_splt[2]:
                            rumor = 0
                        else:
                            rumor=1
                        rel_score = float(line_splt[3])
                        intst_score = float(line_splt[4])
                        popularity = int(line_splt[5])
                        politic = int(line_splt[6])
                        health = int(line_splt[7])
                        tech = int(line_splt[8])
                        myth = int(line_splt[9])
                        death = int(line_splt[10])
                        work = int(line_splt[11])
                        personal = int(line_splt[12])
                        entertainment = int(line_splt[13])
                        food = int(line_splt[14])
                        animal = int(line_splt[15])


                        tweet_id_l.append(tweet_id)
                        tweet_txt_l.append(tweet_txt)
                        rumor_l.append(rumor)
                        rel_score_l.append(tweet_rel_score[tweet_id])
                        # intst_score_l.append(intst_score)
                        popularity_l.append(popularity)
                        politic_l.append(politic)
                        helath_l.append(health)
                        tech_l.append(tech)
                        myth_l.append(myth)
                        death_l.append(death)
                        work_l.append(work)
                        personal_l.append(personal)
                        entertainment_l.append(entertainment)
                        food_l.append(food)
                        andimal_l.append(animal)
                    cc+=1
                f_count+=1

            index_list_m = range(80)
            df_topics = pd.DataFrame({'tweet_id': Series(tweet_id_l, index=index_list_m),
                               'text': Series(tweet_txt_l, index=index_list_m),
                               'rel_score': Series(rel_score_l, index=index_list_m),
                               'rumor': Series(rumor_l, index=index_list_m),
                               # 'intst_score': Series(intst_score_l, index=index_list_m),
                                'popularity' : Series(popularity_l, index=index_list_m),
                                'politic' : Series(politic_l, index=index_list_m),
                                'helath':Series(helath_l, index=index_list_m),
                                'tech' : Series(tech_l, index=index_list_m),
                                'myth' : Series(myth_l, index=index_list_m),
                                'death' : Series(death_l, index=index_list_m),
                                'work' : Series(work_l, index=index_list_m),
                                'personal' : Series(personal_l, index=index_list_m),
                                'entertainment' : Series(entertainment_l, index=index_list_m),
                                'food' : Series(food_l, index=index_list_m),
                                'animal' : Series(andimal_l, index=index_list_m)})

            topic_list = ['politic', 'helath', 'tech', 'myth', 'death', 'work', 'personal', 'entertainment', 'food', 'animal']




            groupby_ftr = 'worker_id'
            grouped = df.groupby(groupby_ftr, sort=False)



            groupby_ftr = 'worker_id'
            grouped = df.groupby(groupby_ftr, sort=False)
            grouped_sum = df.groupby(groupby_ftr, sort=False).sum()

            # for workerid in grouped.groups.keys():
            #     tweet_id_l = grouped.groups[workerid]
            #     for tweetid in tweet_id_l:
            #         index = df[df['tweet_id'] == tweet_id].index.tolist()[0]
            #         if df['lable'][index]==0:
            #             gull +=

            df_dem = df[df['leaning']==1]
            df_rep = df[df['leaning']==2]
            df_neut = df[df['leaning']==3]
            rep_workers_list = list(set(df_rep['worker_id']))
            dem_workers_list = list(set(df_dem['worker_id']))
            neut_workers_list = list(set(df_neut['worker_id']))


            random.shuffle(dem_workers_list,random.random)
            dem_workers_list = dem_workers_list[:6]
            df_dem = df_dem[df_dem['worker_id'].isin(dem_workers_list)]

            random.shuffle(neut_workers_list,random.random)
            neut_workers_list = neut_workers_list[:6]
            df_neut = df_neut[df_neut['worker_id'].isin(neut_workers_list)]


            grouped_sum_dem = df_dem.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_rep = df_rep.groupby(groupby_ftr, sort=False).sum()
            grouped_sum_neut = df_neut.groupby(groupby_ftr, sort=False).sum()

            df_rumor = df[df['lable']==0]
            df_non_rumor = df[df['lable']==1]

            df_dem_rumor = df_dem[df_dem['lable']==0]
            df_dem_non_rumor = df_dem[df_dem['lable']==1]
            df_rep_rumor = df_rep[df_rep['lable']==0]
            df_rep_non_rumor = df_rep[df_rep['lable']==1]
            df_neut_rumor = df_neut[df_neut['lable']==0]
            df_neut_non_rumor = df_neut[df_neut['lable']==1]

            all_gull_list = [x/float(50) for x in list(grouped_sum['gull'])]
            all_cyn_list = [x/float(30) for x in list(grouped_sum['cyn'])]

            # mplpl.hist(all_gull_list, bins=10, color='b')
            # mplpl.xlabel('Gullibility')
            # mplpl.ylabel('Frequency')
            # mplpl.title('All users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/all_gullibility'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            # mplpl.hist(all_cyn_list, bins=10, color='r')
            # mplpl.xlabel('Cynicality')
            # mplpl.ylabel('Frequency')
            # mplpl.title('All users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/all_cynically'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            #
            #
            # all_gull_list = [x/float(50) for x in list(grouped_sum_dem['gull'])]
            # all_cyn_list = [x/float(30) for x in list(grouped_sum_dem['cyn'])]
            #
            # mplpl.hist(all_gull_list, bins=10, color='b')
            # mplpl.xlabel('Gullibility')
            # mplpl.ylabel('Frequency')
            # mplpl.title('Dem users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/dem_gullibility'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            # mplpl.hist(all_cyn_list, bins=10, color='r')
            # mplpl.xlabel('Cynicality')
            # mplpl.ylabel('Frequency')
            # mplpl.title('Dem users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/dem_cynically'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            #
            # all_gull_list = [x/float(50) for x in list(grouped_sum_rep['gull'])]
            # all_cyn_list = [x/float(30) for x in list(grouped_sum_rep['cyn'])]
            #
            # mplpl.hist(all_gull_list, bins=10, color='b')
            # mplpl.xlabel('Gullibility')
            # mplpl.ylabel('Frequency')
            # mplpl.title('Rep users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/rep_gullibility'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            # mplpl.hist(all_cyn_list, bins=10, color='r')
            # mplpl.xlabel('Cynically')
            # mplpl.ylabel('Frequency')
            # mplpl.title('Rep users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/rep_cynicality'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            #
            #
            # all_gull_list = [x/float(50) for x in list(grouped_sum_neut['gull'])]
            # all_cyn_list = [x/float(30) for x in list(grouped_sum_neut['cyn'])]
            #
            # mplpl.hist(all_gull_list, bins=10, color='b')
            # mplpl.xlabel('Gullibility')
            # mplpl.ylabel('Frequency')
            # mplpl.title('Neut users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/neut_gullibility'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()
            #
            # mplpl.hist(all_cyn_list, bins=10, color='r')
            # mplpl.xlabel('Cynically')
            # mplpl.ylabel('Frequency')
            # mplpl.title('Neut users')
            # mplpl.xlim([0,1])
            # pp = remotedir + local_dir_saving + '/fig/neut_cynicality'
            # mplpl.savefig(pp, format='png')
            # mplpl.figure()



            print(np.mean(grouped_sum['gull'])/float(50))
            print(np.mean(grouped_sum['cyn'])/float(30))


            print(np.mean(grouped_sum_dem['gull'])/float(50))
            print(np.mean(grouped_sum_dem['cyn'])/float(30))


            print(np.mean(grouped_sum_rep['gull'])/float(50))
            print(np.mean(grouped_sum_rep['cyn'])/float(30))


            print(np.mean(grouped_sum_neut['gull'])/float(50))
            print(np.mean(grouped_sum_neut['cyn'])/float(30))
            #
            #
            # print(set(df_dem['worker_id']))
            # print(set(df_rep['worker_id']))
            # print(set(df_neut['worker_id']))
            # exit()

            tweet_rel_score = collections.defaultdict()
            tweet_intst_score = collections.defaultdict()
            tweet_rel_score_l = []
            tweet_intst_score_l = []
            tweet_non_rumor_rel_score_l = []
            tweet_non_rumor_intst_score_l = []
            tweet_rumor_rel_score_l = []
            tweet_rumor_intst_score_l = []
            tweet_pop = []
            tweet_pop_rumor = []
            tweet_pop_non_rumor = []


            users = 'all'
            # users = 'dem'
            # users = 'rep'
            # users = 'neut'



            df_rumor = df[df['lable']==0]
            df_non_rumor = df[df['lable']==1]

            df_dem_rumor = df_dem[df_dem['lable']==0]
            df_dem_non_rumor = df_dem[df_dem['lable']==1]
            df_rep_rumor = df_rep[df_rep['lable']==0]
            df_rep_non_rumor = df_rep[df_rep['lable']==1]
            df_neut_rumor = df_neut[df_neut['lable']==0]
            df_neut_non_rumor = df_neut[df_neut['lable']==1]


            if users == 'all':
                df_r_m = df_rumor.copy()
                df_nr_m = df_non_rumor.copy()
            elif users == 'dem':
                df_r_m = df_dem_rumor.copy()
                df_nr_m = df_dem_non_rumor.copy()
            elif users == 'rep':
                df_r_m = df_rep_rumor.copy()
                df_nr_m = df_rep_non_rumor.copy()
            elif users == 'neut':
                df_r_m = df_neut_rumor.copy()
                df_nr_m = df_neut_non_rumor.copy()

            outF_gull = open(remotedir + local_dir_saving  + 'gullibility_users_topics_exp2.txt', 'w')
            outF_cyn = open(remotedir + local_dir_saving  + 'cynical_users_topics_exp2.txt', 'w')
            # outF_gull = open(remotedir + local_dir_saving  + 'weighted_gullibility_users_topics_exp2.txt', 'w')
            # outF_cyn = open(remotedir + local_dir_saving  + 'weighted_cynical_users_topics_exp2.txt', 'w')

            for topic in topic_list:
                outF_gull.write('||' + topic + '||')
                outF_cyn.write('||' + topic + '||')

                for users in ['all', 'dem', 'rep', 'neut']:
                    if users == 'all':
                        df_r_m = df_rumor.copy()
                        df_nr_m = df_non_rumor.copy()
                    elif users == 'dem':
                        df_r_m = df_dem_rumor.copy()
                        df_nr_m = df_dem_non_rumor.copy()
                    elif users == 'rep':
                        df_r_m = df_rep_rumor.copy()
                        df_nr_m = df_rep_non_rumor.copy()
                    elif users == 'neut':
                        df_r_m = df_neut_rumor.copy()
                        df_nr_m = df_neut_non_rumor.copy()


                    groupby_ftr = topic
                    grouped = df_topics.groupby(groupby_ftr, sort=False)
                    # grouped_sum = df_topics.groupby(groupby_ftr, sort=False).sum()
                    tweet_id_list = df_topics['tweet_id'][grouped.groups[1]]
                    num_r_topic = len(set(df_rumor[df_rumor['tweet_id'].isin(tweet_id_list)]))
                    num_nr_topic = len(set(df_non_rumor[df_non_rumor['tweet_id'].isin(tweet_id_list)]))
                    df_r_tmp = df_r_m[df_r_m['tweet_id'].isin(tweet_id_list)]

                    df_nr_tmp = df_nr_m[df_nr_m['tweet_id'].isin(tweet_id_list)]

                    groupby_ftr = 'worker_id'
                    grouped_sum_r = df_r_tmp.groupby(groupby_ftr, sort=False).sum()
                    grouped_sum_nr = df_nr_tmp.groupby(groupby_ftr, sort=False).sum()

                    if num_r_topic>0:
                        outF_gull.write(str(np.round(np.mean(grouped_sum_r['gull'])/float(num_r_topic), 4)) + '||')
                    else:
                        outF_gull.write('?'+ '||')
                    if num_nr_topic>0:
                        outF_cyn.write(str(np.round(np.mean(grouped_sum_nr['cyn'])/float(num_nr_topic),4)) + '||')
                    else:
                        outF_cyn.write('?'+ '||')
                outF_gull.write('\n')
                outF_cyn.write('\n')
                # print("salam")



            # groupby_ftr = 'worker_id'
            # grouped = df_m.groupby(groupby_ftr, sort=False)
            # grouped_sum_w = df_m.groupby(groupby_ftr, sort=False).sum()

    if args.t == "html_parser":
        # remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/txt_file/'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/'
        local = 'true'
        local = 'mostly-ture'
        local = 'half-ture'
        local = 'false'
        local = 'mostly-false'
        local = 'pants-fire'
        diff_cat = ['true', 'mostly-true', 'half-true', 'false', 'mostly-false', 'pants-fire']

        # inp = open(remotedir + 'txt_file/'+local + '/true_page_1-2017-12-25 16:10:44.txt' , 'r')
        for cat in diff_cat:
            inp_all = glob.glob(remotedir + 'txt_file/' + cat + '/*')
            print(inp_all)
            source_dict = {}
            text_dict = {}
            date_dict = {}
            outF = open(remotedir + 'politifact/politifact_' + cat + '.txt' , 'w')
            c_ind_news = 0
            c_ind_source = 0
            c_ind_date = 0
            for inp in inp_all:
                with open(inp, 'r') as f:


                    inptxt = f.read()


                    doc = inptxt

                    html = Html()
                    dom = html.feed(doc)


                    for ind in dom.find('p', ('class', 'statement__source')):
                        c_ind_news+=1
                        source_dict[c_ind_news] = ind.text()

                    for ind in dom.find('p', ('class', 'statement__text')):
                        c_ind_source+=1
                        text_dict[c_ind_source] = ind.text()

                    for ind in dom.find('span', ('class', 'article__meta')):
                        c_ind_date+=1
                        date_dict[c_ind_date] = ind.text()


            for i in range(1,c_ind_news+1):
                news = text_dict[i].replace("\n","")
                news = news.replace('\r', '')
                news = news.replace('\t', '')
                source = source_dict[i]
                date = date_dict[i]
                outF.write(news + '<<||>>' + source + '<<||>>' + cat +'<<||>>'+  date + '\n')

    if args.t == "html_parser_snopes":
        # remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/txt_file/'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'

        # inp = open(remotedir + 'txt_file/'+local + '/true_page_1-2017-12-25 16:10:44.txt' , 'r')
        inp_all = glob.glob(remotedir + 'txt_file_200/*.txt')
        # inp_all = glob.glob(remotedir + 'txt_file/page_24-2017-12-29 13:06:32.txt')
        # inp_all = sorted(inp_all)
        # print(inp_all)
        # exit()
        # inp_all = [remotedir + 'txt_file/page_32-2017-12-29 13:08:06.txt', remotedir + 'txt_file/page_5-2017-12-29 13:02:40.txt']
        # inp_all = glob.glob(remotedir + 'txt_file/*.txt')
        print(inp_all)
        source_dict = {}
        text_dict = {}
        date_dict = {}
        outF = open(remotedir + 'all_snopes_news.txt' , 'w')
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        c_ind_cat = 0
        c_ind_l = 0
        category_dict = {}
        date_dict = {}
        author_dict = {}
        claim_dict = {}
        label_dict = {}
        file_num = 0
        for inp in inp_all:
            out_list = []
            print('--------------------------------------file_num' + str(file_num))
            file_num+=1
            with open(inp, 'r') as f:
                # print(f)
                inptxt = f.read()
                # parser.feed(inptxt)
                # page = requests.get('http://econpy.pythonanywhere.com/ex/001.html')
                # tree = html.fromstring(inptxt)
                # fact_check_label = tree.xpath('//span[@itemprop="alternateName"]/text()')

                parser = MyHTMLParser()
                m_html = Html()

                try:
                    dom1 = parser.feed(inptxt)
                    dom = m_html.feed(inptxt)
                except:
                    print(f)
                    continue

                # parser.close()
                # m_html.close()
                tmp_list = []
                ind_f = []
                for mem in out_list:
                    if 'article-link-category' not in mem:
                        tmp_list.append(mem)
                        continue
                    c_ind_news+=1
                    # print(c_ind_news)
                    date_f = False
                    claim_f = False
                    author_f = False
                    for m_el in tmp_list:
                        m_el = m_el.replace('<','')
                        m_el = m_el.replace('\n','')
                        m_el = m_el.replace('/>','')
                        m_el = m_el.replace('>','')
                        m_el = m_el.replace('\"','')
                        if 'datePublished' in m_el:
                            date_f = True
                            m_splt = m_el.split('=')
                            content = m_splt[2]
                            # content = content[9:]
                            # content = content.replace('\"','')
                            # date = content.replace('\'','')
                            date = content
                            date_dict[c_ind_news] = date
                        elif 'claimReviewed' in m_el:
                            claim_f = True
                            m_splt = m_el.split('=')
                            content = m_splt[2]
                            # content = content[9:]
                            # content = content.replace('\"','')
                            # claim = content.replace('\'','')
                            claim = content
                            claim_dict[c_ind_news] = claim
                        elif 'author' in m_el:
                            author_f = True
                            m_splt = m_el.split('=')
                            content = m_splt[2]
                            # content = content[10:]
                            # content = content.replace('\"','')
                            # author = content.replace('\'','')
                            author = content
                            author_dict[c_ind_news] = author

                    if claim_f == False or date_f==False or author_f==False:
                        ind_f.append(c_ind_news)
                        c_ind_news= c_ind_news-1



                    tmp_list = []
                ff=False
                for ind in dom.find('div', ('class', 'article-link-category')):
                    text_tmp = ""
                    txt = ind.text()
                    txt = txt.replace('\n\t\t\t\t', '')
                    # if "Fact Check" not in txt:
                    #     continue
                    txt_splt = txt.split(' ')
                    c_ind_cat += 1
                    if c_ind_cat in ind_f:
                        c_ind_cat = c_ind_cat - 1
                        ff=True
                        ind_f.remove(c_ind_cat+1)
                        continue

                    for mem in txt_splt[2:]:
                        text_tmp+=mem+' '
                    category_dict[c_ind_cat] = text_tmp
                # span[ @ itemprop = "alternateName"]
                for ind in dom.find('span', ('itemprop', 'alternateName')):
                    txt = ind.text()
                    # txt = txt.replace('\t\t\t', '')
                    c_ind_l += 1
                    label_dict[c_ind_l] = txt


                # for f_l in fact_check_label:
                #     c_ind_l += 1
                #     label_dict[c_ind_l] = f_l

                print('----------------c_ind_l   : ' + str(c_ind_l))
                print('----------------c_ind_news   : ' + str(c_ind_news))
                print('----------------c_ind_cat   : ' + str(c_ind_cat))

        for i in range(1,c_ind_news+1):
            news = claim_dict[i].replace("\n","")
            news = news.replace('\r', '')
            news = news.replace('\t', '')
            source = author_dict[i]
            date = date_dict[i]
            topic = category_dict[i]
            cat = label_dict[i]
            outF.write(news + '<<||>>' + source + '<<||>>' + cat +'<<||>>'+  date + '<<||>>'+ topic+ '\n')


    if args.t == "politifact_data_preprocess":
        # remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/txt_file/'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
        # local = 'true'
        # local = 'mostly-ture'
        # local = 'half-ture'
        # local = 'false'
        # local = 'mostly-false'
        # local = 'pants-fire'
        diff_cat = ['true', 'mostly-true', 'half-true', 'false', 'mostly-false', 'pants-fire']

        # inp = open(remotedir + 'txt_file/'+local + '/true_page_1-2017-12-25 16:10:44.txt' , 'r')
        inp_all = glob.glob(remotedir + '/*')
        print(inp_all)
        source_dict = {}
        text_dict = {}
        date_dict = {}
        outF = open(remotedir + 'politifact_last_100_news.txt', 'w')
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        for inp in inp_all:
            with open(inp, 'r') as f:
                for line in f:
                    line = line.replace('\n','')
                    ln_splt = line.split('<<||>>')
                    news = ln_splt[0]
                    source = ln_splt[1]
                    cat = ln_splt[2]
                    dat = ln_splt[3]
                    dt_splt = dat.split(',')
                    day_name = dt_splt[0].split(" ")[1]
                    m_month = time.strptime(dt_splt[1].split(" ")[1][:3],'%b').tm_mon
                    m_day = int(dt_splt[1].split(" ")[2][:-2])
                    m_year = int(dt_splt[2])
                    m_date = datetime.datetime(year=m_year,month=m_month,day=m_day)
                    news_txt[c_ind_news] = news
                    news_source[c_ind_news] = source
                    news_cat[c_ind_news] = cat
                    news_date[c_ind_news]=m_date
                    c_ind_news+=1


        news_date_sorted = sorted(news_date, key=news_date.get, reverse=True)
        print('|| Ground truth label       ||  Size     || First news story date || Last news story date ||  20th news story date ||')
        print('||All news stories together || ' + str(len(news_date_sorted)) + '||' + str(news_date[news_date_sorted[0]]) + '||'
              + str(news_date[news_date_sorted[len(news_date_sorted)-1]]) + '||' + str(news_date[news_date_sorted[20]]) + '||')


        th_date = datetime.datetime(year=2016, month=1, day=1)

        for df_cat in diff_cat:
            cat_count = 0
            for news_id in news_date_sorted:
                if news_date[news_id] > th_date:
                    if news_cat[news_id] == df_cat:
                        if cat_count == 0:
                            first_date_news_id = news_id
                        cat_count += 1
                        if cat_count == 20:
                            news_date_20 = news_id
                        last_date_news_id = news_id
            if cat_count < 20:
                news_date_20 = last_date_news_id
            print('||' + df_cat + '||' + str(cat_count) + '||' + str(news_date[first_date_news_id]) + '||'
                  + str(news_date[last_date_news_id]) + '||' + str(news_date[news_date_20]) + '||')


        print("#####################")
        print("#####################")

        print('In last 100 news stories')

        for df_cat in diff_cat:
            cat_count = 0
            for news_id in news_date_sorted[:100]:
                if news_cat[news_id] == df_cat:
                    if cat_count == 0:
                        first_date_news_id = news_id
                    if cat_count == 20:
                        news_date_20 = news_id
                    cat_count += 1
                    last_date_news_id = news_id
            if cat_count < 20:
                news_date_20 = last_date_news_id
            print('||' + df_cat + '||' + str(cat_count) + '||' + str(news_date[first_date_news_id]) + '||'
                  + str(news_date[last_date_news_id]) + '||' + str(news_date[news_date_20]) + '||')

        print("#####################")
        print("#####################")
        print('In last 200 news stories')

        for df_cat in diff_cat:
            cat_count = 0
            for news_id in news_date_sorted[:200]:
                if news_cat[news_id] == df_cat:
                    if cat_count == 0:
                        first_date_news_id = news_id
                    if cat_count == 20:
                        news_date_20 = news_id
                    cat_count += 1
                    last_date_news_id = news_id
            if cat_count < 20:
                news_date_20 = last_date_news_id
            print('||' + df_cat + '||' + str(cat_count) + '||' + str(news_date[first_date_news_id]) + '||'
                  + str(news_date[last_date_news_id]) + '||' + str(news_date[news_date_20]) + '||')

            # print(' * There is ' + str(cat_count) + ' news stories in category of ' + df_cat)



            # created_time = datetime.datetime.strptime(tweet_f['created_at'], "%a %b %d %H:%M:%S +0000 %Y")
            # t_created_t = time.mktime(created_time.timetuple())
    if args.t == "snopes_data_preprocess":
        # remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/txt_file/'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
        # local = 'true'
        # local = 'mostly-ture'
        # local = 'half-ture'
        # local = 'false'
        # local = 'mostly-false'
        # local = 'pants-fire'
        # diff_cat = ['true', 'mostly-true', 'half-true', 'false', 'mostly-false', 'pants-fire']

        # inp = open(remotedir + 'txt_file/'+local + '/true_page_1-2017-12-25 16:10:44.txt' , 'r')
        inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
        print(inp_all)
        source_dict = {}
        text_dict = {}
        date_dict = {}
        outF = open(remotedir + 'politifact_last_100_news.txt', 'w')
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        for inp in inp_all:
            with open(inp, 'r') as f:
                for line in f:
                    line = line.replace('\n', '')
                    ln_splt = line.split('<<||>>')
                    news = ln_splt[0]
                    source = ln_splt[1]
                    cat = ln_splt[2]
                    dat = ln_splt[3]
                    topic = ln_splt[4]
                    dt_splt = dat.split('T')
                    dt_splt = dt_splt[0].split('-')
                    m_month = int(dt_splt[1])
                    m_day = int(dt_splt[2])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    news_txt[c_ind_news] = news
                    news_source[c_ind_news] = source
                    news_cat[c_ind_news] = cat
                    news_date[c_ind_news] = m_date
                    news_topic[c_ind_news] = topic
                    c_ind_news += 1

        news_date_sorted = sorted(news_date, key=news_date.get, reverse=True)
        print(
        '|| Ground truth label       ||  Size     || First news story date || Last news story date ||  20th news story date ||')
        print('||All news stories together || ' + str(len(news_date_sorted)) + '||' + str(
            news_date[news_date_sorted[0]]) + '||'
              + str(news_date[news_date_sorted[len(news_date_sorted) - 1]]) + '||' + str(
            news_date[news_date_sorted[20]]) + '||')

        # print('Last news story date : ' + str(news_date[0]))
        # print('First news story date : ' + str(news_date[len(news_date_sorted)-1]))
        news_cat_list = list(set(news_cat.values()))[1:]

        news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE' ]

        th_date =  datetime.datetime(year=2016, month=1, day=1)


        for df_cat in news_cat_list:
            cat_count = 0
            for news_id in news_date_sorted:
                if news_date[news_id]>th_date:
                    if news_cat[news_id] == df_cat:
                        if cat_count == 0:
                            first_date_news_id = news_id
                        cat_count += 1
                        if cat_count==20:
                            news_date_20 = news_id
                        last_date_news_id = news_id
            if cat_count<20:
                news_date_20 = last_date_news_id
            print('||' + df_cat + '||' + str(cat_count) + '||' + str(news_date[first_date_news_id]) + '||'
                  + str(news_date[last_date_news_id]) + '||' + str(news_date[news_date_20]) + '||')
            # print('There is ' + str(cat_count) + ' news stories in category of ' + df_cat)
            # print('    *  Last news story date : ' + str(news_date[first_date_news_id]))
            # print('    *  20th last news story date : ' + str(news_date[20]))
            # print('    *  First news story date : ' + str(news_date[last_date_news_id]))

        print("#####################")
        print("#####################")

        print('In last 100 news stories')


        for df_cat in news_cat_list:
            cat_count = 0
            for news_id in news_date_sorted[:100]:
                if news_cat[news_id] == df_cat:
                    if cat_count == 0:
                        first_date_news_id = news_id
                    if cat_count==20:
                        news_date_20 = news_id
                    cat_count += 1
                    last_date_news_id = news_id
            if cat_count < 20:
                news_date_20 = last_date_news_id
            print('||' + df_cat + '||' + str(cat_count) + '||' + str(news_date[first_date_news_id]) + '||'
                  + str(news_date[last_date_news_id]) + '||' + str(news_date[news_date_20]) + '||')

        print("#####################")
        print("#####################")
        print('In last 200 news stories')

        for df_cat in news_cat_list:
            cat_count = 0
            for news_id in news_date_sorted[:200]:
                if news_cat[news_id] == df_cat:
                    if cat_count == 0:
                        first_date_news_id = news_id
                    if cat_count==20:
                        news_date_20 = news_id
                    cat_count += 1
                    last_date_news_id = news_id
            if cat_count<20:
                news_date_20 = last_date_news_id
            print('||' + df_cat + '||' + str(cat_count) + '||' + str(news_date[first_date_news_id]) + '||'
                  + str(news_date[last_date_news_id]) + '||' + str(news_date[news_date_20]) + '||')

                # print(' * There is ' + str(cat_count) + ' news stories in category of ' + df_cat)



            # created_time = datetime.datetime.strptime(tweet_f['created_at'], "%a %b %d %H:%M:%S +0000 %Y")
            # t_created_t = time.mktime(created_time.timetuple())

    if args.t == "snopes_data_preprocess_topics":
        # remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/txt_file/'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
        # local = 'true'
        # local = 'mostly-ture'
        # local = 'half-ture'
        # local = 'false'
        # local = 'mostly-false'
        # local = 'pants-fire'
        # diff_cat = ['true', 'mostly-true', 'half-true', 'false', 'mostly-false', 'pants-fire']

        # inp = open(remotedir + 'txt_file/'+local + '/true_page_1-2017-12-25 16:10:44.txt' , 'r')
        inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
        print(inp_all)
        source_dict = {}
        text_dict = {}
        date_dict = {}
        # outF = open(remotedir + 'politifact_last_100_news.txt', 'w')
        outF = open(remotedir + 'snopes_latest_25_news_per_lable.txt', 'w')
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)
        for inp in inp_all:
            with open(inp, 'r') as f:
                for line in f:
                    line = line.replace('\n', '')
                    ln_splt = line.split('<<||>>')
                    news = ln_splt[0]
                    source = ln_splt[1]
                    cat = ln_splt[2]
                    dat = ln_splt[3]
                    topic = ln_splt[4]
                    dt_splt = dat.split('T')
                    dt_splt = dt_splt[0].split('-')
                    m_month = int(dt_splt[1])
                    m_day = int(dt_splt[2])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    news_txt[c_ind_news] = news
                    news_source[c_ind_news] = source
                    news_cat[c_ind_news] = cat
                    news_date[c_ind_news] = m_date
                    news_topic[c_ind_news] = topic.replace('\t\t\t','')
                    topic_count_dict[news_topic[c_ind_news]]+=1
                    c_ind_news += 1

        news_date_sorted = sorted(news_date, key=news_date.get, reverse=True)
        # print(
        #     '|| Ground truth label       ||  Size     || First news story date || Last news story date ||  20th news story date ||')
        # print('||All news stories together || ' + str(len(news_date_sorted)) + '||' + str(
        #     news_date[news_date_sorted[0]]) + '||'
        #       + str(news_date[news_date_sorted[len(news_date_sorted) - 1]]) + '||' + str(
        #     news_date[news_date_sorted[20]]) + '||')

        th_date = datetime.datetime(year=2000, month=1, day=1)

        news_topic_sorted = sorted(topic_count_dict, key=topic_count_dict.get, reverse=True)
        news_topic_update = {}
        for topic in news_topic_sorted:

            if topic=='' or topic==' ':
                continue
            cat_count = 0
            for news_id in news_date_sorted[:100]:
                if news_date[news_id] > th_date:
                    if news_topic[news_id] == topic:
                        cat_count += 1

            news_topic_update[topic] = cat_count

        first_date_news_id = 0
        last_date_news_id = 0
        count_c = 0
        news_topic_sorted = sorted(news_topic_update, key=news_topic_update.get, reverse=True)
        for topic in news_topic_sorted:
            if topic=='' or topic==' ':
                continue
            cat_count = 0
            for news_id in news_date_sorted[:100]:
                if news_date[news_id] > th_date:
                    if news_topic[news_id] == topic:
                        if cat_count == 0:
                            first_date_news_id = news_id
                        cat_count += 1
                        if cat_count == 20:
                            news_date_20 = news_id
                        last_date_news_id = news_id
            # if cat_count < 20:
            #     news_date_20 = last_date_news_id
            count_c+=1
            print('||' + str(count_c)+ '||' + str(topic) + '||' + str(cat_count) + '||' + str(news_date[first_date_news_id]) + '||' + str(news_date[last_date_news_id]) + '||')


        # exit()
        news_cat_list = list(set(news_cat.values()))[1:]

        news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']


        output = open(remotedir + 'snopes_topics_cat.txt' , 'w')
        output.write('|| ||Topic || All  || MOSTLY FALSE || FALSE || MIXTURE || TRUE || MOSTLY TRUE||\n')

        line_count = 0
        tmp_dict = {}
        for topic in news_topic_sorted:
            if topic=='' or topic==' ':
                continue
            cat_count = 0
            line_count+=1
            output.write('||' + str(line_count) + '||' + topic + '||')
            sum_all_cat = 0
            for df_cat in news_cat_list:
                cat_count = 0
                for news_id in news_date_sorted[:100]:
                    if news_date[news_id] > th_date:
                        if news_cat[news_id] == df_cat and news_topic[news_id]==topic:
                            cat_count += 1
                tmp_dict[df_cat] = cat_count
                sum_all_cat += cat_count
            output.write(str(sum_all_cat) + '||')
            for categ in news_cat_list:
                output.write(  str(tmp_dict[categ]) + '||')
            output.write('\n')

    if args.t == "snopes_data_preprocess_topics_AMT":
        # remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/txt_file/'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
        # local = 'true'
        # local = 'mostly-ture'
        # local = 'half-ture'
        # local = 'false'
        # local = 'mostly-false'
        # local = 'pants-fire'
        # diff_cat = ['true', 'mostly-true', 'half-true', 'false', 'mostly-false', 'pants-fire']

        # inp = open(remotedir + 'txt_file/'+local + '/true_page_1-2017-12-25 16:10:44.txt' , 'r')
        inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
        print(inp_all)
        source_dict = {}
        text_dict = {}
        date_dict = {}
        # outF = open(remotedir + 'politifact_last_100_news.txt', 'w')
        outF = open(remotedir + 'snopes_latest_25_news_per_lable.txt', 'w')
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)
        for inp in inp_all:
            with open(inp, 'r') as f:
                for line in f:
                    line = line.replace('\n', '')
                    ln_splt = line.split('<<||>>')
                    news = ln_splt[0]
                    news = news.replace('&quot;','\'')
                    news = news.replace('\"','\'')
                    source = ln_splt[1]
                    cat = ln_splt[2]
                    dat = ln_splt[3]
                    topic = ln_splt[4]
                    dt_splt = dat.split('T')
                    dt_splt = dt_splt[0].split('-')
                    m_month = int(dt_splt[1])
                    m_day = int(dt_splt[2])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    news_txt[c_ind_news] = news
                    news_source[c_ind_news] = source
                    news_cat[c_ind_news] = cat
                    news_date[c_ind_news] = m_date
                    news_topic[c_ind_news] = topic.replace('\t\t\t', '')
                    topic_count_dict[news_topic[c_ind_news]] += 1
                    c_ind_news += 1

        news_date_sorted = sorted(news_date, key=news_date.get, reverse=True)
        # print(
        #     '|| Ground truth label       ||  Size     || First news story date || Last news story date ||  20th news story date ||')
        # print('||All news stories together || ' + str(len(news_date_sorted)) + '||' + str(
        #     news_date[news_date_sorted[0]]) + '||'
        #       + str(news_date[news_date_sorted[len(news_date_sorted) - 1]]) + '||' + str(
        #     news_date[news_date_sorted[20]]) + '||')

        th_date = datetime.datetime(year=2000, month=1, day=1)

        news_topic_sorted = sorted(topic_count_dict, key=topic_count_dict.get, reverse=True)
        news_topic_update = {}
        for topic in news_topic_sorted:

            if topic == '' or topic == ' ':
                continue
            cat_count = 0
            for news_id in news_date_sorted:
                if news_date[news_id] > th_date:
                    if news_topic[news_id] == topic:
                        cat_count += 1

            news_topic_update[topic] = cat_count

        first_date_news_id = 0
        last_date_news_id = 0
        count_c = 0
        news_topic_sorted = sorted(news_topic_update, key=news_topic_update.get, reverse=True)


        news_cat_list = list(set(news_cat.values()))[1:]

        news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
        news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

        output = open(remotedir + 'snopes_topics_cat.txt', 'w')
        # output.write('|| ||Topic || All  || MOSTLY FALSE || FALSE || MIXTURE || TRUE || MOSTLY TRUE||\n')

        ini_news_id = 1000
        line_count = 0
        tmp_dict = {}
        for i in range(0,5):
            df_cat = news_cat_list[i]
            df_cat_f = news_cat_list_f[i]
            outF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'w')
            output.write('== ' + df_cat + ' ==\n')
            # output.write('|| || calim_id|| claim || author || category|| date ||\n')
            # output.write('|| || calim_id|| claim || author || date ||\n')
            output.write('|| || claim || Ground-truth lable || date ||\n')
            news_list = []
            cat_count = 0
            sum_all_cat = 0
            for topic in [' Politics ']:#, ' Political News ']:
                for news_id in news_date_sorted:
                    if news_date[news_id] > th_date:
                        if news_cat[news_id] == df_cat and news_topic[news_id] == topic:# and cat_count<25:
                            news = news_txt[news_id]
                            if news in news_list:
                                continue
                            news_list.append(news)
                            source = news_source[news_id]
                            cat = news_cat[news_id]
                            m_date = news_date[news_id]
                            m_topic = news_topic[news_id]

                            line_count += 1
                            cat_count += 1

                            # output.write('|| ' +str(cat_count) + ' || ' + str(ini_news_id+line_count) + ' ||'
                            #              + news + ' || ' +  cat + ' || ' + str(m_date)+ ' ||\n')
                            output.write('|| ' +str(cat_count) + ' ||'
                                         + news + ' || ' +  cat + ' || ' + str(m_date)+ ' ||\n')
                            outF.write('<<||>>' +str(cat_count) + '<<||>>' + str(ini_news_id+line_count) + '<<||>>'
                                         + news + '<<||>>' +  cat + '<<||>>' + str(m_date)+ '<<||>>\n')

                            # output.write('|| ' + str(cat_count) + ' || ' + str(ini_news_id + line_count) + ' ||'
                            #              + news + ' || ' + source + ' || ' + cat + ' || ' + str(m_date) + ' ||\n')

    if args.t == "snopes_data_preprocess_topics_AMT_nonpolitic":
        # remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/txt_file/'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
        # local = 'true'
        # local = 'mostly-ture'
        # local = 'half-ture'
        # local = 'false'
        # local = 'mostly-false'
        # local = 'pants-fire'
        # diff_cat = ['true', 'mostly-true', 'half-true', 'false', 'mostly-false', 'pants-fire']

        # inp = open(remotedir + 'txt_file/'+local + '/true_page_1-2017-12-25 16:10:44.txt' , 'r')
        inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
        print(inp_all)
        source_dict = {}
        text_dict = {}
        date_dict = {}
        # outF = open(remotedir + 'politifact_last_100_news.txt', 'w')
        outF = open(remotedir + 'snopes_latest_20_news_per_lable_non_politics_1.txt', 'w')
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)
        for inp in inp_all:
            with open(inp, 'r') as f:
                for line in f:
                    line = line.replace('\n', '')
                    ln_splt = line.split('<<||>>')
                    news = ln_splt[0]
                    news = news.replace('&quot;','\'')
                    news = news.replace('\"','\'')
                    source = ln_splt[1]
                    cat = ln_splt[2]
                    dat = ln_splt[3]
                    topic = ln_splt[4]
                    dt_splt = dat.split('T')
                    dt_splt = dt_splt[0].split('-')
                    m_month = int(dt_splt[1])
                    m_day = int(dt_splt[2])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    news_txt[c_ind_news] = news
                    news_source[c_ind_news] = source
                    news_cat[c_ind_news] = cat
                    news_date[c_ind_news] = m_date
                    news_topic[c_ind_news] = topic.replace('\t\t\t', '')
                    topic_count_dict[news_topic[c_ind_news]] += 1
                    c_ind_news += 1

        news_date_sorted = sorted(news_date, key=news_date.get, reverse=True)
        # print(
        #     '|| Ground truth label       ||  Size     || First news story date || Last news story date ||  20th news story date ||')
        # print('||All news stories together || ' + str(len(news_date_sorted)) + '||' + str(
        #     news_date[news_date_sorted[0]]) + '||'
        #       + str(news_date[news_date_sorted[len(news_date_sorted) - 1]]) + '||' + str(
        #     news_date[news_date_sorted[20]]) + '||')

        th_date = datetime.datetime(year=2000, month=1, day=1)

        news_topic_sorted = sorted(topic_count_dict, key=topic_count_dict.get, reverse=True)
        news_topic_update = {}
        for topic in news_topic_sorted:

            if topic == '' or topic == ' ':
                continue
            cat_count = 0
            for news_id in news_date_sorted:
                if news_date[news_id] > th_date:
                    if news_topic[news_id] == topic:
                        cat_count += 1

            news_topic_update[topic] = cat_count

        first_date_news_id = 0
        last_date_news_id = 0
        count_c = 0
        news_topic_sorted = sorted(news_topic_update, key=news_topic_update.get, reverse=True)


        news_cat_list = list(set(news_cat.values()))[1:]

        news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
        news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

        output = open(remotedir + 'snopes_topics_cat_1.txt', 'w')
        # output.write('|| ||Topic || All  || MOSTLY FALSE || FALSE || MIXTURE || TRUE || MOSTLY TRUE||\n')

        ini_news_id = 1000
        line_count = 0
        tmp_dict = {}
        th_date = datetime.datetime(year=2017, month=1, day=1)
        for i in range(0,5):
            df_cat = news_cat_list[i]
            df_cat_f = news_cat_list_f[i]
            outF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims_1.txt', 'w')
            output.write('== ' + df_cat + ' ==\n')
            # output.write('|| || calim_id|| claim || author || category|| date ||\n')
            # output.write('|| || calim_id|| claim || author || date ||\n')
            output.write('|| || claim || Ground-truth lable || date ||\n')
            news_list = []
            cat_count = 0
            sum_all_cat = 0
            topic_count=collections.defaultdict(int)
            topic_list = list(set(news_topic.values()))
            for topic in topic_list:
                #, ' Political News ']:
                if topic in [' Politics ', ' Political News ', ' Fake News ', '']:
                    continue
                for news_id in news_date_sorted:
                    if news_date[news_id] > th_date:
                        if news_cat[news_id] == df_cat and news_topic[news_id] == topic:# and cat_count<25:
                            news = news_txt[news_id]
                            if news in news_list:
                                continue
                            news_list.append(news)
                            source = news_source[news_id]
                            cat = news_cat[news_id]
                            m_date = news_date[news_id]
                            m_topic = news_topic[news_id]
                            topic_count[topic]+=1
                            if topic_count[topic]>5:
                                continue
                            line_count += 1
                            cat_count += 1

                            # output.write('|| ' +str(cat_count) + ' || ' + str(ini_news_id+line_count) + ' ||'
                            #              + news + ' || ' +  cat + ' || ' + str(m_date)+ ' ||\n')
                            output.write('|| ' +str(cat_count) + ' ||'
                                         + news + ' || ' +  cat + ' || ' + str(m_date)+ ' ||\n')
                            outF.write('<<||>>' +str(cat_count) + '<<||>>' + str(ini_news_id+line_count) + '<<||>>'
                                         + news + '<<||>>' +  cat + '<<||>>' +topic + '<<||>>' + str(m_date)+ '<<||>>\n')

                            # output.write('|| ' + str(cat_count) + ' || ' + str(ini_news_id + line_count) + ' ||'
                            #              + news + ' || ' + source + ' || ' + cat + ' || ' + str(m_date) + ' ||\n')

    if args.t == "politifact_data_preprocess_topics_AMT":
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
        # local = 'true'
        # local = 'mostly-ture'
        # local = 'half-ture'
        # local = 'false'
        # local = 'mostly-false'
        # local = 'pants-fire'
        diff_cat = ['true', 'mostly-true', 'half-true', 'false', 'mostly-false', 'pants-fire']

        # inp = open(remotedir + 'txt_file/'+local + '/true_page_1-2017-12-25 16:10:44.txt' , 'r')
        inp_all = glob.glob(remotedir + '/*.txt')
        print(inp_all)
        source_dict = {}
        text_dict = {}
        date_dict = {}
        outF = open(remotedir + 'politifact_latest_30_news_per_lable.txt', 'w')
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        for inp in inp_all:
            with open(inp, 'r') as f:
                print(f)
                for line in f:
                    try:
                        line = line.replace('\n','')
                        ln_splt = line.split('<<||>>')
                        news = ln_splt[0]
                        news = news.replace('&quot','')
                        news = news.replace('&#39', '\'')
                        news = news.replace('&nbsp', '')
                        news = news.replace('&rsquo', '\'')
                        news = news.replace('&lsquo', '\'')
                        news = news.replace('\"', '\'')
                        source = ln_splt[1]
                        cat = ln_splt[2]
                        dat = ln_splt[3]
                        dt_splt = dat.split(',')
                        day_name = dt_splt[0].split(" ")[1]
                        m_month = time.strptime(dt_splt[1].split(" ")[1][:3],'%b').tm_mon
                        m_day = int(dt_splt[1].split(" ")[2][:-2])
                        m_year = int(dt_splt[2])
                        m_date = datetime.datetime(year=m_year,month=m_month,day=m_day)
                        news_txt[c_ind_news] = news
                        news_source[c_ind_news] = source
                        news_cat[c_ind_news] = cat
                        news_date[c_ind_news]=m_date
                        c_ind_news+=1
                    except:
                        continue

        news_date_sorted = sorted(news_date, key=news_date.get, reverse=True)

        th_date = datetime.datetime(year=2000, month=1, day=1)


        first_date_news_id = 0
        last_date_news_id = 0
        count_c = 0

        news_cat_list = list(set(news_cat.values()))[1:]


        output = open(remotedir + 'politifact_topics_cat', 'w')
        # output.write('|| ||Topic || All  || MOSTLY FALSE || FALSE || MIXTURE || TRUE || MOSTLY TRUE||\n')

        ini_news_id = 2000
        line_count = 0
        tmp_dict = {}
        for i in range(0, 6):
            df_cat = diff_cat[i]
            df_cat_f = diff_cat[i]
            outF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'w')
            output.write('== ' + df_cat + ' ==\n')
            output.write('|| || calim_id|| claim || author || category|| date ||\n')
            # output.write('|| || calim_id|| claim || author || date ||\n')
            cat_count = 0
            # output.write('||' + str(line_count) + '||' + topic + '||')
            sum_all_cat = 0
            news_list = []
            for news_id in news_date_sorted:
                if news_date[news_id] > th_date:
                    if news_cat[news_id] == df_cat and cat_count<40:
                        news_t = news_txt[news_id]
                        news=""
                        news_tmp=""
                        first_c_f = False
                        for cc in news_t:
                            if first_c_f==False:
                                if cc==' ':
                                    continue
                                first_c_f = True
                            news_tmp+=cc


                        for cc in range(0,len(news_tmp)):
                            if news_tmp[len(news_tmp)-1]==' ':
                                news_tmp=news_tmp[:-1]
                            else:
                                break

                        news = news_tmp
                        if news[:4]=='Says':
                            news = news[5:]
                        if news in news_list:
                            continue
                        line_count += 1
                        cat_count += 1
                        news_list.append(news)

                        source = news_source[news_id]
                        cat = news_cat[news_id]
                        m_date = news_date[news_id]

                        # output.write(
                        #     '|| ' + str(cat_count) + ' || ' + str(ini_news_id + line_count) + ' ||'
                        #     + news + ' || ' + cat + ' || ' + str(m_date) + ' ||\n')

                        outF.write('<<||>>' + str(cat_count) + '<<||>>' + str(
                            ini_news_id + line_count) + '<<||>>'
                                   + news + '<<||>>' + source + '<<||>>' + cat + '<<||>>' + str(m_date) + '<<||>>\n')

                        output.write('|| ' + str(cat_count) + ' || ' + str(ini_news_id + line_count) + ' ||'
                                     + news + ' || ' + source + ' || ' + cat + ' || ' + str(m_date) + ' ||\n')


    if args.t == "snopes_data_preprocess_topics_AMT_preparing":
        # remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/txt_file/'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
        inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
        print(inp_all)
        source_dict = {}
        text_dict = {}
        date_dict = {}
        # outF = open(remotedir + 'politifact_last_100_news.txt', 'w')
        final_output_exp1 = open(remotedir + 'snopes_latest_30_news_per_lable_AMT.txt', 'w')
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)

        news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
        news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

        line_count = 0
        tmp_dict = {}
        claims_list = collections.defaultdict(list)
        for i in range(0, 5):
            df_cat = news_cat_list[i]
            df_cat_f = news_cat_list_f[i]
            # inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
            inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
            cat_count = 0
            for line in inF:
                claims_list[i].append(line)
                cat_count += 1
                # if cat_count > 30:
                if cat_count > 20:
                        break
            random.shuffle(claims_list[i])

        news_list = {}
        for i in [0]:
            news_list[i] = []
            for j in range(0,5):
                news_list[i]+=claims_list[j][i*20:(i+1) *20]
            random.shuffle(news_list[i])


        # for i in [0,1,2]:
        #     news_list[i] = []
        #     for j in range(0,5):
        #         news_list[i]+=claims_list[j][i*10:(i+1) *10]
        #     random.shuffle(news_list[i])

        # final_output_exp1 = open(remotedir + 'snopes_latest_30_news_per_lable_AMT_1.txt', 'w')
        # final_output_exp2 = open(remotedir + 'snopes_latest_30_news_per_lable_AMT_2.txt', 'w')
        # final_output_exp3 = open(remotedir + 'snopes_latest_30_news_per_lable_AMT_3.txt', 'w')


        random.shuffle(claims_list)
        sample_tweets_exp1 = []
        # for i in [0,1,2]:
        for i in [0]:
            sample_tweets_exp1 = []
            final_output_exp1 = open(remotedir + 'snopes_nonpolitics_latest_20_news_per_lable_AMT_'+str(i+1)+'.txt', 'w')
            for line in news_list[i]:
                line_splt = line.split('<<||>>')
                claim_id = int(line_splt[2])
                claim_txt = line_splt[3]
                tweet_id = claim_id
                publisher_name = claim_id
                tweet_txt = claim_txt
                tweet_link = claim_txt
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]

                sample_tweets_exp1.append(tmp_list)

            random.shuffle(sample_tweets_exp1)
            print(len(sample_tweets_exp1))
            json.dump(sample_tweets_exp1, final_output_exp1)



    if args.t == "politifact_data_preprocess_topics_AMT_preparing":
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
        final_output_exp1 = open(remotedir + 'snopes_latest_25_news_per_lable_AMT.txt', 'w')

        diff_cat = ['true', 'mostly-true', 'half-true', 'false', 'mostly-false', 'pants-fire']

        line_count = 0
        tmp_dict = {}
        claims_list = collections.defaultdict(list)
        for i in range(0, 6):

            df_cat = diff_cat[i]
            df_cat_f = diff_cat[i]
            inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
            cat_count = 0
            for line in inF:
                claims_list[i].append(line)
                cat_count+=1
                if cat_count>30:
                    break

            random.shuffle(claims_list[i])

        news_list = {}
        for i in [0,1,2]:
            news_list[i] = []
            for j in range(0,6):
                news_list[i]+=claims_list[j][i*10:(i+1) *10]
            random.shuffle(news_list[i])


        random.shuffle(claims_list)
        sample_tweets_exp1 = []
        for i in [0,1,2]:
            sample_tweets_exp1 = []
            final_output_exp1 = open(remotedir + 'politifact_latest_30_news_per_lable_AMT_'+str(i+1)+'.txt', 'w')
            for line in news_list[i]:
                line_splt = line.split('<<||>>')
                claim_id = int(line_splt[2])
                claim_txt = line_splt[3]
                claim_txt = claim_txt.replace('&lsquo', '')
                claim_txt = claim_txt.replace('&rsquo', '\'')
                claim_txt = claim_txt.replace('&nbsp', '')

                author_txt = line_splt[4]
                author_txt = author_txt.replace('&lsquo', '')
                author_txt = author_txt.replace('&rsquo', '\'')
                author_txt = author_txt.replace('&nbsp', '')

                tweet_id = claim_id
                publisher_name = claim_id
                tweet_txt = claim_txt
                tweet_link = claim_txt
                tmp_list = [tweet_id, publisher_name, tweet_txt, author_txt]

                sample_tweets_exp1.append(tmp_list)

            random.shuffle(sample_tweets_exp1)
            print(len(sample_tweets_exp1))
            json.dump(sample_tweets_exp1, final_output_exp1)

    if args.t == "word_counting":
        nrows = 1e5



        feat = 'tweet_text'
        normalization_clustering = 'minmax'


        max_f = 100



        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        dataset = 'snopes'
        # dataset = 'mia'
        # dataset = 'politifact'

        publisher_leaning = 1
        source_dict = {}
        text_dict = {}
        date_dict = {}
        c_ind_news = 0
        c_ind_source = 0
        c_ind_date = 0
        news_txt = {}
        news_source = {}
        news_cat = {}
        news_date = {}
        news_topic = {}
        topic_count_dict = collections.defaultdict(int)


        line_count = 0
        tmp_dict = {}
        claims_list = []

        if dataset=='mia':
            local_dir_saving = ''
            remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'


            final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                     + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

            sample_tweets_exp1 = json.load(final_inp_exp1)

            input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
            input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets','r')



            exp1_list = sample_tweets_exp1
            tweet_id = 100010
            publisher_name = 110
            tweet_popularity = {}
            tweet_text_dic = {}
            for input_file in [input_rumor, input_non_rumor]:
                for line in input_file:
                    line.replace('\n', '')
                    line_splt = line.split('\t')
                    tweet_txt = line_splt[1]
                    tweet_link = line_splt[1]
                    tweet_id += 1
                    publisher_name += 1
                    tweet_popularity[tweet_id] = int(line_splt[2])
                    tweet_text_dic[tweet_id] = tweet_txt

            out_list = []
            cnn_list = []
            foxnews_list = []
            ap_list = []
            tweet_txt_dict = {}
            tweet_link_dict = {}
            tweet_publisher_dict = {}
            tweet_rumor= {}
            tweet_lable_dict = {}
            tweet_non_rumor = {}
            pub_dict = collections.defaultdict(list)
            for tweet in exp1_list:

                tweet_id = tweet[0]
                publisher_name = tweet[1]
                tweet_txt = tweet[2]
                tweet_link = tweet[3]
                tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_link_dict[tweet_id] = tweet_link
                tweet_publisher_dict[tweet_id] = publisher_name
                if int(tweet_id)<100060:
                    tweet_lable_dict[tweet_id]='rumor'
                else:
                    tweet_lable_dict[tweet_id]='non-rumor'


        if dataset == 'snopes':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
            inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
            news_cat_list = ['MOSTLY FALSE', 'FALSE', 'MIXTURE', 'TRUE', 'MOSTLY TRUE']
            news_cat_list_f = ['mostly_false', 'false', 'mixture', 'true', 'mostly_true']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 5):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            for line in claims_list:
                line_splt = line.split('<<||>>')
                publisher_name = int(line_splt[2])
                tweet_txt = line_splt[3]
                tweet_id = publisher_name
                cat_lable = line_splt[4]
                dat = line_splt[5]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable


        if dataset == 'politifact':
            remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
            inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
            news_cat_list = ['mostly_false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']
            news_cat_list_f = ['mostly-false', 'false', 'half-true', 'true', 'mostly-true', 'pants-fire']

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            print(inp_all)

            for i in range(0, 6):
                df_cat = news_cat_list[i]
                df_cat_f = news_cat_list_f[i]
                inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                cat_count = 0
                for line in inF:
                    claims_list.append(line)
                    cat_count += 1

            sample_tweets_exp1 = []

            tweet_txt_dict = {}
            tweet_date_dict = {}
            tweet_lable_dict = {}
            tweet_publisher_dict = {}
            for line in claims_list:
                line_splt = line.split('<<||>>')
                tweet_id = int(line_splt[2])
                tweet_txt = line_splt[3]
                publisher_name = line_splt[4]
                cat_lable = line_splt[5]
                dat = line_splt[6]
                dt_splt = dat.split(' ')[0].split('-')
                m_day = int(dt_splt[2])
                m_month = int(dt_splt[1])
                m_year = int(dt_splt[0])
                m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                tweet_txt_dict[tweet_id] = tweet_txt
                tweet_date_dict[tweet_id] = m_date
                tweet_lable_dict[tweet_id] = cat_lable
                tweet_publisher_dict[tweet_id] = publisher_name



        # run = 'plot'
        run = 'analysis'
        # run = 'second-analysis'
        exp1_list = sample_tweets_exp1
        df = collections.defaultdict()
        df_w = collections.defaultdict()
        tweet_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg_med_var = collections.defaultdict(list)
        tweet_dev_avg = {}
        tweet_dev_med = {}
        tweet_dev_var = {}
        tweet_avg = {}
        tweet_med = {}
        tweet_var = {}
        tweet_gt_var = {}

        tweet_dev_avg_l = []
        tweet_dev_med_l = []
        tweet_dev_var_l = []
        tweet_avg_l = []
        tweet_med_l = []
        tweet_var_l = []
        tweet_gt_var_l = []
        avg_susc = 0
        avg_gull = 0
        avg_cyn = 0

        tweet_abs_dev_avg = {}
        tweet_abs_dev_med = {}
        tweet_abs_dev_var = {}

        tweet_abs_dev_avg_l = []
        tweet_abs_dev_med_l= []
        tweet_abs_dev_var_l = []

        # for ind in [1,2,3]:
        all_acc = []

        ##########################prepare balanced data (same number of rep, dem, neut #############

        #
        # if dataset=='snopes':
        #     data_n = 'sp'
        #     ind_l = [1,2,3]
        # elif dataset=='politifact':
        #     data_n = 'pf'
        #     ind_l = [1,2,3]
        # elif dataset=='mia':
        #     data_n = 'mia'
        #     ind_l = [1]
        #
        # for ind in ind_l:
        #     if dataset == 'mia':
        #         inp1 = remotedir + 'amt_answers_' + data_n + '_claims_exp_final.csv'
        #         inp1_w = remotedir + 'worker_amt_answers_' + data_n + '_claims_exp.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #     else:
        #         inp1 = remotedir  +'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final.csv'
        #         inp1_w = remotedir  +'worker_amt_answers_'+data_n+'_claims_exp'+str(ind)+'.csv'
        #         df[ind] = pd.read_csv(inp1, sep="\t")
        #         df_w[ind] = pd.read_csv(inp1_w, sep="\t")
        #
        #         df_m = df[ind].copy()
        #
        #
        #
        #     rep_num = len(df_m[df_m['leaning']==-1])/float(60)
        #     dem_num = len(df_m[df_m['leaning'] == 1])/float(60)
        #     neut_num = len(df_m[df_m['leaning'] == 0])/float(60)
        #
        #     min_num = np.min([int(rep_num), int(dem_num), int(neut_num)])
        #
        #     dem_workers = list(set(df_m[df_m['leaning'] == 1]['worker_id']))
        #     rep_workers = list(set(df_m[df_m['leaning'] == -1]['worker_id']))
        #     neut_workers = list(set(df_m[df_m['leaning'] == 0]['worker_id']))
        #
        #     random.shuffle(dem_workers)
        #     random.shuffle(rep_workers)
        #     random.shuffle(neut_workers)
        #
        #     dem_workers = dem_workers[:min_num]
        #     rep_workers = rep_workers[:min_num]
        #     neut_workers = neut_workers[:min_num]
        #
        #     all_workers = []
        #     all_workers += dem_workers
        #     all_workers += rep_workers
        #     all_workers += neut_workers
        #
        #     df[ind] = df_m[df_m['worker_id'].isin(all_workers)]
        #
        #     df[ind].to_csv(remotedir + 'amt_answers_'+data_n+'_claims_exp'+str(ind)+'_final_balanced.csv',
        #                 columns=df[ind].columns, sep="\t", index=False)
        #
        # exit()

        # balance_f = 'balanced'
        remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/'

        balance_f = 'un_balanced'
        # balance_f = 'balanced'

        # fig_f = True
        fig_f = False



        gt_l_dict = collections.defaultdict(list)
        perc_l_dict = collections.defaultdict(list)
        abs_perc_l_dict = collections.defaultdict(list)
        gt_set_dict = collections.defaultdict(list)
        perc_mean_dict = collections.defaultdict(list)
        abs_perc_mean_dict = collections.defaultdict(list)
        for dataset in  ['snopes_nonpol','snopes','politifact','mia']:
            if dataset == 'mia':
                claims_list = []
                local_dir_saving = ''
                remotedir_1 = '/NS/twitter-8/work/Reza/reliable_news/data/'
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/mia/'
                news_cat_list = [ 'rumor', 'non-rumor']
                final_inp_exp1 = open(remotedir_1 + local_dir_saving
                                      + 'amt_data-set/rumor_non-rumor_news_exp1.txt', 'r')

                sample_tweets_exp1 = json.load(final_inp_exp1)

                input_rumor = open(remotedir_1 + local_dir_saving + 'rumer_tweets', 'r')
                input_non_rumor = open(remotedir_1 + local_dir_saving + 'non_rumer_tweets', 'r')

                exp1_list = sample_tweets_exp1

                out_list = []
                cnn_list = []
                foxnews_list = []
                ap_list = []
                tweet_txt_dict = {}
                tweet_link_dict = {}
                tweet_publisher_dict = {}
                tweet_rumor = {}
                tweet_lable_dict = {}
                tweet_non_rumor = {}
                pub_dict = collections.defaultdict(list)
                for tweet in exp1_list:

                    tweet_id = tweet[0]
                    publisher_name = tweet[1]
                    tweet_txt = tweet[2]
                    tweet_link = tweet[3]
                    tmp_list = [tweet_id, publisher_name, tweet_txt, tweet_link]
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_link_dict[tweet_id] = tweet_link
                    tweet_publisher_dict[tweet_id] = publisher_name
                    if int(tweet_id) < 100060:
                        tweet_lable_dict[tweet_id] = 'rumor'
                    else:
                        tweet_lable_dict[tweet_id] = 'non-rumor'
                # outF = open(remotedir + 'table_out.txt', 'w')


            if dataset == 'snopes':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')
            if dataset == 'snopes_nonpol':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/snopes/'
                inp_all = glob.glob(remotedir + '/all_snopes_news.txt')
                news_cat_list = [ 'FALSE','MOSTLY FALSE', 'MIXTURE', 'MOSTLY TRUE', 'TRUE']
                news_cat_list_f = ['false', 'mostly_false',  'mixture', 'mostly_true', 'true']

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 5):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/non_politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    publisher_name = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    tweet_id = publisher_name
                    cat_lable = line_splt[4]
                    dat = line_splt[5]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                # outF = open(remotedir + 'table_out.txt', 'w')

            if dataset == 'politifact':
                claims_list = []
                remotedir = '/NS/twitter-8/work/Reza/reliable_news/new_collected_data/politifact/'
                inp_all = glob.glob(remotedir + 'politic_claims/*.txt')
                news_cat_list = [ 'pants-fire', 'false', 'mostly-false', 'half-true', 'mostly-true','true']
                news_cat_list_f = ['pants-fire', 'false', 'mostly-false','half-true', 'mostly-true',  'true']
                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                print(inp_all)

                for i in range(0, 6):
                    df_cat = news_cat_list[i]
                    df_cat_f = news_cat_list_f[i]
                    inF = open(remotedir + 'politic_claims/' + df_cat_f + '/politic_claims.txt', 'r')
                    cat_count = 0
                    for line in inF:
                        claims_list.append(line)
                        cat_count += 1

                sample_tweets_exp1 = []

                tweet_txt_dict = {}
                tweet_date_dict = {}
                tweet_lable_dict = {}
                tweet_publisher_dict = {}
                for line in claims_list:
                    line_splt = line.split('<<||>>')
                    tweet_id = int(line_splt[2])
                    tweet_txt = line_splt[3]
                    publisher_name = line_splt[4]
                    cat_lable = line_splt[5]
                    dat = line_splt[6]
                    dt_splt = dat.split(' ')[0].split('-')
                    m_day = int(dt_splt[2])
                    m_month = int(dt_splt[1])
                    m_year = int(dt_splt[0])
                    m_date = datetime.datetime(year=m_year, month=m_month, day=m_day)
                    tweet_txt_dict[tweet_id] = tweet_txt
                    tweet_date_dict[tweet_id] = m_date
                    tweet_lable_dict[tweet_id] = cat_lable
                    tweet_publisher_dict[tweet_id] = publisher_name
                # outF = open(remotedir + 'table_out.txt', 'w')





            if dataset=='snopes':
                data_n = 'sp'
                data_addr = 'snopes'
                ind_l = [1,2,3]
                data_name = 'Snopes'
            if dataset == 'snopes_nonpol':
                data_n = 'sp_nonpol'
                data_addr = 'snopes'
                ind_l = [1]
                data_name = 'Snopes_nonpolitical'
            elif dataset=='politifact':
                data_addr = 'politifact/fig/'
                data_n = 'pf'
                ind_l = [1,2,3]
                data_name = 'PolitiFact'
            elif dataset=='mia':
                data_addr = 'mia/fig/'

                data_n = 'mia'
                ind_l = [1]
                data_name = 'Rumors'

            df = collections.defaultdict()
            df_w = collections.defaultdict()
            df_wei = collections.defaultdict()
            tweet_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg_med_var = collections.defaultdict(list)
            tweet_dev_avg = {}
            tweet_dev_med = {}
            tweet_dev_var = {}
            tweet_avg = {}
            tweet_med = {}
            tweet_var = {}
            tweet_gt_var = {}

            tweet_dev_avg_l = []
            tweet_dev_med_l = []
            tweet_dev_var_l = []
            tweet_avg_l = []
            tweet_med_l = []
            tweet_var_l = []
            tweet_gt_var_l = []
            avg_susc = 0
            avg_gull = 0
            avg_cyn = 0

            tweet_abs_dev_avg = {}
            tweet_abs_dev_med = {}
            tweet_abs_dev_var = {}

            tweet_abs_dev_avg_l = []
            tweet_abs_dev_med_l = []
            tweet_abs_dev_var_l = []

            tweet_abs_dev_avg_rnd = {}
            tweet_dev_avg_rnd = {}

            tweet_skew = {}
            tweet_skew_l = []

            tweet_vote_avg_med_var = collections.defaultdict(list)
            tweet_vote_avg = collections.defaultdict()
            tweet_vote_med = collections.defaultdict()
            tweet_vote_var = collections.defaultdict()

            tweet_avg_group = collections.defaultdict()
            tweet_med_group = collections.defaultdict()
            tweet_var_group = collections.defaultdict()

            tweet_kldiv_group= collections.defaultdict()

            w_cyn_dict= collections.defaultdict()
            w_gull_dict= collections.defaultdict()
            w_apb_dict= collections.defaultdict()

            tweet_vote_avg_l = []
            tweet_vote_med_l = []
            tweet_vote_var_l = []
            cat_time_dict = collections.defaultdict(dict)
            time_dict = collections.defaultdict()
            cat_time_list = collections.defaultdict(list)
            cat_time_dict_var = collections.defaultdict(dict)
            time_dict_var = collections.defaultdict()
            cat_time_list_var = collections.defaultdict(list)
            w_apb_dict_wei = collections.defaultdict()

            tweet_avg_wei= collections.defaultdict()
            # tweet_med[t_id] = np.median(val_list)
            tweet_var_wei= collections.defaultdict()
            tweet_id_text = collections.defaultdict()
            for ind in ind_l:
                if balance_f == 'balanced':
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final_balanced.csv'
                else:
                    inp1 = remotedir + 'amt_answers_'+data_n+'_claims_exp' + str(ind) + '_final.csv'
                    inp1_wei = remotedir + 'amt_answers_' + data_n + '_claims_exp' + str(ind) + '_final_weighted.csv'
                inp1_w = remotedir + 'worker_amt_answers_'+data_n+'_claims_exp' + str(ind) + '.csv'
                df[ind] = pd.read_csv(inp1, sep="\t")
                df_wei[ind] = pd.read_csv(inp1_wei, sep="\t")
                df_w[ind] = pd.read_csv(inp1_w, sep="\t")

                df_m = df[ind].copy()
                df_m_wei = df_wei[ind].copy()

                groupby_ftr = 'tweet_id'
                grouped = df_m.groupby(groupby_ftr, sort=False)
                grouped_sum = df_m.groupby(groupby_ftr, sort=False).sum()


                for t_id in grouped.groups.keys():
                    df_tmp = df_m[df_m['tweet_id'] == t_id]
                    ind_t = df_tmp.index.tolist()[0]
                    df_tmp_wei = df_m_wei[df_m_wei['tweet_id'] == t_id]
                    ind_t_wei = df_tmp_wei.index.tolist()[0]
                    weights = []
                    tweet_id_text[t_id] = df_tmp['text'][ind_t]

                    dem_df = df_tmp[df_tmp['leaning']==1]
                    rep_df = df_tmp[df_tmp['leaning']==-1]
                    neut_df = df_tmp[df_tmp['leaning']==0]
                    dem_val_list = list(dem_df['rel_v'])
                    rep_val_list = list(rep_df['rel_v'])
                    neut_val_list = list(neut_df['rel_v'])
                    # tweet_avg_group[t_id] = np.mean(dem_val_list) - np.mean(rep_val_list)
                    # tweet_med_group[t_id] = np.median(dem_val_list) - np.median(rep_val_list)
                    # tweet_var_group[t_id] = np.var(dem_val_list) - np.var(rep_val_list)
                    # tweet_kldiv_group[t_id] = np.mean(dem_val_list)+np.mean(rep_val_list) + np.mean(neut_val_list)
                    # tweet_kldiv_group[t_id] = np.var(dem_val_list) * np.var(rep_val_list) / np.var(neut_val_list)

                    tweet_avg_group[t_id] = np.abs(np.mean(dem_val_list) - np.mean(rep_val_list))
                    tweet_med_group[t_id] = np.abs(np.median(dem_val_list) - np.median(rep_val_list))
                    tweet_var_group[t_id] = np.abs(np.var(dem_val_list) - np.var(rep_val_list))
                    tweet_kldiv_group[t_id] = np.round(scipy.stats.ks_2samp(dem_val_list,rep_val_list)[1], 4)



                    cat_time_dict[tweet_lable_dict[t_id]][t_id] = np.mean(df_tmp['delta_time'])
                    cat_time_list[tweet_lable_dict[t_id]].append(np.mean(df_tmp['delta_time']))
                    time_dict[t_id] = np.mean(df_tmp['delta_time'])

                    cat_time_dict_var[tweet_lable_dict[t_id]][t_id] = np.std(df_tmp['delta_time'])
                    cat_time_list_var[tweet_lable_dict[t_id]].append(np.std(df_tmp['delta_time']))
                    time_dict_var[t_id] = np.std(df_tmp['delta_time'])


                    w_pt_list = list(df_tmp['rel_v'])
                    w_err_list = list(df_tmp['err'])
                    # w_abs_err_list = list(df_tmp['abs_err'])
                    w_sus_list = list(df_tmp['susc'])
                    w_sus_list_wei = list(df_tmp_wei['susc'])
                    # w_norm_err_list = list(df_tmp['norm_err'])
                    # w_norm_abs_err_list = list(df_tmp['norm_abs_err'])
                    # w_cyn_list = list(df_tmp['cyn'])
                    # w_gull_list = list(df_tmp['gull'])
                    w_acc_list_tmp = list(df_tmp['acc'])


                    df_cyn = df_tmp[df_tmp['cyn']>0]
                    df_gull = df_tmp[df_tmp['gull']>0]

                    w_cyn_list = list(df_cyn['cyn'])
                    w_gull_list = list(df_gull['gull'])

                    w_cyn_dict[t_id] = np.mean(w_cyn_list)
                    w_gull_dict[t_id] = np.mean(w_gull_list)
                    w_apb_dict[t_id] = np.mean(w_sus_list)
                    w_apb_dict_wei[t_id] = np.mean(w_sus_list_wei)


                    weights.append(np.ones_like(list(df_tmp['rel_v'])) / float(len(df_tmp)))
                    val_list = list(df_tmp['rel_v'])
                    tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg[t_id] = np.mean(val_list)
                    tweet_med[t_id] = np.median(val_list)
                    tweet_var[t_id] = np.var(val_list)
                    tweet_gt_var[t_id] = df_tmp['rel_gt_v'][ind_t]


                    val_list_wei = list(df_tmp_wei['rel_v'])
                    # tweet_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_avg_wei[t_id] = np.mean(val_list_wei)
                    # tweet_med[t_id] = np.median(val_list)
                    tweet_var_wei[t_id] = np.var(val_list_wei)





                    tweet_avg_l.append(np.mean(val_list))
                    tweet_med_l.append(np.median(val_list))
                    tweet_var_l.append(np.var(val_list))
                    tweet_gt_var_l.append(df_tmp['rel_gt_v'][ind_t])

                    vot_list = []
                    vot_list_tmp = list(df_tmp['vote'])

                    for vot in vot_list_tmp:
                        if vot < 0 :
                            vot_list.append(vot)
                    tweet_vote_avg_med_var[t_id] = [np.mean(vot_list), np.median(vot_list), np.var(vot_list)]
                    tweet_vote_avg[t_id] = np.mean(vot_list)
                    tweet_vote_med[t_id] = np.median(vot_list)
                    tweet_vote_var[t_id] = np.var(vot_list)

                    tweet_vote_avg_l.append(np.mean(vot_list))
                    tweet_vote_med_l.append(np.median(vot_list))
                    tweet_vote_var_l.append(np.var(vot_list))



                    # accuracy = np.sum(df_tmp['acc']) / float(len(df_tmp))
                    # all_acc.append(accuracy)


                    tweet_skew[t_id] = scipy.stats.skew(val_list)
                    tweet_skew_l.append(tweet_skew[t_id])



                    # val_list = list(df_tmp['susc'])
                    val_list = list(df_tmp['err'])
                    abs_var_err = [np.abs(x) for x in val_list]
                    tweet_dev_avg_med_var[t_id] = [np.mean(val_list), np.median(val_list), np.var(val_list)]
                    tweet_dev_avg[t_id] = np.mean(val_list)
                    tweet_dev_med[t_id] = np.median(val_list)
                    tweet_dev_var[t_id] = np.var(val_list)


                    tweet_dev_avg_l.append(np.mean(val_list))
                    tweet_dev_med_l.append(np.median(val_list))
                    tweet_dev_var_l.append(np.var(val_list))

                    tweet_abs_dev_avg[t_id] = np.mean(abs_var_err)
                    tweet_abs_dev_med[t_id] = np.median(abs_var_err)
                    tweet_abs_dev_var[t_id] = np.var(abs_var_err)

                    tweet_abs_dev_avg_l.append(np.mean(abs_var_err))
                    tweet_abs_dev_med_l.append(np.median(abs_var_err))
                    tweet_abs_dev_var_l.append(np.var(abs_var_err))

                    # tweet_popularity_dict[t_id] = tweet_popularity[t_id]
                    sum_rnd_abs_perc = 0
                    sum_rnd_perc = 0
                    for val in [-1, -2/float(3), -1/float(3), 0, 1/float(3), 2/float(3),1]:
                        sum_rnd_perc+= val - df_tmp['rel_gt_v'][ind_t]
                        sum_rnd_abs_perc += np.abs(val - df_tmp['rel_gt_v'][ind_t])
                    random_perc = np.abs(sum_rnd_perc / float(7))
                    random_abs_perc = sum_rnd_abs_perc / float(7)

                    tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)
                    # tweet_dev_avg_rnd[t_id] = np.mean(val_list) / float(random_perc)
                    # tweet_abs_dev_avg_rnd[t_id] = np.mean(abs_var_err) / float(random_abs_perc)






            word_dict_list = collections.defaultdict(list)
            vocab_count_dict_dict = collections.defaultdict(dict)

            # out_dict = w_apb_dict_wei
            out_dict = w_apb_dict
            # out_dict = w_gull_dict
            # out_dict = w_cyn_dict
            # out_dict = tweet_var
            # out_dict = time_dict
            w_apb_wei_list = []
            w_apb_list = []
            # tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=False)
            tweet_l_sort = sorted(out_dict, key=out_dict.get, reverse=True)
            output = open(
                remotedir  + data_n+'_tfidf_top.txt', 'w')
            output_micro = open(
                remotedir + data_n+ '_tfidf_micro_top.txt',
                'w')
            list_text = []
            count = 0
            for t_id in tweet_l_sort[:int(0.2 * len(tweet_l_sort))]:


                text_t = tweet_id_text[t_id]
                # print(text_t.dtype)
                try:
                    # list_text += [str(text_t.encode('utf-8'))]
                    text = text_t.lower().replace("\n", " ").replace("\t", " ")

                    pattern = re.compile("^[a-z#][a-z0-9#-]+$")
                    words = twokenize.tokenizeRawTweetText(text)

                    stems = []
                    for word in words:
                        if len(word) <= 2:
                            continue
                        if '/' in word or '\\' in word:
                            continue
                        list_text += [str(word)]
                except:
                    count += 1
                    # print(text_t)
                    continue

            print('err : ' + str(count))
            count = 0
            vocab_features_dict = {}
            num_feat_after_vect = collections.defaultdict(int)
            num_feat_after_vect_tf_idf = collections.defaultdict(int)
            num_appearances_word = []
            num_appearances_word_micro = []
            num_appearances_word_micro_uniform = []
            num_appearances_tf_idf = []
            vocab_count_dict = collections.defaultdict(int)
            vocab_tfidf_dict = collections.defaultdict(int)

            Y_train_counts_matrix = []

            print('word_counting is started')
            # count_vect = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False, analyzer='word', stop_words='english', max_features=max_f)
            count_vect = CountVectorizer(analyzer='word', stop_words='english', max_features=max_f)
            X_train_counts = count_vect.fit_transform(list_text)
            # X_train_counts_matrix = X_train_counts.toarray()
            # num_appearances_word += [sum(x) for x in zip(*X_train_counts_matrix)]

            tfidf_transformer = TfidfTransformer(use_idf=True)
            X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
            X_train_counts_matrix = X_train_tfidf.toarray()
            num_appearances_word += [sum(x) for x in zip(*X_train_counts_matrix)]

            for x_list in X_train_counts_matrix:
                sum_l = sum(x_list)
                if sum(x_list) == 0:
                    sum_l = 1
                tmp_list = [float(x) / sum_l for x in x_list]
                Y_train_counts_matrix.append(tmp_list)
            num_appearances_word_micro += [sum(x) for x in zip(*Y_train_counts_matrix)]

            for x_list in X_train_counts_matrix:
                tmp_list = []
                sum_l = sum(x_list)
                if sum(x_list) == 0:
                    sum_l = 1
                # tmp_list = [float(x)/sum_l for x in x_list]
                for x in x_list:
                    if float(x) > 0:
                        tmp_list.append(1)
                    else:
                        tmp_list.append(0)
                Y_train_counts_matrix.append(tmp_list)
            num_appearances_word_micro_uniform += [sum(x) for x in zip(*Y_train_counts_matrix)]

            # tmp_dict = {}
            # len_vocab_features_dict = len(vocab_features_dict)
            vocab_features_dict = dict((value, key) for (key, value) in count_vect.vocabulary_.items())
            print('number of appearance of words are computed')

            count_vect_list = count_vect.vocabulary_.items()
            ## count_vect_list = sorted(count_vect_list, key=lambda x: (-x[1], x[0]), reverse=False)
            for (key, value) in count_vect_list:
                if key not in vocab_features_dict:
                    vocab_features_dict[key] = value
                else:
                    continue

            max_f = min(max_f, len(num_appearances_word))

            for word_index in range(max_f):
                vocab_count_dict[vocab_features_dict[word_index]] = num_appearances_word[word_index]

            words_list = sorted(vocab_count_dict, key=vocab_count_dict.get, reverse=True)
            # word_dict_list[i_cluster] = words_list
            # vocab_count_dict_dict[i_cluster] = vocab_count_dict
            for word in words_list:
                output.write(str(int(vocab_count_dict[word] * 10)) + '\t' + word + '\n')
            output.close()

            vocab_count_dict = collections.defaultdict(int)

            for word_index in range(max_f):
                vocab_count_dict[vocab_features_dict[word_index]] = num_appearances_word_micro[word_index]

            words_list = sorted(vocab_count_dict, key=vocab_count_dict.get, reverse=True)

            for word in words_list:
                output_micro.write(str(vocab_count_dict[word]) + '\t' + word + '\n')
            output_micro.close()

        # word_dict_list_new = collections.defaultdict(list)
        # for i in range(number_clusters):
        #     my_word_list = word_dict_list[i]
        #     tmp_new_word_list = []
        #     for word_el in my_word_list:
        #         word_count = 0
        #         for j in range(number_clusters):
        #             if word_el in word_dict_list[j] and j != i:
        #                 word_count += 1
        #         if word_count == 0:
        #             tmp_new_word_list.append(word_el)
        #     word_dict_list_new[i] = tmp_new_word_list
        #
        # for i in range(number_clusters):
        #     words_list = word_dict_list_new[i]
        #     print('cluster ' + str(i) + '----------------')
        #     for word in words_list:
        #         print(str(int(vocab_count_dict_dict[i][word] * 10)) + '\t' + word + '\n')

# tmp_dict[df_cat] = cat_count
# sum_all_cat += cat_count

